{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da53e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f510a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Send the model to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bffe969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms to be applied to the image data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    #transforms.CenterCrop(100),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07cd5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(\"D:/tay/Data/Data_crop/Data_split/classes_image/image_train/\", transform=transform)\n",
    "\n",
    "# Define the data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Define the label names\n",
    "label_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cb55aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "val_dataset = datasets.ImageFolder(\"D:/tay/Data/Data_crop/Data_split/classes_image/image_test/\", transform=transform)\n",
    "\n",
    "# Define the data loader\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Define the label names\n",
    "label_names = val_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9d0570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hand_A', 'hand_A2', 'hand_B', 'hand_C', 'hand_D', 'hand_D2', 'hand_E', 'hand_G', 'hand_H', 'hand_I', 'hand_K', 'hand_L', 'hand_M', 'hand_N', 'hand_O', 'hand_O3', 'hand_P', 'hand_Q', 'hand_R', 'hand_S', 'hand_T', 'hand_U', 'hand_V', 'hand_X', 'hand_Y']\n"
     ]
    }
   ],
   "source": [
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "da00c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(128 * 8 * 8, 25)  # fully-connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 128 * 8 * 8)  # flatten the output of the last convolutional layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "07272d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=8192, out_features=25, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d2d9644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/274360], Loss: 27.5935, Train time: 0.04\n",
      "Epoch [1/1], Step [200/274360], Loss: 2.8099, Train time: 0.08\n",
      "Epoch [1/1], Step [300/274360], Loss: 3.9319, Train time: 0.12\n",
      "Epoch [1/1], Step [400/274360], Loss: 1.1444, Train time: 0.16\n",
      "Epoch [1/1], Step [500/274360], Loss: 4.1925, Train time: 0.20\n",
      "Epoch [1/1], Step [600/274360], Loss: 1.2972, Train time: 0.28\n",
      "Epoch [1/1], Step [700/274360], Loss: 3.9364, Train time: 0.32\n",
      "Epoch [1/1], Step [800/274360], Loss: 0.0151, Train time: 0.37\n",
      "Epoch [1/1], Step [900/274360], Loss: 9.6515, Train time: 0.46\n",
      "Epoch [1/1], Step [1000/274360], Loss: 3.1977, Train time: 0.50\n",
      "Epoch [1/1], Step [1100/274360], Loss: 3.1546, Train time: 0.59\n",
      "Epoch [1/1], Step [1200/274360], Loss: 2.0256, Train time: 0.63\n",
      "Epoch [1/1], Step [1300/274360], Loss: 2.8301, Train time: 0.68\n",
      "Epoch [1/1], Step [1400/274360], Loss: 1.2383, Train time: 0.75\n",
      "Epoch [1/1], Step [1500/274360], Loss: 0.8676, Train time: 0.80\n",
      "Epoch [1/1], Step [1600/274360], Loss: 5.1049, Train time: 0.84\n",
      "Epoch [1/1], Step [1700/274360], Loss: 0.0077, Train time: 0.90\n",
      "Epoch [1/1], Step [1800/274360], Loss: 0.0890, Train time: 0.96\n",
      "Epoch [1/1], Step [1900/274360], Loss: 0.5395, Train time: 1.03\n",
      "Epoch [1/1], Step [2000/274360], Loss: 0.8012, Train time: 1.08\n",
      "Epoch [1/1], Step [2100/274360], Loss: 0.5955, Train time: 1.14\n",
      "Epoch [1/1], Step [2200/274360], Loss: 0.0130, Train time: 1.19\n",
      "Epoch [1/1], Step [2300/274360], Loss: 0.2826, Train time: 1.24\n",
      "Epoch [1/1], Step [2400/274360], Loss: 2.2765, Train time: 1.29\n",
      "Epoch [1/1], Step [2500/274360], Loss: 0.9897, Train time: 1.35\n",
      "Epoch [1/1], Step [2600/274360], Loss: 6.0954, Train time: 1.40\n",
      "Epoch [1/1], Step [2700/274360], Loss: 0.1315, Train time: 1.47\n",
      "Epoch [1/1], Step [2800/274360], Loss: 0.9350, Train time: 1.51\n",
      "Epoch [1/1], Step [2900/274360], Loss: 4.1991, Train time: 1.60\n",
      "Epoch [1/1], Step [3000/274360], Loss: 0.0935, Train time: 1.67\n",
      "Epoch [1/1], Step [3100/274360], Loss: 0.5375, Train time: 1.74\n",
      "Epoch [1/1], Step [3200/274360], Loss: 0.3109, Train time: 1.80\n",
      "Epoch [1/1], Step [3300/274360], Loss: 1.5116, Train time: 1.87\n",
      "Epoch [1/1], Step [3400/274360], Loss: 6.1247, Train time: 1.91\n",
      "Epoch [1/1], Step [3500/274360], Loss: 0.0230, Train time: 2.00\n",
      "Epoch [1/1], Step [3600/274360], Loss: 0.2215, Train time: 2.10\n",
      "Epoch [1/1], Step [3700/274360], Loss: 0.0026, Train time: 2.20\n",
      "Epoch [1/1], Step [3800/274360], Loss: 0.1236, Train time: 2.25\n",
      "Epoch [1/1], Step [3900/274360], Loss: 0.0192, Train time: 2.33\n",
      "Epoch [1/1], Step [4000/274360], Loss: 0.1166, Train time: 2.39\n",
      "Epoch [1/1], Step [4100/274360], Loss: 0.0013, Train time: 2.46\n",
      "Epoch [1/1], Step [4200/274360], Loss: 0.0021, Train time: 2.54\n",
      "Epoch [1/1], Step [4300/274360], Loss: 0.0159, Train time: 2.59\n",
      "Epoch [1/1], Step [4400/274360], Loss: 0.0050, Train time: 2.68\n",
      "Epoch [1/1], Step [4500/274360], Loss: 0.0108, Train time: 2.73\n",
      "Epoch [1/1], Step [4600/274360], Loss: 1.2126, Train time: 2.80\n",
      "Epoch [1/1], Step [4700/274360], Loss: 2.7516, Train time: 2.86\n",
      "Epoch [1/1], Step [4800/274360], Loss: 0.0256, Train time: 2.98\n",
      "Epoch [1/1], Step [4900/274360], Loss: 1.0899, Train time: 3.06\n",
      "Epoch [1/1], Step [5000/274360], Loss: 0.0101, Train time: 3.12\n",
      "Epoch [1/1], Step [5100/274360], Loss: 0.1024, Train time: 3.22\n",
      "Epoch [1/1], Step [5200/274360], Loss: 0.0858, Train time: 3.31\n",
      "Epoch [1/1], Step [5300/274360], Loss: 0.2907, Train time: 3.38\n",
      "Epoch [1/1], Step [5400/274360], Loss: 0.1145, Train time: 3.47\n",
      "Epoch [1/1], Step [5500/274360], Loss: 0.0032, Train time: 3.53\n",
      "Epoch [1/1], Step [5600/274360], Loss: 3.1661, Train time: 3.63\n",
      "Epoch [1/1], Step [5700/274360], Loss: 0.0257, Train time: 3.68\n",
      "Epoch [1/1], Step [5800/274360], Loss: 0.1960, Train time: 3.76\n",
      "Epoch [1/1], Step [5900/274360], Loss: 0.0113, Train time: 3.85\n",
      "Epoch [1/1], Step [6000/274360], Loss: 0.1268, Train time: 3.91\n",
      "Epoch [1/1], Step [6100/274360], Loss: 0.0178, Train time: 4.01\n",
      "Epoch [1/1], Step [6200/274360], Loss: 0.0551, Train time: 4.05\n",
      "Epoch [1/1], Step [6300/274360], Loss: 0.0065, Train time: 4.17\n",
      "Epoch [1/1], Step [6400/274360], Loss: 0.1689, Train time: 4.28\n",
      "Epoch [1/1], Step [6500/274360], Loss: 0.0171, Train time: 4.35\n",
      "Epoch [1/1], Step [6600/274360], Loss: 0.2145, Train time: 4.46\n",
      "Epoch [1/1], Step [6700/274360], Loss: 0.0250, Train time: 4.53\n",
      "Epoch [1/1], Step [6800/274360], Loss: 0.0226, Train time: 4.60\n",
      "Epoch [1/1], Step [6900/274360], Loss: 0.0011, Train time: 4.69\n",
      "Epoch [1/1], Step [7000/274360], Loss: 0.1042, Train time: 4.77\n",
      "Epoch [1/1], Step [7100/274360], Loss: 0.0019, Train time: 4.90\n",
      "Epoch [1/1], Step [7200/274360], Loss: 0.0224, Train time: 5.00\n",
      "Epoch [1/1], Step [7300/274360], Loss: 0.0100, Train time: 5.06\n",
      "Epoch [1/1], Step [7400/274360], Loss: 0.0223, Train time: 5.15\n",
      "Epoch [1/1], Step [7500/274360], Loss: 0.9564, Train time: 5.23\n",
      "Epoch [1/1], Step [7600/274360], Loss: 0.0045, Train time: 5.31\n",
      "Epoch [1/1], Step [7700/274360], Loss: 1.8489, Train time: 5.44\n",
      "Epoch [1/1], Step [7800/274360], Loss: 0.0317, Train time: 5.56\n",
      "Epoch [1/1], Step [7900/274360], Loss: 0.0029, Train time: 5.65\n",
      "Epoch [1/1], Step [8000/274360], Loss: 0.0021, Train time: 5.76\n",
      "Epoch [1/1], Step [8100/274360], Loss: 0.0220, Train time: 5.92\n",
      "Epoch [1/1], Step [8200/274360], Loss: 0.1141, Train time: 6.00\n",
      "Epoch [1/1], Step [8300/274360], Loss: 0.0073, Train time: 6.08\n",
      "Epoch [1/1], Step [8400/274360], Loss: 0.0064, Train time: 6.13\n",
      "Epoch [1/1], Step [8500/274360], Loss: 0.0042, Train time: 6.21\n",
      "Epoch [1/1], Step [8600/274360], Loss: 2.0479, Train time: 6.39\n",
      "Epoch [1/1], Step [8700/274360], Loss: 0.0108, Train time: 6.51\n",
      "Epoch [1/1], Step [8800/274360], Loss: 0.0006, Train time: 6.57\n",
      "Epoch [1/1], Step [8900/274360], Loss: 0.0908, Train time: 6.71\n",
      "Epoch [1/1], Step [9000/274360], Loss: 0.0956, Train time: 6.80\n",
      "Epoch [1/1], Step [9100/274360], Loss: 0.0093, Train time: 6.88\n",
      "Epoch [1/1], Step [9200/274360], Loss: 0.0241, Train time: 6.99\n",
      "Epoch [1/1], Step [9300/274360], Loss: 0.0005, Train time: 7.08\n",
      "Epoch [1/1], Step [9400/274360], Loss: 0.0181, Train time: 7.17\n",
      "Epoch [1/1], Step [9500/274360], Loss: 0.0768, Train time: 7.26\n",
      "Epoch [1/1], Step [9600/274360], Loss: 0.8805, Train time: 7.31\n",
      "Epoch [1/1], Step [9700/274360], Loss: 0.0017, Train time: 7.37\n",
      "Epoch [1/1], Step [9800/274360], Loss: 0.0008, Train time: 7.44\n",
      "Epoch [1/1], Step [9900/274360], Loss: 0.1307, Train time: 7.51\n",
      "Epoch [1/1], Step [10000/274360], Loss: 0.0465, Train time: 7.66\n",
      "Epoch [1/1], Step [10100/274360], Loss: 6.7400, Train time: 7.72\n",
      "Epoch [1/1], Step [10200/274360], Loss: 0.3574, Train time: 7.86\n",
      "Epoch [1/1], Step [10300/274360], Loss: 0.2769, Train time: 7.93\n",
      "Epoch [1/1], Step [10400/274360], Loss: 0.0046, Train time: 8.04\n",
      "Epoch [1/1], Step [10500/274360], Loss: 0.0013, Train time: 8.08\n",
      "Epoch [1/1], Step [10600/274360], Loss: 0.0026, Train time: 8.18\n",
      "Epoch [1/1], Step [10700/274360], Loss: 0.0287, Train time: 8.36\n",
      "Epoch [1/1], Step [10800/274360], Loss: 0.0048, Train time: 8.55\n",
      "Epoch [1/1], Step [10900/274360], Loss: 0.2518, Train time: 8.61\n",
      "Epoch [1/1], Step [11000/274360], Loss: 1.4380, Train time: 8.67\n",
      "Epoch [1/1], Step [11100/274360], Loss: 0.1028, Train time: 8.76\n",
      "Epoch [1/1], Step [11200/274360], Loss: 0.0067, Train time: 8.83\n",
      "Epoch [1/1], Step [11300/274360], Loss: 0.7216, Train time: 8.87\n",
      "Epoch [1/1], Step [11400/274360], Loss: 0.0104, Train time: 9.32\n",
      "Epoch [1/1], Step [11500/274360], Loss: 0.0128, Train time: 9.36\n",
      "Epoch [1/1], Step [11600/274360], Loss: 0.0008, Train time: 9.41\n",
      "Epoch [1/1], Step [11700/274360], Loss: 0.1762, Train time: 9.45\n",
      "Epoch [1/1], Step [11800/274360], Loss: 0.0009, Train time: 9.51\n",
      "Epoch [1/1], Step [11900/274360], Loss: 0.0130, Train time: 9.60\n",
      "Epoch [1/1], Step [12000/274360], Loss: 0.0033, Train time: 9.68\n",
      "Epoch [1/1], Step [12100/274360], Loss: 0.1433, Train time: 9.72\n",
      "Epoch [1/1], Step [12200/274360], Loss: 0.0004, Train time: 9.82\n",
      "Epoch [1/1], Step [12300/274360], Loss: 0.1320, Train time: 9.95\n",
      "Epoch [1/1], Step [12400/274360], Loss: 0.0754, Train time: 10.01\n",
      "Epoch [1/1], Step [12500/274360], Loss: 0.0006, Train time: 10.07\n",
      "Epoch [1/1], Step [12600/274360], Loss: 0.0030, Train time: 10.29\n",
      "Epoch [1/1], Step [12700/274360], Loss: 0.0625, Train time: 10.33\n",
      "Epoch [1/1], Step [12800/274360], Loss: 0.0080, Train time: 10.41\n",
      "Epoch [1/1], Step [12900/274360], Loss: 0.0451, Train time: 10.53\n",
      "Epoch [1/1], Step [13000/274360], Loss: 0.0039, Train time: 10.61\n",
      "Epoch [1/1], Step [13100/274360], Loss: 0.0317, Train time: 10.92\n",
      "Epoch [1/1], Step [13200/274360], Loss: 0.0289, Train time: 10.98\n",
      "Epoch [1/1], Step [13300/274360], Loss: 0.0010, Train time: 11.02\n",
      "Epoch [1/1], Step [13400/274360], Loss: 0.0138, Train time: 11.08\n",
      "Epoch [1/1], Step [13500/274360], Loss: 0.0173, Train time: 11.16\n",
      "Epoch [1/1], Step [13600/274360], Loss: 0.0000, Train time: 11.26\n",
      "Epoch [1/1], Step [13700/274360], Loss: 0.4110, Train time: 11.40\n",
      "Epoch [1/1], Step [13800/274360], Loss: 0.0203, Train time: 11.44\n",
      "Epoch [1/1], Step [13900/274360], Loss: 0.0029, Train time: 11.50\n",
      "Epoch [1/1], Step [14000/274360], Loss: 0.1156, Train time: 11.59\n",
      "Epoch [1/1], Step [14100/274360], Loss: 0.0080, Train time: 11.66\n",
      "Epoch [1/1], Step [14200/274360], Loss: 0.4122, Train time: 11.90\n",
      "Epoch [1/1], Step [14300/274360], Loss: 0.1925, Train time: 11.94\n",
      "Epoch [1/1], Step [14400/274360], Loss: 0.0008, Train time: 11.98\n",
      "Epoch [1/1], Step [14500/274360], Loss: 0.0024, Train time: 12.04\n",
      "Epoch [1/1], Step [14600/274360], Loss: 0.0255, Train time: 12.10\n",
      "Epoch [1/1], Step [14700/274360], Loss: 0.0160, Train time: 12.20\n",
      "Epoch [1/1], Step [14800/274360], Loss: 0.0001, Train time: 12.45\n",
      "Epoch [1/1], Step [14900/274360], Loss: 0.0546, Train time: 12.50\n",
      "Epoch [1/1], Step [15000/274360], Loss: 0.0002, Train time: 12.54\n",
      "Epoch [1/1], Step [15100/274360], Loss: 0.0017, Train time: 12.62\n",
      "Epoch [1/1], Step [15200/274360], Loss: 0.0009, Train time: 12.83\n",
      "Epoch [1/1], Step [15300/274360], Loss: 1.1954, Train time: 12.89\n",
      "Epoch [1/1], Step [15400/274360], Loss: 0.0705, Train time: 12.99\n",
      "Epoch [1/1], Step [15500/274360], Loss: 0.0004, Train time: 13.13\n",
      "Epoch [1/1], Step [15600/274360], Loss: 0.0004, Train time: 13.20\n",
      "Epoch [1/1], Step [15700/274360], Loss: 0.0087, Train time: 13.35\n",
      "Epoch [1/1], Step [15800/274360], Loss: 0.0017, Train time: 13.48\n",
      "Epoch [1/1], Step [15900/274360], Loss: 0.0001, Train time: 13.53\n",
      "Epoch [1/1], Step [16000/274360], Loss: 0.0075, Train time: 13.58\n",
      "Epoch [1/1], Step [16100/274360], Loss: 0.4549, Train time: 13.68\n",
      "Epoch [1/1], Step [16200/274360], Loss: 0.0015, Train time: 13.84\n",
      "Epoch [1/1], Step [16300/274360], Loss: 0.0032, Train time: 13.93\n",
      "Epoch [1/1], Step [16400/274360], Loss: 0.0000, Train time: 13.97\n",
      "Epoch [1/1], Step [16500/274360], Loss: 0.0004, Train time: 14.07\n",
      "Epoch [1/1], Step [16600/274360], Loss: 0.0010, Train time: 14.22\n",
      "Epoch [1/1], Step [16700/274360], Loss: 0.0000, Train time: 14.27\n",
      "Epoch [1/1], Step [16800/274360], Loss: 0.0006, Train time: 14.32\n",
      "Epoch [1/1], Step [16900/274360], Loss: 0.0003, Train time: 14.54\n",
      "Epoch [1/1], Step [17000/274360], Loss: 0.0002, Train time: 14.66\n",
      "Epoch [1/1], Step [17100/274360], Loss: 0.0002, Train time: 14.77\n",
      "Epoch [1/1], Step [17200/274360], Loss: 0.0003, Train time: 14.81\n",
      "Epoch [1/1], Step [17300/274360], Loss: 0.0692, Train time: 15.03\n",
      "Epoch [1/1], Step [17400/274360], Loss: 0.0104, Train time: 15.09\n",
      "Epoch [1/1], Step [17500/274360], Loss: 0.0001, Train time: 15.19\n",
      "Epoch [1/1], Step [17600/274360], Loss: 0.0154, Train time: 15.27\n",
      "Epoch [1/1], Step [17700/274360], Loss: 0.0014, Train time: 15.50\n",
      "Epoch [1/1], Step [17800/274360], Loss: 0.0039, Train time: 15.58\n",
      "Epoch [1/1], Step [17900/274360], Loss: 0.0400, Train time: 15.67\n",
      "Epoch [1/1], Step [18000/274360], Loss: 0.0000, Train time: 15.74\n",
      "Epoch [1/1], Step [18100/274360], Loss: 0.1190, Train time: 15.84\n",
      "Epoch [1/1], Step [18200/274360], Loss: 0.0025, Train time: 15.98\n",
      "Epoch [1/1], Step [18300/274360], Loss: 0.0000, Train time: 16.06\n",
      "Epoch [1/1], Step [18400/274360], Loss: 0.0035, Train time: 16.16\n",
      "Epoch [1/1], Step [18500/274360], Loss: 0.0990, Train time: 16.32\n",
      "Epoch [1/1], Step [18600/274360], Loss: 0.0044, Train time: 16.40\n",
      "Epoch [1/1], Step [18700/274360], Loss: 0.0013, Train time: 16.45\n",
      "Epoch [1/1], Step [18800/274360], Loss: 0.0049, Train time: 16.63\n",
      "Epoch [1/1], Step [18900/274360], Loss: 0.0008, Train time: 16.72\n",
      "Epoch [1/1], Step [19000/274360], Loss: 0.0002, Train time: 16.80\n",
      "Epoch [1/1], Step [19100/274360], Loss: 0.0010, Train time: 16.88\n",
      "Epoch [1/1], Step [19200/274360], Loss: 0.0000, Train time: 17.14\n",
      "Epoch [1/1], Step [19300/274360], Loss: 0.0051, Train time: 17.20\n",
      "Epoch [1/1], Step [19400/274360], Loss: 0.0001, Train time: 17.27\n",
      "Epoch [1/1], Step [19500/274360], Loss: 0.0130, Train time: 17.35\n",
      "Epoch [1/1], Step [19600/274360], Loss: 0.0015, Train time: 17.63\n",
      "Epoch [1/1], Step [19700/274360], Loss: 0.0011, Train time: 17.67\n",
      "Epoch [1/1], Step [19800/274360], Loss: 0.0000, Train time: 17.73\n",
      "Epoch [1/1], Step [19900/274360], Loss: 0.0009, Train time: 17.77\n",
      "Epoch [1/1], Step [20000/274360], Loss: 0.0066, Train time: 18.04\n",
      "Epoch [1/1], Step [20100/274360], Loss: 0.2508, Train time: 18.10\n",
      "Epoch [1/1], Step [20200/274360], Loss: 0.0002, Train time: 18.20\n",
      "Epoch [1/1], Step [20300/274360], Loss: 0.0000, Train time: 18.38\n",
      "Epoch [1/1], Step [20400/274360], Loss: 0.0002, Train time: 18.43\n",
      "Epoch [1/1], Step [20500/274360], Loss: 0.0038, Train time: 18.50\n",
      "Epoch [1/1], Step [20600/274360], Loss: 0.0000, Train time: 18.60\n",
      "Epoch [1/1], Step [20700/274360], Loss: 0.0116, Train time: 18.80\n",
      "Epoch [1/1], Step [20800/274360], Loss: 0.0127, Train time: 18.84\n",
      "Epoch [1/1], Step [20900/274360], Loss: 0.0000, Train time: 18.89\n",
      "Epoch [1/1], Step [21000/274360], Loss: 0.0162, Train time: 19.07\n",
      "Epoch [1/1], Step [21100/274360], Loss: 0.0001, Train time: 19.23\n",
      "Epoch [1/1], Step [21200/274360], Loss: 0.0011, Train time: 19.33\n",
      "Epoch [1/1], Step [21300/274360], Loss: 0.0015, Train time: 19.42\n",
      "Epoch [1/1], Step [21400/274360], Loss: 0.0014, Train time: 19.49\n",
      "Epoch [1/1], Step [21500/274360], Loss: 0.0479, Train time: 19.69\n",
      "Epoch [1/1], Step [21600/274360], Loss: 0.0034, Train time: 19.83\n",
      "Epoch [1/1], Step [21700/274360], Loss: 0.0323, Train time: 19.88\n",
      "Epoch [1/1], Step [21800/274360], Loss: 0.0020, Train time: 19.96\n",
      "Epoch [1/1], Step [21900/274360], Loss: 0.1214, Train time: 20.16\n",
      "Epoch [1/1], Step [22000/274360], Loss: 0.0000, Train time: 20.24\n",
      "Epoch [1/1], Step [22100/274360], Loss: 0.0038, Train time: 20.33\n",
      "Epoch [1/1], Step [22200/274360], Loss: 0.0152, Train time: 20.41\n",
      "Epoch [1/1], Step [22300/274360], Loss: 0.0001, Train time: 20.63\n",
      "Epoch [1/1], Step [22400/274360], Loss: 0.0356, Train time: 20.71\n",
      "Epoch [1/1], Step [22500/274360], Loss: 0.0000, Train time: 20.83\n",
      "Epoch [1/1], Step [22600/274360], Loss: 0.0002, Train time: 20.91\n",
      "Epoch [1/1], Step [22700/274360], Loss: 0.0013, Train time: 21.15\n",
      "Epoch [1/1], Step [22800/274360], Loss: 0.0559, Train time: 21.20\n",
      "Epoch [1/1], Step [22900/274360], Loss: 0.0080, Train time: 21.29\n",
      "Epoch [1/1], Step [23000/274360], Loss: 0.0001, Train time: 21.49\n",
      "Epoch [1/1], Step [23100/274360], Loss: 0.0016, Train time: 21.56\n",
      "Epoch [1/1], Step [23200/274360], Loss: 0.1632, Train time: 21.64\n",
      "Epoch [1/1], Step [23300/274360], Loss: 0.0108, Train time: 21.80\n",
      "Epoch [1/1], Step [23400/274360], Loss: 0.0003, Train time: 21.89\n",
      "Epoch [1/1], Step [23500/274360], Loss: 0.0001, Train time: 22.02\n",
      "Epoch [1/1], Step [23600/274360], Loss: 0.0006, Train time: 22.08\n",
      "Epoch [1/1], Step [23700/274360], Loss: 0.0026, Train time: 22.33\n",
      "Epoch [1/1], Step [23800/274360], Loss: 0.0967, Train time: 22.38\n",
      "Epoch [1/1], Step [23900/274360], Loss: 0.2811, Train time: 22.42\n",
      "Epoch [1/1], Step [24000/274360], Loss: 0.0383, Train time: 22.62\n",
      "Epoch [1/1], Step [24100/274360], Loss: 0.0006, Train time: 22.66\n",
      "Epoch [1/1], Step [24200/274360], Loss: 0.0010, Train time: 22.74\n",
      "Epoch [1/1], Step [24300/274360], Loss: 0.0329, Train time: 22.94\n",
      "Epoch [1/1], Step [24400/274360], Loss: 0.0336, Train time: 23.00\n",
      "Epoch [1/1], Step [24500/274360], Loss: 0.1727, Train time: 23.14\n",
      "Epoch [1/1], Step [24600/274360], Loss: 0.0002, Train time: 23.25\n",
      "Epoch [1/1], Step [24700/274360], Loss: 0.0108, Train time: 23.34\n",
      "Epoch [1/1], Step [24800/274360], Loss: 0.0011, Train time: 23.46\n",
      "Epoch [1/1], Step [24900/274360], Loss: 3.9202, Train time: 23.58\n",
      "Epoch [1/1], Step [25000/274360], Loss: 0.0016, Train time: 23.62\n",
      "Epoch [1/1], Step [25100/274360], Loss: 0.0564, Train time: 23.72\n",
      "Epoch [1/1], Step [25200/274360], Loss: 0.0049, Train time: 23.87\n",
      "Epoch [1/1], Step [25300/274360], Loss: 0.0049, Train time: 24.08\n",
      "Epoch [1/1], Step [25400/274360], Loss: 0.0001, Train time: 24.22\n",
      "Epoch [1/1], Step [25500/274360], Loss: 0.0064, Train time: 24.31\n",
      "Epoch [1/1], Step [25600/274360], Loss: 0.0170, Train time: 24.50\n",
      "Epoch [1/1], Step [25700/274360], Loss: 2.5959, Train time: 24.60\n",
      "Epoch [1/1], Step [25800/274360], Loss: 0.0002, Train time: 24.69\n",
      "Epoch [1/1], Step [25900/274360], Loss: 0.0001, Train time: 24.75\n",
      "Epoch [1/1], Step [26000/274360], Loss: 0.0047, Train time: 24.99\n",
      "Epoch [1/1], Step [26100/274360], Loss: 0.0003, Train time: 25.09\n",
      "Epoch [1/1], Step [26200/274360], Loss: 0.0101, Train time: 25.18\n",
      "Epoch [1/1], Step [26300/274360], Loss: 0.0003, Train time: 25.24\n",
      "Epoch [1/1], Step [26400/274360], Loss: 0.0001, Train time: 25.48\n",
      "Epoch [1/1], Step [26500/274360], Loss: 0.0014, Train time: 25.54\n",
      "Epoch [1/1], Step [26600/274360], Loss: 0.0350, Train time: 25.62\n",
      "Epoch [1/1], Step [26700/274360], Loss: 0.0002, Train time: 25.71\n",
      "Epoch [1/1], Step [26800/274360], Loss: 0.0080, Train time: 26.01\n",
      "Epoch [1/1], Step [26900/274360], Loss: 0.0001, Train time: 26.09\n",
      "Epoch [1/1], Step [27000/274360], Loss: 0.0046, Train time: 26.18\n",
      "Epoch [1/1], Step [27100/274360], Loss: 0.0022, Train time: 26.31\n",
      "Epoch [1/1], Step [27200/274360], Loss: 0.0010, Train time: 26.52\n",
      "Epoch [1/1], Step [27300/274360], Loss: 2.1545, Train time: 26.63\n",
      "Epoch [1/1], Step [27400/274360], Loss: 0.1125, Train time: 26.73\n",
      "Epoch [1/1], Step [27500/274360], Loss: 0.1245, Train time: 26.81\n",
      "Epoch [1/1], Step [27600/274360], Loss: 0.2120, Train time: 26.88\n",
      "Epoch [1/1], Step [27700/274360], Loss: 0.0871, Train time: 27.12\n",
      "Epoch [1/1], Step [27800/274360], Loss: 0.0561, Train time: 27.24\n",
      "Epoch [1/1], Step [27900/274360], Loss: 0.0002, Train time: 27.28\n",
      "Epoch [1/1], Step [28000/274360], Loss: 0.0001, Train time: 27.36\n",
      "Epoch [1/1], Step [28100/274360], Loss: 0.0005, Train time: 27.41\n",
      "Epoch [1/1], Step [28200/274360], Loss: 0.0084, Train time: 27.66\n",
      "Epoch [1/1], Step [28300/274360], Loss: 0.1324, Train time: 27.78\n",
      "Epoch [1/1], Step [28400/274360], Loss: 0.0636, Train time: 27.85\n",
      "Epoch [1/1], Step [28500/274360], Loss: 0.0045, Train time: 27.89\n",
      "Epoch [1/1], Step [28600/274360], Loss: 0.0003, Train time: 28.11\n",
      "Epoch [1/1], Step [28700/274360], Loss: 0.0003, Train time: 28.15\n",
      "Epoch [1/1], Step [28800/274360], Loss: 0.0028, Train time: 28.23\n",
      "Epoch [1/1], Step [28900/274360], Loss: 0.0024, Train time: 28.33\n",
      "Epoch [1/1], Step [29000/274360], Loss: 0.4704, Train time: 28.53\n",
      "Epoch [1/1], Step [29100/274360], Loss: 0.0800, Train time: 28.83\n",
      "Epoch [1/1], Step [29200/274360], Loss: 0.0002, Train time: 28.89\n",
      "Epoch [1/1], Step [29300/274360], Loss: 0.0067, Train time: 28.97\n",
      "Epoch [1/1], Step [29400/274360], Loss: 0.0001, Train time: 29.13\n",
      "Epoch [1/1], Step [29500/274360], Loss: 0.0001, Train time: 29.27\n",
      "Epoch [1/1], Step [29600/274360], Loss: 0.0000, Train time: 29.58\n",
      "Epoch [1/1], Step [29700/274360], Loss: 0.0002, Train time: 29.66\n",
      "Epoch [1/1], Step [29800/274360], Loss: 0.0023, Train time: 29.75\n",
      "Epoch [1/1], Step [29900/274360], Loss: 0.0001, Train time: 29.82\n",
      "Epoch [1/1], Step [30000/274360], Loss: 0.1533, Train time: 29.96\n",
      "Epoch [1/1], Step [30100/274360], Loss: 0.0018, Train time: 30.04\n",
      "Epoch [1/1], Step [30200/274360], Loss: 0.0375, Train time: 30.30\n",
      "Epoch [1/1], Step [30300/274360], Loss: 0.0010, Train time: 30.35\n",
      "Epoch [1/1], Step [30400/274360], Loss: 15.5831, Train time: 30.41\n",
      "Epoch [1/1], Step [30500/274360], Loss: 0.0000, Train time: 30.52\n",
      "Epoch [1/1], Step [30600/274360], Loss: 0.0029, Train time: 30.66\n",
      "Epoch [1/1], Step [30700/274360], Loss: 0.0076, Train time: 30.74\n",
      "Epoch [1/1], Step [30800/274360], Loss: 0.0002, Train time: 31.02\n",
      "Epoch [1/1], Step [30900/274360], Loss: 0.0000, Train time: 31.07\n",
      "Epoch [1/1], Step [31000/274360], Loss: 0.0582, Train time: 31.16\n",
      "Epoch [1/1], Step [31100/274360], Loss: 0.0002, Train time: 31.30\n",
      "Epoch [1/1], Step [31200/274360], Loss: 0.3439, Train time: 31.37\n",
      "Epoch [1/1], Step [31300/274360], Loss: 0.0506, Train time: 31.73\n",
      "Epoch [1/1], Step [31400/274360], Loss: 0.0041, Train time: 31.80\n",
      "Epoch [1/1], Step [31500/274360], Loss: 0.0002, Train time: 31.88\n",
      "Epoch [1/1], Step [31600/274360], Loss: 0.0008, Train time: 31.96\n",
      "Epoch [1/1], Step [31700/274360], Loss: 0.0002, Train time: 32.14\n",
      "Epoch [1/1], Step [31800/274360], Loss: 0.0161, Train time: 32.21\n",
      "Epoch [1/1], Step [31900/274360], Loss: 1.2746, Train time: 32.27\n",
      "Epoch [1/1], Step [32000/274360], Loss: 0.0050, Train time: 32.41\n",
      "Epoch [1/1], Step [32100/274360], Loss: 0.0637, Train time: 32.65\n",
      "Epoch [1/1], Step [32200/274360], Loss: 0.0152, Train time: 32.72\n",
      "Epoch [1/1], Step [32300/274360], Loss: 0.0001, Train time: 32.78\n",
      "Epoch [1/1], Step [32400/274360], Loss: 0.0208, Train time: 32.89\n",
      "Epoch [1/1], Step [32500/274360], Loss: 0.0004, Train time: 32.99\n",
      "Epoch [1/1], Step [32600/274360], Loss: 0.0000, Train time: 33.19\n",
      "Epoch [1/1], Step [32700/274360], Loss: 0.0000, Train time: 33.23\n",
      "Epoch [1/1], Step [32800/274360], Loss: 0.0037, Train time: 33.29\n",
      "Epoch [1/1], Step [32900/274360], Loss: 0.0000, Train time: 33.48\n",
      "Epoch [1/1], Step [33000/274360], Loss: 0.0000, Train time: 33.55\n",
      "Epoch [1/1], Step [33100/274360], Loss: 0.0103, Train time: 33.61\n",
      "Epoch [1/1], Step [33200/274360], Loss: 0.0002, Train time: 33.79\n",
      "Epoch [1/1], Step [33300/274360], Loss: 0.0044, Train time: 33.86\n",
      "Epoch [1/1], Step [33400/274360], Loss: 0.0497, Train time: 33.97\n",
      "Epoch [1/1], Step [33500/274360], Loss: 0.1272, Train time: 34.10\n",
      "Epoch [1/1], Step [33600/274360], Loss: 0.0003, Train time: 34.35\n",
      "Epoch [1/1], Step [33700/274360], Loss: 0.0043, Train time: 34.45\n",
      "Epoch [1/1], Step [33800/274360], Loss: 0.0007, Train time: 34.50\n",
      "Epoch [1/1], Step [33900/274360], Loss: 0.0009, Train time: 34.58\n",
      "Epoch [1/1], Step [34000/274360], Loss: 0.0003, Train time: 34.65\n",
      "Epoch [1/1], Step [34100/274360], Loss: 0.0000, Train time: 34.97\n",
      "Epoch [1/1], Step [34200/274360], Loss: 0.0343, Train time: 35.07\n",
      "Epoch [1/1], Step [34300/274360], Loss: 0.0029, Train time: 35.16\n",
      "Epoch [1/1], Step [34400/274360], Loss: 0.0050, Train time: 35.23\n",
      "Epoch [1/1], Step [34500/274360], Loss: 0.1522, Train time: 35.37\n",
      "Epoch [1/1], Step [34600/274360], Loss: 0.0026, Train time: 35.50\n",
      "Epoch [1/1], Step [34700/274360], Loss: 0.0018, Train time: 35.60\n",
      "Epoch [1/1], Step [34800/274360], Loss: 0.0043, Train time: 35.79\n",
      "Epoch [1/1], Step [34900/274360], Loss: 0.0000, Train time: 35.83\n",
      "Epoch [1/1], Step [35000/274360], Loss: 0.0007, Train time: 35.90\n",
      "Epoch [1/1], Step [35100/274360], Loss: 0.1307, Train time: 36.07\n",
      "Epoch [1/1], Step [35200/274360], Loss: 0.0002, Train time: 36.16\n",
      "Epoch [1/1], Step [35300/274360], Loss: 0.0008, Train time: 36.24\n",
      "Epoch [1/1], Step [35400/274360], Loss: 0.0228, Train time: 36.31\n",
      "Epoch [1/1], Step [35500/274360], Loss: 0.0000, Train time: 36.65\n",
      "Epoch [1/1], Step [35600/274360], Loss: 0.0007, Train time: 36.74\n",
      "Epoch [1/1], Step [35700/274360], Loss: 0.0168, Train time: 36.79\n",
      "Epoch [1/1], Step [35800/274360], Loss: 0.0488, Train time: 36.97\n",
      "Epoch [1/1], Step [35900/274360], Loss: 0.0031, Train time: 37.09\n",
      "Epoch [1/1], Step [36000/274360], Loss: 0.0098, Train time: 37.15\n",
      "Epoch [1/1], Step [36100/274360], Loss: 0.0013, Train time: 37.21\n",
      "Epoch [1/1], Step [36200/274360], Loss: 0.0126, Train time: 37.46\n",
      "Epoch [1/1], Step [36300/274360], Loss: 0.0596, Train time: 37.59\n",
      "Epoch [1/1], Step [36400/274360], Loss: 0.0001, Train time: 37.70\n",
      "Epoch [1/1], Step [36500/274360], Loss: 0.0000, Train time: 37.80\n",
      "Epoch [1/1], Step [36600/274360], Loss: 0.0001, Train time: 38.04\n",
      "Epoch [1/1], Step [36700/274360], Loss: 0.0018, Train time: 38.13\n",
      "Epoch [1/1], Step [36800/274360], Loss: 0.0005, Train time: 38.29\n",
      "Epoch [1/1], Step [36900/274360], Loss: 0.0222, Train time: 38.42\n",
      "Epoch [1/1], Step [37000/274360], Loss: 0.0131, Train time: 38.51\n",
      "Epoch [1/1], Step [37100/274360], Loss: 0.0085, Train time: 38.73\n",
      "Epoch [1/1], Step [37200/274360], Loss: 0.0013, Train time: 38.86\n",
      "Epoch [1/1], Step [37300/274360], Loss: 0.0075, Train time: 38.92\n",
      "Epoch [1/1], Step [37400/274360], Loss: 0.0009, Train time: 39.16\n",
      "Epoch [1/1], Step [37500/274360], Loss: 0.0485, Train time: 39.26\n",
      "Epoch [1/1], Step [37600/274360], Loss: 0.0010, Train time: 39.32\n",
      "Epoch [1/1], Step [37700/274360], Loss: 0.0022, Train time: 39.47\n",
      "Epoch [1/1], Step [37800/274360], Loss: 0.0000, Train time: 39.53\n",
      "Epoch [1/1], Step [37900/274360], Loss: 0.0010, Train time: 39.64\n",
      "Epoch [1/1], Step [38000/274360], Loss: 0.0000, Train time: 39.73\n",
      "Epoch [1/1], Step [38100/274360], Loss: 0.0000, Train time: 40.01\n",
      "Epoch [1/1], Step [38200/274360], Loss: 0.0072, Train time: 40.07\n",
      "Epoch [1/1], Step [38300/274360], Loss: 0.0014, Train time: 40.12\n",
      "Epoch [1/1], Step [38400/274360], Loss: 0.0000, Train time: 40.26\n",
      "Epoch [1/1], Step [38500/274360], Loss: 0.0131, Train time: 40.53\n",
      "Epoch [1/1], Step [38600/274360], Loss: 0.0123, Train time: 40.59\n",
      "Epoch [1/1], Step [38700/274360], Loss: 0.0115, Train time: 40.65\n",
      "Epoch [1/1], Step [38800/274360], Loss: 0.0052, Train time: 40.78\n",
      "Epoch [1/1], Step [38900/274360], Loss: 0.0002, Train time: 40.85\n",
      "Epoch [1/1], Step [39000/274360], Loss: 0.0552, Train time: 41.19\n",
      "Epoch [1/1], Step [39100/274360], Loss: 0.0027, Train time: 41.24\n",
      "Epoch [1/1], Step [39200/274360], Loss: 0.0003, Train time: 41.30\n",
      "Epoch [1/1], Step [39300/274360], Loss: 0.0018, Train time: 41.38\n",
      "Epoch [1/1], Step [39400/274360], Loss: 0.0120, Train time: 41.46\n",
      "Epoch [1/1], Step [39500/274360], Loss: 1.1934, Train time: 41.67\n",
      "Epoch [1/1], Step [39600/274360], Loss: 0.0184, Train time: 41.85\n",
      "Epoch [1/1], Step [39700/274360], Loss: 0.0003, Train time: 41.92\n",
      "Epoch [1/1], Step [39800/274360], Loss: 0.0003, Train time: 42.04\n",
      "Epoch [1/1], Step [39900/274360], Loss: 0.0048, Train time: 42.22\n",
      "Epoch [1/1], Step [40000/274360], Loss: 0.0030, Train time: 42.28\n",
      "Epoch [1/1], Step [40100/274360], Loss: 0.0116, Train time: 42.39\n",
      "Epoch [1/1], Step [40200/274360], Loss: 0.0144, Train time: 42.53\n",
      "Epoch [1/1], Step [40300/274360], Loss: 0.0006, Train time: 42.64\n",
      "Epoch [1/1], Step [40400/274360], Loss: 0.0047, Train time: 42.69\n",
      "Epoch [1/1], Step [40500/274360], Loss: 0.0011, Train time: 42.76\n",
      "Epoch [1/1], Step [40600/274360], Loss: 0.0002, Train time: 42.95\n",
      "Epoch [1/1], Step [40700/274360], Loss: 0.0013, Train time: 43.03\n",
      "Epoch [1/1], Step [40800/274360], Loss: 0.0003, Train time: 43.25\n",
      "Epoch [1/1], Step [40900/274360], Loss: 0.0036, Train time: 43.31\n",
      "Epoch [1/1], Step [41000/274360], Loss: 0.0384, Train time: 43.40\n",
      "Epoch [1/1], Step [41100/274360], Loss: 0.0003, Train time: 43.47\n",
      "Epoch [1/1], Step [41200/274360], Loss: 0.0005, Train time: 43.75\n",
      "Epoch [1/1], Step [41300/274360], Loss: 0.0010, Train time: 43.92\n",
      "Epoch [1/1], Step [41400/274360], Loss: 0.0031, Train time: 44.01\n",
      "Epoch [1/1], Step [41500/274360], Loss: 0.0106, Train time: 44.07\n",
      "Epoch [1/1], Step [41600/274360], Loss: 0.0003, Train time: 44.35\n",
      "Epoch [1/1], Step [41700/274360], Loss: 0.0002, Train time: 44.40\n",
      "Epoch [1/1], Step [41800/274360], Loss: 0.0001, Train time: 44.46\n",
      "Epoch [1/1], Step [41900/274360], Loss: 0.0147, Train time: 44.59\n",
      "Epoch [1/1], Step [42000/274360], Loss: 0.0000, Train time: 44.66\n",
      "Epoch [1/1], Step [42100/274360], Loss: 0.0206, Train time: 44.93\n",
      "Epoch [1/1], Step [42200/274360], Loss: 1.7345, Train time: 45.02\n",
      "Epoch [1/1], Step [42300/274360], Loss: 0.0232, Train time: 45.08\n",
      "Epoch [1/1], Step [42400/274360], Loss: 0.0000, Train time: 45.31\n",
      "Epoch [1/1], Step [42500/274360], Loss: 4.8498, Train time: 45.36\n",
      "Epoch [1/1], Step [42600/274360], Loss: 0.0041, Train time: 45.49\n",
      "Epoch [1/1], Step [42700/274360], Loss: 0.0008, Train time: 45.59\n",
      "Epoch [1/1], Step [42800/274360], Loss: 0.0006, Train time: 45.93\n",
      "Epoch [1/1], Step [42900/274360], Loss: 0.0001, Train time: 45.98\n",
      "Epoch [1/1], Step [43000/274360], Loss: 0.0004, Train time: 46.03\n",
      "Epoch [1/1], Step [43100/274360], Loss: 0.0000, Train time: 46.07\n",
      "Epoch [1/1], Step [43200/274360], Loss: 0.0009, Train time: 46.20\n",
      "Epoch [1/1], Step [43300/274360], Loss: 0.0006, Train time: 46.45\n",
      "Epoch [1/1], Step [43400/274360], Loss: 0.0003, Train time: 46.52\n",
      "Epoch [1/1], Step [43500/274360], Loss: 0.0123, Train time: 46.59\n",
      "Epoch [1/1], Step [43600/274360], Loss: 0.1277, Train time: 46.68\n",
      "Epoch [1/1], Step [43700/274360], Loss: 0.0001, Train time: 46.86\n",
      "Epoch [1/1], Step [43800/274360], Loss: 0.0569, Train time: 46.95\n",
      "Epoch [1/1], Step [43900/274360], Loss: 0.0008, Train time: 47.08\n",
      "Epoch [1/1], Step [44000/274360], Loss: 0.0000, Train time: 47.24\n",
      "Epoch [1/1], Step [44100/274360], Loss: 0.0160, Train time: 47.36\n",
      "Epoch [1/1], Step [44200/274360], Loss: 0.0001, Train time: 47.46\n",
      "Epoch [1/1], Step [44300/274360], Loss: 0.0000, Train time: 47.54\n",
      "Epoch [1/1], Step [44400/274360], Loss: 0.0233, Train time: 47.66\n",
      "Epoch [1/1], Step [44500/274360], Loss: 0.2093, Train time: 47.91\n",
      "Epoch [1/1], Step [44600/274360], Loss: 0.0002, Train time: 48.00\n",
      "Epoch [1/1], Step [44700/274360], Loss: 0.0006, Train time: 48.04\n",
      "Epoch [1/1], Step [44800/274360], Loss: 0.0107, Train time: 48.14\n",
      "Epoch [1/1], Step [44900/274360], Loss: 0.0004, Train time: 48.44\n",
      "Epoch [1/1], Step [45000/274360], Loss: 0.0012, Train time: 48.54\n",
      "Epoch [1/1], Step [45100/274360], Loss: 0.0025, Train time: 48.66\n",
      "Epoch [1/1], Step [45200/274360], Loss: 0.0019, Train time: 48.95\n",
      "Epoch [1/1], Step [45300/274360], Loss: 0.0100, Train time: 49.00\n",
      "Epoch [1/1], Step [45400/274360], Loss: 0.0004, Train time: 49.07\n",
      "Epoch [1/1], Step [45500/274360], Loss: 0.0003, Train time: 49.29\n",
      "Epoch [1/1], Step [45600/274360], Loss: 0.0000, Train time: 49.33\n",
      "Epoch [1/1], Step [45700/274360], Loss: 0.0311, Train time: 49.39\n",
      "Epoch [1/1], Step [45800/274360], Loss: 0.0405, Train time: 49.56\n",
      "Epoch [1/1], Step [45900/274360], Loss: 0.0001, Train time: 49.69\n",
      "Epoch [1/1], Step [46000/274360], Loss: 0.0001, Train time: 49.78\n",
      "Epoch [1/1], Step [46100/274360], Loss: 0.0077, Train time: 50.02\n",
      "Epoch [1/1], Step [46200/274360], Loss: 0.0000, Train time: 50.11\n",
      "Epoch [1/1], Step [46300/274360], Loss: 0.0016, Train time: 50.19\n",
      "Epoch [1/1], Step [46400/274360], Loss: 0.0002, Train time: 50.25\n",
      "Epoch [1/1], Step [46500/274360], Loss: 0.0174, Train time: 50.41\n",
      "Epoch [1/1], Step [46600/274360], Loss: 0.0519, Train time: 50.50\n",
      "Epoch [1/1], Step [46700/274360], Loss: 0.0000, Train time: 50.58\n",
      "Epoch [1/1], Step [46800/274360], Loss: 0.0001, Train time: 50.68\n",
      "Epoch [1/1], Step [46900/274360], Loss: 0.0000, Train time: 50.89\n",
      "Epoch [1/1], Step [47000/274360], Loss: 0.0024, Train time: 51.08\n",
      "Epoch [1/1], Step [47100/274360], Loss: 0.0001, Train time: 51.14\n",
      "Epoch [1/1], Step [47200/274360], Loss: 0.0009, Train time: 51.27\n",
      "Epoch [1/1], Step [47300/274360], Loss: 0.0060, Train time: 51.35\n",
      "Epoch [1/1], Step [47400/274360], Loss: 0.0001, Train time: 51.50\n",
      "Epoch [1/1], Step [47500/274360], Loss: 0.0012, Train time: 51.74\n",
      "Epoch [1/1], Step [47600/274360], Loss: 0.0041, Train time: 51.78\n",
      "Epoch [1/1], Step [47700/274360], Loss: 0.0005, Train time: 51.87\n",
      "Epoch [1/1], Step [47800/274360], Loss: 0.0000, Train time: 52.07\n",
      "Epoch [1/1], Step [47900/274360], Loss: 0.0062, Train time: 52.33\n",
      "Epoch [1/1], Step [48000/274360], Loss: 0.0000, Train time: 52.39\n",
      "Epoch [1/1], Step [48100/274360], Loss: 0.0000, Train time: 52.46\n",
      "Epoch [1/1], Step [48200/274360], Loss: 0.1554, Train time: 52.67\n",
      "Epoch [1/1], Step [48300/274360], Loss: 0.0514, Train time: 52.75\n",
      "Epoch [1/1], Step [48400/274360], Loss: 0.0000, Train time: 52.82\n",
      "Epoch [1/1], Step [48500/274360], Loss: 0.1984, Train time: 52.87\n",
      "Epoch [1/1], Step [48600/274360], Loss: 0.0062, Train time: 52.99\n",
      "Epoch [1/1], Step [48700/274360], Loss: 0.0389, Train time: 53.24\n",
      "Epoch [1/1], Step [48800/274360], Loss: 0.0000, Train time: 53.33\n",
      "Epoch [1/1], Step [48900/274360], Loss: 0.0002, Train time: 53.43\n",
      "Epoch [1/1], Step [49000/274360], Loss: 0.0000, Train time: 53.49\n",
      "Epoch [1/1], Step [49100/274360], Loss: 0.0025, Train time: 53.66\n",
      "Epoch [1/1], Step [49200/274360], Loss: 0.0001, Train time: 53.94\n",
      "Epoch [1/1], Step [49300/274360], Loss: 0.0027, Train time: 54.03\n",
      "Epoch [1/1], Step [49400/274360], Loss: 0.0053, Train time: 54.09\n",
      "Epoch [1/1], Step [49500/274360], Loss: 0.0623, Train time: 54.17\n",
      "Epoch [1/1], Step [49600/274360], Loss: 0.0037, Train time: 54.50\n",
      "Epoch [1/1], Step [49700/274360], Loss: 0.0391, Train time: 54.56\n",
      "Epoch [1/1], Step [49800/274360], Loss: 0.0003, Train time: 54.61\n",
      "Epoch [1/1], Step [49900/274360], Loss: 0.0556, Train time: 54.66\n",
      "Epoch [1/1], Step [50000/274360], Loss: 0.0026, Train time: 54.83\n",
      "Epoch [1/1], Step [50100/274360], Loss: 0.0000, Train time: 54.90\n",
      "Epoch [1/1], Step [50200/274360], Loss: 0.0450, Train time: 54.95\n",
      "Epoch [1/1], Step [50300/274360], Loss: 0.0009, Train time: 54.99\n",
      "Epoch [1/1], Step [50400/274360], Loss: 0.0003, Train time: 55.32\n",
      "Epoch [1/1], Step [50500/274360], Loss: 0.0001, Train time: 55.51\n",
      "Epoch [1/1], Step [50600/274360], Loss: 0.0001, Train time: 55.59\n",
      "Epoch [1/1], Step [50700/274360], Loss: 0.0000, Train time: 55.67\n",
      "Epoch [1/1], Step [50800/274360], Loss: 0.0000, Train time: 55.77\n",
      "Epoch [1/1], Step [50900/274360], Loss: 0.0049, Train time: 56.07\n",
      "Epoch [1/1], Step [51000/274360], Loss: 0.0003, Train time: 56.14\n",
      "Epoch [1/1], Step [51100/274360], Loss: 0.0001, Train time: 56.19\n",
      "Epoch [1/1], Step [51200/274360], Loss: 0.0020, Train time: 56.43\n",
      "Epoch [1/1], Step [51300/274360], Loss: 0.0384, Train time: 56.53\n",
      "Epoch [1/1], Step [51400/274360], Loss: 0.0102, Train time: 56.63\n",
      "Epoch [1/1], Step [51500/274360], Loss: 0.0240, Train time: 56.75\n",
      "Epoch [1/1], Step [51600/274360], Loss: 0.0290, Train time: 56.86\n",
      "Epoch [1/1], Step [51700/274360], Loss: 0.0000, Train time: 57.09\n",
      "Epoch [1/1], Step [51800/274360], Loss: 0.0018, Train time: 57.14\n",
      "Epoch [1/1], Step [51900/274360], Loss: 0.0000, Train time: 57.22\n",
      "Epoch [1/1], Step [52000/274360], Loss: 0.0000, Train time: 57.31\n",
      "Epoch [1/1], Step [52100/274360], Loss: 0.0000, Train time: 57.37\n",
      "Epoch [1/1], Step [52200/274360], Loss: 0.0271, Train time: 57.54\n",
      "Epoch [1/1], Step [52300/274360], Loss: 0.1267, Train time: 57.63\n",
      "Epoch [1/1], Step [52400/274360], Loss: 0.0008, Train time: 57.71\n",
      "Epoch [1/1], Step [52500/274360], Loss: 0.0037, Train time: 57.82\n",
      "Epoch [1/1], Step [52600/274360], Loss: 0.0012, Train time: 58.06\n",
      "Epoch [1/1], Step [52700/274360], Loss: 0.0001, Train time: 58.14\n",
      "Epoch [1/1], Step [52800/274360], Loss: 0.0001, Train time: 58.18\n",
      "Epoch [1/1], Step [52900/274360], Loss: 0.0004, Train time: 58.30\n",
      "Epoch [1/1], Step [53000/274360], Loss: 0.0336, Train time: 58.44\n",
      "Epoch [1/1], Step [53100/274360], Loss: 0.0009, Train time: 58.53\n",
      "Epoch [1/1], Step [53200/274360], Loss: 0.0000, Train time: 58.68\n",
      "Epoch [1/1], Step [53300/274360], Loss: 0.0003, Train time: 58.77\n",
      "Epoch [1/1], Step [53400/274360], Loss: 0.0021, Train time: 59.15\n",
      "Epoch [1/1], Step [53500/274360], Loss: 0.0003, Train time: 59.21\n",
      "Epoch [1/1], Step [53600/274360], Loss: 0.0000, Train time: 59.26\n",
      "Epoch [1/1], Step [53700/274360], Loss: 0.0158, Train time: 59.31\n",
      "Epoch [1/1], Step [53800/274360], Loss: 0.0000, Train time: 59.54\n",
      "Epoch [1/1], Step [53900/274360], Loss: 0.0000, Train time: 59.70\n",
      "Epoch [1/1], Step [54000/274360], Loss: 0.0015, Train time: 59.78\n",
      "Epoch [1/1], Step [54100/274360], Loss: 0.0015, Train time: 59.97\n",
      "Epoch [1/1], Step [54200/274360], Loss: 0.0015, Train time: 60.02\n",
      "Epoch [1/1], Step [54300/274360], Loss: 0.0083, Train time: 60.13\n",
      "Epoch [1/1], Step [54400/274360], Loss: 0.0111, Train time: 60.19\n",
      "Epoch [1/1], Step [54500/274360], Loss: 0.0000, Train time: 60.42\n",
      "Epoch [1/1], Step [54600/274360], Loss: 0.0066, Train time: 60.48\n",
      "Epoch [1/1], Step [54700/274360], Loss: 0.0002, Train time: 60.53\n",
      "Epoch [1/1], Step [54800/274360], Loss: 0.0005, Train time: 60.64\n",
      "Epoch [1/1], Step [54900/274360], Loss: 0.0118, Train time: 60.89\n",
      "Epoch [1/1], Step [55000/274360], Loss: 0.0007, Train time: 60.98\n",
      "Epoch [1/1], Step [55100/274360], Loss: 0.0000, Train time: 61.08\n",
      "Epoch [1/1], Step [55200/274360], Loss: 0.0001, Train time: 61.38\n",
      "Epoch [1/1], Step [55300/274360], Loss: 0.1184, Train time: 61.50\n",
      "Epoch [1/1], Step [55400/274360], Loss: 0.0000, Train time: 61.59\n",
      "Epoch [1/1], Step [55500/274360], Loss: 0.0011, Train time: 61.66\n",
      "Epoch [1/1], Step [55600/274360], Loss: 0.0000, Train time: 61.96\n",
      "Epoch [1/1], Step [55700/274360], Loss: 0.0002, Train time: 62.12\n",
      "Epoch [1/1], Step [55800/274360], Loss: 0.0005, Train time: 62.21\n",
      "Epoch [1/1], Step [55900/274360], Loss: 0.0139, Train time: 62.29\n",
      "Epoch [1/1], Step [56000/274360], Loss: 0.0000, Train time: 62.51\n",
      "Epoch [1/1], Step [56100/274360], Loss: 0.0024, Train time: 62.57\n",
      "Epoch [1/1], Step [56200/274360], Loss: 0.0006, Train time: 62.64\n",
      "Epoch [1/1], Step [56300/274360], Loss: 0.0057, Train time: 62.70\n",
      "Epoch [1/1], Step [56400/274360], Loss: 0.0021, Train time: 62.87\n",
      "Epoch [1/1], Step [56500/274360], Loss: 0.0003, Train time: 62.92\n",
      "Epoch [1/1], Step [56600/274360], Loss: 0.1117, Train time: 62.99\n",
      "Epoch [1/1], Step [56700/274360], Loss: 0.0000, Train time: 63.08\n",
      "Epoch [1/1], Step [56800/274360], Loss: 0.0000, Train time: 63.29\n",
      "Epoch [1/1], Step [56900/274360], Loss: 0.0000, Train time: 63.36\n",
      "Epoch [1/1], Step [57000/274360], Loss: 0.0166, Train time: 63.45\n",
      "Epoch [1/1], Step [57100/274360], Loss: 0.0000, Train time: 63.55\n",
      "Epoch [1/1], Step [57200/274360], Loss: 0.0097, Train time: 63.64\n",
      "Epoch [1/1], Step [57300/274360], Loss: 0.0002, Train time: 63.93\n",
      "Epoch [1/1], Step [57400/274360], Loss: 0.0000, Train time: 64.06\n",
      "Epoch [1/1], Step [57500/274360], Loss: 0.0955, Train time: 64.14\n",
      "Epoch [1/1], Step [57600/274360], Loss: 0.0014, Train time: 64.24\n",
      "Epoch [1/1], Step [57700/274360], Loss: 0.0016, Train time: 64.47\n",
      "Epoch [1/1], Step [57800/274360], Loss: 0.0290, Train time: 64.62\n",
      "Epoch [1/1], Step [57900/274360], Loss: 0.0001, Train time: 64.67\n",
      "Epoch [1/1], Step [58000/274360], Loss: 0.0158, Train time: 64.72\n",
      "Epoch [1/1], Step [58100/274360], Loss: 0.0000, Train time: 64.82\n",
      "Epoch [1/1], Step [58200/274360], Loss: 0.0000, Train time: 64.93\n",
      "Epoch [1/1], Step [58300/274360], Loss: 0.0017, Train time: 65.12\n",
      "Epoch [1/1], Step [58400/274360], Loss: 0.0003, Train time: 65.24\n",
      "Epoch [1/1], Step [58500/274360], Loss: 0.0279, Train time: 65.33\n",
      "Epoch [1/1], Step [58600/274360], Loss: 0.0024, Train time: 65.39\n",
      "Epoch [1/1], Step [58700/274360], Loss: 0.0932, Train time: 65.72\n",
      "Epoch [1/1], Step [58800/274360], Loss: 0.0165, Train time: 65.79\n",
      "Epoch [1/1], Step [58900/274360], Loss: 0.0000, Train time: 65.87\n",
      "Epoch [1/1], Step [59000/274360], Loss: 0.0010, Train time: 65.96\n",
      "Epoch [1/1], Step [59100/274360], Loss: 0.0024, Train time: 66.06\n",
      "Epoch [1/1], Step [59200/274360], Loss: 0.0000, Train time: 66.35\n",
      "Epoch [1/1], Step [59300/274360], Loss: 0.0002, Train time: 66.41\n",
      "Epoch [1/1], Step [59400/274360], Loss: 0.0107, Train time: 66.49\n",
      "Epoch [1/1], Step [59500/274360], Loss: 0.0004, Train time: 66.60\n",
      "Epoch [1/1], Step [59600/274360], Loss: 0.0001, Train time: 66.77\n",
      "Epoch [1/1], Step [59700/274360], Loss: 0.0199, Train time: 66.86\n",
      "Epoch [1/1], Step [59800/274360], Loss: 0.0054, Train time: 66.99\n",
      "Epoch [1/1], Step [59900/274360], Loss: 0.0001, Train time: 67.10\n",
      "Epoch [1/1], Step [60000/274360], Loss: 0.0002, Train time: 67.19\n",
      "Epoch [1/1], Step [60100/274360], Loss: 0.0008, Train time: 67.57\n",
      "Epoch [1/1], Step [60200/274360], Loss: 0.0000, Train time: 67.64\n",
      "Epoch [1/1], Step [60300/274360], Loss: 0.0000, Train time: 67.69\n",
      "Epoch [1/1], Step [60400/274360], Loss: 0.0001, Train time: 67.81\n",
      "Epoch [1/1], Step [60500/274360], Loss: 0.0115, Train time: 67.96\n",
      "Epoch [1/1], Step [60600/274360], Loss: 0.0000, Train time: 68.12\n",
      "Epoch [1/1], Step [60700/274360], Loss: 0.0001, Train time: 68.16\n",
      "Epoch [1/1], Step [60800/274360], Loss: 0.0006, Train time: 68.24\n",
      "Epoch [1/1], Step [60900/274360], Loss: 0.0003, Train time: 68.31\n",
      "Epoch [1/1], Step [61000/274360], Loss: 0.0006, Train time: 68.54\n",
      "Epoch [1/1], Step [61100/274360], Loss: 0.0350, Train time: 68.64\n",
      "Epoch [1/1], Step [61200/274360], Loss: 0.0003, Train time: 68.71\n",
      "Epoch [1/1], Step [61300/274360], Loss: 0.0014, Train time: 68.79\n",
      "Epoch [1/1], Step [61400/274360], Loss: 0.0138, Train time: 68.91\n",
      "Epoch [1/1], Step [61500/274360], Loss: 0.0115, Train time: 69.29\n",
      "Epoch [1/1], Step [61600/274360], Loss: 0.0000, Train time: 69.38\n",
      "Epoch [1/1], Step [61700/274360], Loss: 0.0004, Train time: 69.44\n",
      "Epoch [1/1], Step [61800/274360], Loss: 0.0014, Train time: 69.68\n",
      "Epoch [1/1], Step [61900/274360], Loss: 0.0000, Train time: 69.72\n",
      "Epoch [1/1], Step [62000/274360], Loss: 0.0014, Train time: 69.77\n",
      "Epoch [1/1], Step [62100/274360], Loss: 0.0002, Train time: 69.97\n",
      "Epoch [1/1], Step [62200/274360], Loss: 0.0000, Train time: 70.19\n",
      "Epoch [1/1], Step [62300/274360], Loss: 0.0012, Train time: 70.34\n",
      "Epoch [1/1], Step [62400/274360], Loss: 0.0004, Train time: 70.54\n",
      "Epoch [1/1], Step [62500/274360], Loss: 0.0001, Train time: 70.64\n",
      "Epoch [1/1], Step [62600/274360], Loss: 0.0001, Train time: 70.72\n",
      "Epoch [1/1], Step [62700/274360], Loss: 0.0000, Train time: 70.80\n",
      "Epoch [1/1], Step [62800/274360], Loss: 0.0000, Train time: 71.09\n",
      "Epoch [1/1], Step [62900/274360], Loss: 0.0001, Train time: 71.14\n",
      "Epoch [1/1], Step [63000/274360], Loss: 0.0005, Train time: 71.23\n",
      "Epoch [1/1], Step [63100/274360], Loss: 3.8818, Train time: 71.34\n",
      "Epoch [1/1], Step [63200/274360], Loss: 0.0012, Train time: 71.39\n",
      "Epoch [1/1], Step [63300/274360], Loss: 0.0058, Train time: 71.50\n",
      "Epoch [1/1], Step [63400/274360], Loss: 0.0012, Train time: 71.87\n",
      "Epoch [1/1], Step [63500/274360], Loss: 0.0000, Train time: 71.94\n",
      "Epoch [1/1], Step [63600/274360], Loss: 0.0003, Train time: 71.99\n",
      "Epoch [1/1], Step [63700/274360], Loss: 0.0002, Train time: 72.08\n",
      "Epoch [1/1], Step [63800/274360], Loss: 0.0000, Train time: 72.29\n",
      "Epoch [1/1], Step [63900/274360], Loss: 0.0012, Train time: 72.34\n",
      "Epoch [1/1], Step [64000/274360], Loss: 0.0000, Train time: 72.40\n",
      "Epoch [1/1], Step [64100/274360], Loss: 0.0001, Train time: 72.50\n",
      "Epoch [1/1], Step [64200/274360], Loss: 0.0001, Train time: 72.67\n",
      "Epoch [1/1], Step [64300/274360], Loss: 0.0038, Train time: 72.75\n",
      "Epoch [1/1], Step [64400/274360], Loss: 0.0004, Train time: 73.05\n",
      "Epoch [1/1], Step [64500/274360], Loss: 0.0000, Train time: 73.12\n",
      "Epoch [1/1], Step [64600/274360], Loss: 0.0077, Train time: 73.17\n",
      "Epoch [1/1], Step [64700/274360], Loss: 0.0000, Train time: 73.26\n",
      "Epoch [1/1], Step [64800/274360], Loss: 0.0000, Train time: 73.37\n",
      "Epoch [1/1], Step [64900/274360], Loss: 0.0002, Train time: 73.59\n",
      "Epoch [1/1], Step [65000/274360], Loss: 0.0208, Train time: 73.78\n",
      "Epoch [1/1], Step [65100/274360], Loss: 0.0030, Train time: 73.88\n",
      "Epoch [1/1], Step [65200/274360], Loss: 0.0001, Train time: 73.97\n",
      "Epoch [1/1], Step [65300/274360], Loss: 0.0001, Train time: 74.24\n",
      "Epoch [1/1], Step [65400/274360], Loss: 0.0007, Train time: 74.48\n",
      "Epoch [1/1], Step [65500/274360], Loss: 0.0102, Train time: 74.56\n",
      "Epoch [1/1], Step [65600/274360], Loss: 0.0582, Train time: 74.70\n",
      "Epoch [1/1], Step [65700/274360], Loss: 0.0006, Train time: 74.94\n",
      "Epoch [1/1], Step [65800/274360], Loss: 0.0015, Train time: 74.98\n",
      "Epoch [1/1], Step [65900/274360], Loss: 0.0092, Train time: 75.02\n",
      "Epoch [1/1], Step [66000/274360], Loss: 0.0010, Train time: 75.07\n",
      "Epoch [1/1], Step [66100/274360], Loss: 0.0006, Train time: 75.47\n",
      "Epoch [1/1], Step [66200/274360], Loss: 0.0001, Train time: 75.69\n",
      "Epoch [1/1], Step [66300/274360], Loss: 0.0023, Train time: 75.76\n",
      "Epoch [1/1], Step [66400/274360], Loss: 0.0000, Train time: 75.81\n",
      "Epoch [1/1], Step [66500/274360], Loss: 0.0000, Train time: 76.03\n",
      "Epoch [1/1], Step [66600/274360], Loss: 0.0001, Train time: 76.17\n",
      "Epoch [1/1], Step [66700/274360], Loss: 0.0000, Train time: 76.22\n",
      "Epoch [1/1], Step [66800/274360], Loss: 0.0003, Train time: 76.29\n",
      "Epoch [1/1], Step [66900/274360], Loss: 0.0000, Train time: 76.44\n",
      "Epoch [1/1], Step [67000/274360], Loss: 0.0011, Train time: 76.71\n",
      "Epoch [1/1], Step [67100/274360], Loss: 0.0000, Train time: 76.82\n",
      "Epoch [1/1], Step [67200/274360], Loss: 0.0009, Train time: 76.88\n",
      "Epoch [1/1], Step [67300/274360], Loss: 0.0002, Train time: 76.98\n",
      "Epoch [1/1], Step [67400/274360], Loss: 0.0000, Train time: 77.16\n",
      "Epoch [1/1], Step [67500/274360], Loss: 0.0018, Train time: 77.34\n",
      "Epoch [1/1], Step [67600/274360], Loss: 0.0009, Train time: 77.52\n",
      "Epoch [1/1], Step [67700/274360], Loss: 0.0002, Train time: 77.78\n",
      "Epoch [1/1], Step [67800/274360], Loss: 0.0009, Train time: 77.94\n",
      "Epoch [1/1], Step [67900/274360], Loss: 0.0100, Train time: 78.02\n",
      "Epoch [1/1], Step [68000/274360], Loss: 0.0000, Train time: 78.10\n",
      "Epoch [1/1], Step [68100/274360], Loss: 0.0000, Train time: 78.30\n",
      "Epoch [1/1], Step [68200/274360], Loss: 0.0001, Train time: 78.46\n",
      "Epoch [1/1], Step [68300/274360], Loss: 0.6766, Train time: 78.52\n",
      "Epoch [1/1], Step [68400/274360], Loss: 0.0001, Train time: 78.58\n",
      "Epoch [1/1], Step [68500/274360], Loss: 0.0000, Train time: 78.76\n",
      "Epoch [1/1], Step [68600/274360], Loss: 0.0019, Train time: 78.86\n",
      "Epoch [1/1], Step [68700/274360], Loss: 0.0001, Train time: 78.93\n",
      "Epoch [1/1], Step [68800/274360], Loss: 0.0003, Train time: 78.98\n",
      "Epoch [1/1], Step [68900/274360], Loss: 0.0014, Train time: 79.22\n",
      "Epoch [1/1], Step [69000/274360], Loss: 0.0000, Train time: 79.44\n",
      "Epoch [1/1], Step [69100/274360], Loss: 0.0018, Train time: 79.57\n",
      "Epoch [1/1], Step [69200/274360], Loss: 0.0000, Train time: 79.68\n",
      "Epoch [1/1], Step [69300/274360], Loss: 0.0000, Train time: 79.95\n",
      "Epoch [1/1], Step [69400/274360], Loss: 0.0119, Train time: 80.03\n",
      "Epoch [1/1], Step [69500/274360], Loss: 0.0002, Train time: 80.13\n",
      "Epoch [1/1], Step [69600/274360], Loss: 0.1770, Train time: 80.32\n",
      "Epoch [1/1], Step [69700/274360], Loss: 0.0000, Train time: 80.55\n",
      "Epoch [1/1], Step [69800/274360], Loss: 0.0018, Train time: 80.63\n",
      "Epoch [1/1], Step [69900/274360], Loss: 0.0001, Train time: 80.68\n",
      "Epoch [1/1], Step [70000/274360], Loss: 0.0003, Train time: 80.74\n",
      "Epoch [1/1], Step [70100/274360], Loss: 0.0031, Train time: 81.06\n",
      "Epoch [1/1], Step [70200/274360], Loss: 0.1234, Train time: 81.12\n",
      "Epoch [1/1], Step [70300/274360], Loss: 0.0000, Train time: 81.17\n",
      "Epoch [1/1], Step [70400/274360], Loss: 0.0003, Train time: 81.33\n",
      "Epoch [1/1], Step [70500/274360], Loss: 0.0188, Train time: 81.58\n",
      "Epoch [1/1], Step [70600/274360], Loss: 0.0013, Train time: 81.63\n",
      "Epoch [1/1], Step [70700/274360], Loss: 0.0005, Train time: 81.71\n",
      "Epoch [1/1], Step [70800/274360], Loss: 0.0058, Train time: 81.93\n",
      "Epoch [1/1], Step [70900/274360], Loss: 0.0219, Train time: 81.99\n",
      "Epoch [1/1], Step [71000/274360], Loss: 0.0003, Train time: 82.23\n",
      "Epoch [1/1], Step [71100/274360], Loss: 0.0000, Train time: 82.29\n",
      "Epoch [1/1], Step [71200/274360], Loss: 0.0000, Train time: 82.38\n",
      "Epoch [1/1], Step [71300/274360], Loss: 0.0021, Train time: 82.70\n",
      "Epoch [1/1], Step [71400/274360], Loss: 0.0118, Train time: 82.82\n",
      "Epoch [1/1], Step [71500/274360], Loss: 0.0000, Train time: 82.92\n",
      "Epoch [1/1], Step [71600/274360], Loss: 0.0003, Train time: 83.17\n",
      "Epoch [1/1], Step [71700/274360], Loss: 0.0826, Train time: 83.27\n",
      "Epoch [1/1], Step [71800/274360], Loss: 0.0000, Train time: 83.34\n",
      "Epoch [1/1], Step [71900/274360], Loss: 0.0009, Train time: 83.59\n",
      "Epoch [1/1], Step [72000/274360], Loss: 0.0132, Train time: 83.71\n",
      "Epoch [1/1], Step [72100/274360], Loss: 0.0000, Train time: 83.81\n",
      "Epoch [1/1], Step [72200/274360], Loss: 0.0001, Train time: 84.05\n",
      "Epoch [1/1], Step [72300/274360], Loss: 0.0215, Train time: 84.19\n",
      "Epoch [1/1], Step [72400/274360], Loss: 0.0009, Train time: 84.29\n",
      "Epoch [1/1], Step [72500/274360], Loss: 0.0005, Train time: 84.35\n",
      "Epoch [1/1], Step [72600/274360], Loss: 0.0034, Train time: 84.45\n",
      "Epoch [1/1], Step [72700/274360], Loss: 0.0008, Train time: 84.58\n",
      "Epoch [1/1], Step [72800/274360], Loss: 0.0964, Train time: 84.97\n",
      "Epoch [1/1], Step [72900/274360], Loss: 0.0084, Train time: 85.02\n",
      "Epoch [1/1], Step [73000/274360], Loss: 0.0000, Train time: 85.13\n",
      "Epoch [1/1], Step [73100/274360], Loss: 0.0000, Train time: 85.27\n",
      "Epoch [1/1], Step [73200/274360], Loss: 0.0000, Train time: 85.36\n",
      "Epoch [1/1], Step [73300/274360], Loss: 0.0098, Train time: 85.49\n",
      "Epoch [1/1], Step [73400/274360], Loss: 0.0000, Train time: 85.55\n",
      "Epoch [1/1], Step [73500/274360], Loss: 0.0000, Train time: 85.75\n",
      "Epoch [1/1], Step [73600/274360], Loss: 0.0000, Train time: 85.85\n",
      "Epoch [1/1], Step [73700/274360], Loss: 0.0241, Train time: 85.95\n",
      "Epoch [1/1], Step [73800/274360], Loss: 0.0000, Train time: 86.24\n",
      "Epoch [1/1], Step [73900/274360], Loss: 0.0001, Train time: 86.31\n",
      "Epoch [1/1], Step [74000/274360], Loss: 0.0004, Train time: 86.36\n",
      "Epoch [1/1], Step [74100/274360], Loss: 0.0006, Train time: 86.42\n",
      "Epoch [1/1], Step [74200/274360], Loss: 0.0076, Train time: 86.52\n",
      "Epoch [1/1], Step [74300/274360], Loss: 0.2975, Train time: 86.71\n",
      "Epoch [1/1], Step [74400/274360], Loss: 0.0005, Train time: 86.75\n",
      "Epoch [1/1], Step [74500/274360], Loss: 0.0019, Train time: 86.96\n",
      "Epoch [1/1], Step [74600/274360], Loss: 0.0003, Train time: 87.23\n",
      "Epoch [1/1], Step [74700/274360], Loss: 0.0001, Train time: 87.31\n",
      "Epoch [1/1], Step [74800/274360], Loss: 0.0003, Train time: 87.46\n",
      "Epoch [1/1], Step [74900/274360], Loss: 0.0025, Train time: 87.57\n",
      "Epoch [1/1], Step [75000/274360], Loss: 0.0002, Train time: 87.75\n",
      "Epoch [1/1], Step [75100/274360], Loss: 0.0000, Train time: 87.93\n",
      "Epoch [1/1], Step [75200/274360], Loss: 0.0000, Train time: 87.98\n",
      "Epoch [1/1], Step [75300/274360], Loss: 0.0001, Train time: 88.05\n",
      "Epoch [1/1], Step [75400/274360], Loss: 0.0001, Train time: 88.16\n",
      "Epoch [1/1], Step [75500/274360], Loss: 0.0000, Train time: 88.28\n",
      "Epoch [1/1], Step [75600/274360], Loss: 0.0000, Train time: 88.53\n",
      "Epoch [1/1], Step [75700/274360], Loss: 0.0021, Train time: 88.57\n",
      "Epoch [1/1], Step [75800/274360], Loss: 0.0000, Train time: 88.62\n",
      "Epoch [1/1], Step [75900/274360], Loss: 0.0002, Train time: 88.67\n",
      "Epoch [1/1], Step [76000/274360], Loss: 0.0000, Train time: 88.79\n",
      "Epoch [1/1], Step [76100/274360], Loss: 0.0000, Train time: 88.86\n",
      "Epoch [1/1], Step [76200/274360], Loss: 0.0000, Train time: 89.28\n",
      "Epoch [1/1], Step [76300/274360], Loss: 0.0000, Train time: 89.38\n",
      "Epoch [1/1], Step [76400/274360], Loss: 0.0000, Train time: 89.43\n",
      "Epoch [1/1], Step [76500/274360], Loss: 0.0430, Train time: 89.51\n",
      "Epoch [1/1], Step [76600/274360], Loss: 0.0001, Train time: 89.70\n",
      "Epoch [1/1], Step [76700/274360], Loss: 0.6696, Train time: 89.93\n",
      "Epoch [1/1], Step [76800/274360], Loss: 0.0003, Train time: 89.99\n",
      "Epoch [1/1], Step [76900/274360], Loss: 0.0000, Train time: 90.07\n",
      "Epoch [1/1], Step [77000/274360], Loss: 0.0065, Train time: 90.18\n",
      "Epoch [1/1], Step [77100/274360], Loss: 0.0100, Train time: 90.34\n",
      "Epoch [1/1], Step [77200/274360], Loss: 0.0975, Train time: 90.41\n",
      "Epoch [1/1], Step [77300/274360], Loss: 0.0001, Train time: 90.46\n",
      "Epoch [1/1], Step [77400/274360], Loss: 0.0077, Train time: 90.59\n",
      "Epoch [1/1], Step [77500/274360], Loss: 2.2523, Train time: 90.84\n",
      "Epoch [1/1], Step [77600/274360], Loss: 0.0052, Train time: 90.92\n",
      "Epoch [1/1], Step [77700/274360], Loss: 0.0000, Train time: 91.36\n",
      "Epoch [1/1], Step [77800/274360], Loss: 0.0000, Train time: 91.44\n",
      "Epoch [1/1], Step [77900/274360], Loss: 0.0000, Train time: 91.48\n",
      "Epoch [1/1], Step [78000/274360], Loss: 0.2996, Train time: 91.71\n",
      "Epoch [1/1], Step [78100/274360], Loss: 0.0008, Train time: 91.82\n",
      "Epoch [1/1], Step [78200/274360], Loss: 0.0003, Train time: 92.15\n",
      "Epoch [1/1], Step [78300/274360], Loss: 0.0000, Train time: 92.21\n",
      "Epoch [1/1], Step [78400/274360], Loss: 0.0066, Train time: 92.32\n",
      "Epoch [1/1], Step [78500/274360], Loss: 0.0002, Train time: 92.40\n",
      "Epoch [1/1], Step [78600/274360], Loss: 0.0000, Train time: 92.47\n",
      "Epoch [1/1], Step [78700/274360], Loss: 0.0008, Train time: 92.84\n",
      "Epoch [1/1], Step [78800/274360], Loss: 0.0001, Train time: 92.90\n",
      "Epoch [1/1], Step [78900/274360], Loss: 0.0026, Train time: 92.97\n",
      "Epoch [1/1], Step [79000/274360], Loss: 0.0092, Train time: 93.10\n",
      "Epoch [1/1], Step [79100/274360], Loss: 0.0002, Train time: 93.15\n",
      "Epoch [1/1], Step [79200/274360], Loss: 0.0012, Train time: 93.50\n",
      "Epoch [1/1], Step [79300/274360], Loss: 0.0002, Train time: 93.70\n",
      "Epoch [1/1], Step [79400/274360], Loss: 0.0020, Train time: 93.75\n",
      "Epoch [1/1], Step [79500/274360], Loss: 0.0149, Train time: 93.79\n",
      "Epoch [1/1], Step [79600/274360], Loss: 0.0024, Train time: 93.85\n",
      "Epoch [1/1], Step [79700/274360], Loss: 0.0012, Train time: 94.04\n",
      "Epoch [1/1], Step [79800/274360], Loss: 0.0083, Train time: 94.13\n",
      "Epoch [1/1], Step [79900/274360], Loss: 0.0001, Train time: 94.20\n",
      "Epoch [1/1], Step [80000/274360], Loss: 0.0019, Train time: 94.25\n",
      "Epoch [1/1], Step [80100/274360], Loss: 0.0046, Train time: 94.36\n",
      "Epoch [1/1], Step [80200/274360], Loss: 0.0001, Train time: 94.44\n",
      "Epoch [1/1], Step [80300/274360], Loss: 0.0010, Train time: 94.66\n",
      "Epoch [1/1], Step [80400/274360], Loss: 0.0000, Train time: 94.74\n",
      "Epoch [1/1], Step [80500/274360], Loss: 0.0000, Train time: 94.94\n",
      "Epoch [1/1], Step [80600/274360], Loss: 0.0000, Train time: 95.00\n",
      "Epoch [1/1], Step [80700/274360], Loss: 0.0000, Train time: 95.17\n",
      "Epoch [1/1], Step [80800/274360], Loss: 0.0000, Train time: 95.27\n",
      "Epoch [1/1], Step [80900/274360], Loss: 0.0103, Train time: 95.34\n",
      "Epoch [1/1], Step [81000/274360], Loss: 0.0014, Train time: 95.45\n",
      "Epoch [1/1], Step [81100/274360], Loss: 0.0002, Train time: 95.54\n",
      "Epoch [1/1], Step [81200/274360], Loss: 0.0018, Train time: 95.60\n",
      "Epoch [1/1], Step [81300/274360], Loss: 0.0011, Train time: 95.69\n",
      "Epoch [1/1], Step [81400/274360], Loss: 0.0001, Train time: 95.80\n",
      "Epoch [1/1], Step [81500/274360], Loss: 0.0044, Train time: 96.07\n",
      "Epoch [1/1], Step [81600/274360], Loss: 0.0000, Train time: 96.14\n",
      "Epoch [1/1], Step [81700/274360], Loss: 0.0317, Train time: 96.24\n",
      "Epoch [1/1], Step [81800/274360], Loss: 0.0001, Train time: 96.31\n",
      "Epoch [1/1], Step [81900/274360], Loss: 0.0000, Train time: 96.44\n",
      "Epoch [1/1], Step [82000/274360], Loss: 0.0000, Train time: 96.51\n",
      "Epoch [1/1], Step [82100/274360], Loss: 0.0005, Train time: 96.72\n",
      "Epoch [1/1], Step [82200/274360], Loss: 0.0001, Train time: 96.79\n",
      "Epoch [1/1], Step [82300/274360], Loss: 0.1143, Train time: 96.85\n",
      "Epoch [1/1], Step [82400/274360], Loss: 0.0001, Train time: 97.01\n",
      "Epoch [1/1], Step [82500/274360], Loss: 0.0004, Train time: 97.05\n",
      "Epoch [1/1], Step [82600/274360], Loss: 0.0005, Train time: 97.10\n",
      "Epoch [1/1], Step [82700/274360], Loss: 0.0001, Train time: 97.16\n",
      "Epoch [1/1], Step [82800/274360], Loss: 0.0015, Train time: 97.38\n",
      "Epoch [1/1], Step [82900/274360], Loss: 0.0006, Train time: 97.45\n",
      "Epoch [1/1], Step [83000/274360], Loss: 0.0015, Train time: 97.50\n",
      "Epoch [1/1], Step [83100/274360], Loss: 0.0003, Train time: 97.57\n",
      "Epoch [1/1], Step [83200/274360], Loss: 0.0003, Train time: 97.63\n",
      "Epoch [1/1], Step [83300/274360], Loss: 0.0016, Train time: 97.84\n",
      "Epoch [1/1], Step [83400/274360], Loss: 0.0000, Train time: 97.92\n",
      "Epoch [1/1], Step [83500/274360], Loss: 0.0006, Train time: 97.97\n",
      "Epoch [1/1], Step [83600/274360], Loss: 0.0005, Train time: 98.29\n",
      "Epoch [1/1], Step [83700/274360], Loss: 0.0000, Train time: 98.41\n",
      "Epoch [1/1], Step [83800/274360], Loss: 0.0001, Train time: 98.49\n",
      "Epoch [1/1], Step [83900/274360], Loss: 0.0095, Train time: 98.54\n",
      "Epoch [1/1], Step [84000/274360], Loss: 0.0017, Train time: 98.65\n",
      "Epoch [1/1], Step [84100/274360], Loss: 0.0522, Train time: 98.74\n",
      "Epoch [1/1], Step [84200/274360], Loss: 0.0007, Train time: 98.85\n",
      "Epoch [1/1], Step [84300/274360], Loss: 0.0007, Train time: 99.03\n",
      "Epoch [1/1], Step [84400/274360], Loss: 0.0077, Train time: 99.16\n",
      "Epoch [1/1], Step [84500/274360], Loss: 0.0073, Train time: 99.21\n",
      "Epoch [1/1], Step [84600/274360], Loss: 0.0000, Train time: 99.25\n",
      "Epoch [1/1], Step [84700/274360], Loss: 0.0000, Train time: 99.38\n",
      "Epoch [1/1], Step [84800/274360], Loss: 0.0002, Train time: 99.42\n",
      "Epoch [1/1], Step [84900/274360], Loss: 0.0000, Train time: 99.46\n",
      "Epoch [1/1], Step [85000/274360], Loss: 0.0025, Train time: 99.50\n",
      "Epoch [1/1], Step [85100/274360], Loss: 0.0003, Train time: 99.66\n",
      "Epoch [1/1], Step [85200/274360], Loss: 0.0000, Train time: 99.84\n",
      "Epoch [1/1], Step [85300/274360], Loss: 0.0016, Train time: 99.95\n",
      "Epoch [1/1], Step [85400/274360], Loss: 0.0002, Train time: 100.03\n",
      "Epoch [1/1], Step [85500/274360], Loss: 0.0025, Train time: 100.09\n",
      "Epoch [1/1], Step [85600/274360], Loss: 0.0000, Train time: 100.28\n",
      "Epoch [1/1], Step [85700/274360], Loss: 0.0002, Train time: 100.33\n",
      "Epoch [1/1], Step [85800/274360], Loss: 0.0000, Train time: 100.38\n",
      "Epoch [1/1], Step [85900/274360], Loss: 0.0000, Train time: 100.42\n",
      "Epoch [1/1], Step [86000/274360], Loss: 0.0005, Train time: 100.59\n",
      "Epoch [1/1], Step [86100/274360], Loss: 0.0015, Train time: 100.64\n",
      "Epoch [1/1], Step [86200/274360], Loss: 0.0001, Train time: 100.73\n",
      "Epoch [1/1], Step [86300/274360], Loss: 0.0001, Train time: 100.80\n",
      "Epoch [1/1], Step [86400/274360], Loss: 0.0045, Train time: 100.88\n",
      "Epoch [1/1], Step [86500/274360], Loss: 0.0005, Train time: 101.10\n",
      "Epoch [1/1], Step [86600/274360], Loss: 0.0003, Train time: 101.14\n",
      "Epoch [1/1], Step [86700/274360], Loss: 0.0023, Train time: 101.20\n",
      "Epoch [1/1], Step [86800/274360], Loss: 0.0000, Train time: 101.28\n",
      "Epoch [1/1], Step [86900/274360], Loss: 0.0009, Train time: 101.56\n",
      "Epoch [1/1], Step [87000/274360], Loss: 0.0000, Train time: 101.66\n",
      "Epoch [1/1], Step [87100/274360], Loss: 0.0000, Train time: 101.76\n",
      "Epoch [1/1], Step [87200/274360], Loss: 0.0000, Train time: 101.81\n",
      "Epoch [1/1], Step [87300/274360], Loss: 0.0014, Train time: 101.96\n",
      "Epoch [1/1], Step [87400/274360], Loss: 0.0000, Train time: 102.01\n",
      "Epoch [1/1], Step [87500/274360], Loss: 0.0143, Train time: 102.08\n",
      "Epoch [1/1], Step [87600/274360], Loss: 0.0000, Train time: 102.23\n",
      "Epoch [1/1], Step [87700/274360], Loss: 0.0011, Train time: 102.41\n",
      "Epoch [1/1], Step [87800/274360], Loss: 0.0178, Train time: 102.48\n",
      "Epoch [1/1], Step [87900/274360], Loss: 0.0075, Train time: 102.53\n",
      "Epoch [1/1], Step [88000/274360], Loss: 0.0030, Train time: 102.74\n",
      "Epoch [1/1], Step [88100/274360], Loss: 0.0012, Train time: 102.81\n",
      "Epoch [1/1], Step [88200/274360], Loss: 0.0000, Train time: 102.85\n",
      "Epoch [1/1], Step [88300/274360], Loss: 0.0002, Train time: 103.05\n",
      "Epoch [1/1], Step [88400/274360], Loss: 0.0082, Train time: 103.15\n",
      "Epoch [1/1], Step [88500/274360], Loss: 0.0000, Train time: 103.23\n",
      "Epoch [1/1], Step [88600/274360], Loss: 0.8068, Train time: 103.29\n",
      "Epoch [1/1], Step [88700/274360], Loss: 0.0024, Train time: 103.46\n",
      "Epoch [1/1], Step [88800/274360], Loss: 0.0000, Train time: 103.52\n",
      "Epoch [1/1], Step [88900/274360], Loss: 0.0001, Train time: 103.58\n",
      "Epoch [1/1], Step [89000/274360], Loss: 0.0000, Train time: 103.66\n",
      "Epoch [1/1], Step [89100/274360], Loss: 1.1181, Train time: 103.72\n",
      "Epoch [1/1], Step [89200/274360], Loss: 0.0001, Train time: 103.94\n",
      "Epoch [1/1], Step [89300/274360], Loss: 0.0067, Train time: 104.03\n",
      "Epoch [1/1], Step [89400/274360], Loss: 0.0104, Train time: 104.09\n",
      "Epoch [1/1], Step [89500/274360], Loss: 0.0007, Train time: 104.27\n",
      "Epoch [1/1], Step [89600/274360], Loss: 0.0000, Train time: 104.37\n",
      "Epoch [1/1], Step [89700/274360], Loss: 0.0001, Train time: 104.41\n",
      "Epoch [1/1], Step [89800/274360], Loss: 0.0078, Train time: 104.49\n",
      "Epoch [1/1], Step [89900/274360], Loss: 0.0269, Train time: 104.62\n",
      "Epoch [1/1], Step [90000/274360], Loss: 0.0701, Train time: 104.82\n",
      "Epoch [1/1], Step [90100/274360], Loss: 0.0009, Train time: 104.92\n",
      "Epoch [1/1], Step [90200/274360], Loss: 0.0005, Train time: 104.99\n",
      "Epoch [1/1], Step [90300/274360], Loss: 0.0001, Train time: 105.07\n",
      "Epoch [1/1], Step [90400/274360], Loss: 0.0004, Train time: 105.13\n",
      "Epoch [1/1], Step [90500/274360], Loss: 0.0004, Train time: 105.41\n",
      "Epoch [1/1], Step [90600/274360], Loss: 0.0000, Train time: 105.46\n",
      "Epoch [1/1], Step [90700/274360], Loss: 0.0309, Train time: 105.52\n",
      "Epoch [1/1], Step [90800/274360], Loss: 0.0003, Train time: 105.62\n",
      "Epoch [1/1], Step [90900/274360], Loss: 0.0003, Train time: 105.69\n",
      "Epoch [1/1], Step [91000/274360], Loss: 0.0287, Train time: 105.76\n",
      "Epoch [1/1], Step [91100/274360], Loss: 0.0001, Train time: 105.87\n",
      "Epoch [1/1], Step [91200/274360], Loss: 0.0001, Train time: 106.06\n",
      "Epoch [1/1], Step [91300/274360], Loss: 0.0004, Train time: 106.10\n",
      "Epoch [1/1], Step [91400/274360], Loss: 0.0009, Train time: 106.18\n",
      "Epoch [1/1], Step [91500/274360], Loss: 0.0006, Train time: 106.27\n",
      "Epoch [1/1], Step [91600/274360], Loss: 0.0001, Train time: 106.50\n",
      "Epoch [1/1], Step [91700/274360], Loss: 0.0061, Train time: 106.57\n",
      "Epoch [1/1], Step [91800/274360], Loss: 0.0000, Train time: 106.69\n",
      "Epoch [1/1], Step [91900/274360], Loss: 0.0094, Train time: 106.79\n",
      "Epoch [1/1], Step [92000/274360], Loss: 0.8886, Train time: 106.90\n",
      "Epoch [1/1], Step [92100/274360], Loss: 0.0000, Train time: 107.12\n",
      "Epoch [1/1], Step [92200/274360], Loss: 0.0003, Train time: 107.21\n",
      "Epoch [1/1], Step [92300/274360], Loss: 0.0187, Train time: 107.25\n",
      "Epoch [1/1], Step [92400/274360], Loss: 0.0014, Train time: 107.40\n",
      "Epoch [1/1], Step [92500/274360], Loss: 0.0000, Train time: 107.58\n",
      "Epoch [1/1], Step [92600/274360], Loss: 0.0064, Train time: 107.62\n",
      "Epoch [1/1], Step [92700/274360], Loss: 0.0001, Train time: 107.75\n",
      "Epoch [1/1], Step [92800/274360], Loss: 0.0013, Train time: 107.88\n",
      "Epoch [1/1], Step [92900/274360], Loss: 0.0057, Train time: 107.98\n",
      "Epoch [1/1], Step [93000/274360], Loss: 0.0002, Train time: 108.11\n",
      "Epoch [1/1], Step [93100/274360], Loss: 0.0000, Train time: 108.22\n",
      "Epoch [1/1], Step [93200/274360], Loss: 0.0006, Train time: 108.29\n",
      "Epoch [1/1], Step [93300/274360], Loss: 0.0000, Train time: 108.44\n",
      "Epoch [1/1], Step [93400/274360], Loss: 0.0000, Train time: 108.51\n",
      "Epoch [1/1], Step [93500/274360], Loss: 0.0000, Train time: 108.62\n",
      "Epoch [1/1], Step [93600/274360], Loss: 0.0000, Train time: 108.93\n",
      "Epoch [1/1], Step [93700/274360], Loss: 0.0000, Train time: 108.97\n",
      "Epoch [1/1], Step [93800/274360], Loss: 0.0004, Train time: 109.01\n",
      "Epoch [1/1], Step [93900/274360], Loss: 0.0000, Train time: 109.08\n",
      "Epoch [1/1], Step [94000/274360], Loss: 0.0000, Train time: 109.20\n",
      "Epoch [1/1], Step [94100/274360], Loss: 0.0000, Train time: 109.39\n",
      "Epoch [1/1], Step [94200/274360], Loss: 0.0536, Train time: 109.53\n",
      "Epoch [1/1], Step [94300/274360], Loss: 0.0000, Train time: 109.61\n",
      "Epoch [1/1], Step [94400/274360], Loss: 0.0002, Train time: 109.68\n",
      "Epoch [1/1], Step [94500/274360], Loss: 0.0001, Train time: 109.86\n",
      "Epoch [1/1], Step [94600/274360], Loss: 0.0003, Train time: 109.96\n",
      "Epoch [1/1], Step [94700/274360], Loss: 0.0524, Train time: 110.04\n",
      "Epoch [1/1], Step [94800/274360], Loss: 0.0002, Train time: 110.12\n",
      "Epoch [1/1], Step [94900/274360], Loss: 0.0000, Train time: 110.57\n",
      "Epoch [1/1], Step [95000/274360], Loss: 0.0083, Train time: 110.66\n",
      "Epoch [1/1], Step [95100/274360], Loss: 0.0003, Train time: 110.71\n",
      "Epoch [1/1], Step [95200/274360], Loss: 0.0000, Train time: 110.81\n",
      "Epoch [1/1], Step [95300/274360], Loss: 0.0013, Train time: 110.92\n",
      "Epoch [1/1], Step [95400/274360], Loss: 0.0014, Train time: 111.17\n",
      "Epoch [1/1], Step [95500/274360], Loss: 0.0000, Train time: 111.26\n",
      "Epoch [1/1], Step [95600/274360], Loss: 0.0130, Train time: 111.31\n",
      "Epoch [1/1], Step [95700/274360], Loss: 0.0000, Train time: 111.36\n",
      "Epoch [1/1], Step [95800/274360], Loss: 0.0067, Train time: 111.60\n",
      "Epoch [1/1], Step [95900/274360], Loss: 0.0003, Train time: 111.68\n",
      "Epoch [1/1], Step [96000/274360], Loss: 0.0010, Train time: 111.73\n",
      "Epoch [1/1], Step [96100/274360], Loss: 0.0008, Train time: 111.85\n",
      "Epoch [1/1], Step [96200/274360], Loss: 0.0034, Train time: 111.99\n",
      "Epoch [1/1], Step [96300/274360], Loss: 0.0005, Train time: 112.04\n",
      "Epoch [1/1], Step [96400/274360], Loss: 0.0009, Train time: 112.13\n",
      "Epoch [1/1], Step [96500/274360], Loss: 0.0293, Train time: 112.28\n",
      "Epoch [1/1], Step [96600/274360], Loss: 0.0015, Train time: 112.36\n",
      "Epoch [1/1], Step [96700/274360], Loss: 0.0298, Train time: 112.44\n",
      "Epoch [1/1], Step [96800/274360], Loss: 0.0049, Train time: 112.58\n",
      "Epoch [1/1], Step [96900/274360], Loss: 0.0001, Train time: 112.68\n",
      "Epoch [1/1], Step [97000/274360], Loss: 0.0000, Train time: 112.74\n",
      "Epoch [1/1], Step [97100/274360], Loss: 0.0463, Train time: 112.96\n",
      "Epoch [1/1], Step [97200/274360], Loss: 0.0001, Train time: 113.03\n",
      "Epoch [1/1], Step [97300/274360], Loss: 0.0073, Train time: 113.08\n",
      "Epoch [1/1], Step [97400/274360], Loss: 0.2062, Train time: 113.36\n",
      "Epoch [1/1], Step [97500/274360], Loss: 0.0006, Train time: 113.46\n",
      "Epoch [1/1], Step [97600/274360], Loss: 0.0006, Train time: 113.54\n",
      "Epoch [1/1], Step [97700/274360], Loss: 0.0001, Train time: 113.63\n",
      "Epoch [1/1], Step [97800/274360], Loss: 0.0000, Train time: 113.71\n",
      "Epoch [1/1], Step [97900/274360], Loss: 0.0011, Train time: 113.77\n",
      "Epoch [1/1], Step [98000/274360], Loss: 0.0007, Train time: 114.06\n",
      "Epoch [1/1], Step [98100/274360], Loss: 0.0001, Train time: 114.10\n",
      "Epoch [1/1], Step [98200/274360], Loss: 0.0000, Train time: 114.16\n",
      "Epoch [1/1], Step [98300/274360], Loss: 0.0006, Train time: 114.21\n",
      "Epoch [1/1], Step [98400/274360], Loss: 0.0000, Train time: 114.40\n",
      "Epoch [1/1], Step [98500/274360], Loss: 0.0013, Train time: 114.48\n",
      "Epoch [1/1], Step [98600/274360], Loss: 0.0001, Train time: 114.52\n",
      "Epoch [1/1], Step [98700/274360], Loss: 0.0000, Train time: 114.59\n",
      "Epoch [1/1], Step [98800/274360], Loss: 0.0092, Train time: 114.76\n",
      "Epoch [1/1], Step [98900/274360], Loss: 0.0001, Train time: 114.84\n",
      "Epoch [1/1], Step [99000/274360], Loss: 0.0003, Train time: 115.05\n",
      "Epoch [1/1], Step [99100/274360], Loss: 0.0002, Train time: 115.25\n",
      "Epoch [1/1], Step [99200/274360], Loss: 0.0000, Train time: 115.33\n",
      "Epoch [1/1], Step [99300/274360], Loss: 0.0063, Train time: 115.41\n",
      "Epoch [1/1], Step [99400/274360], Loss: 0.0009, Train time: 115.60\n",
      "Epoch [1/1], Step [99500/274360], Loss: 0.4084, Train time: 115.70\n",
      "Epoch [1/1], Step [99600/274360], Loss: 0.0000, Train time: 115.92\n",
      "Epoch [1/1], Step [99700/274360], Loss: 0.0000, Train time: 115.96\n",
      "Epoch [1/1], Step [99800/274360], Loss: 0.0000, Train time: 116.03\n",
      "Epoch [1/1], Step [99900/274360], Loss: 0.0003, Train time: 116.10\n",
      "Epoch [1/1], Step [100000/274360], Loss: 0.0026, Train time: 116.18\n",
      "Epoch [1/1], Step [100100/274360], Loss: 0.0038, Train time: 116.44\n",
      "Epoch [1/1], Step [100200/274360], Loss: 0.0001, Train time: 116.49\n",
      "Epoch [1/1], Step [100300/274360], Loss: 0.0003, Train time: 116.56\n",
      "Epoch [1/1], Step [100400/274360], Loss: 0.0042, Train time: 116.72\n",
      "Epoch [1/1], Step [100500/274360], Loss: 0.0003, Train time: 116.79\n",
      "Epoch [1/1], Step [100600/274360], Loss: 0.0001, Train time: 117.02\n",
      "Epoch [1/1], Step [100700/274360], Loss: 0.0001, Train time: 117.09\n",
      "Epoch [1/1], Step [100800/274360], Loss: 0.0000, Train time: 117.18\n",
      "Epoch [1/1], Step [100900/274360], Loss: 0.0130, Train time: 117.28\n",
      "Epoch [1/1], Step [101000/274360], Loss: 0.0131, Train time: 117.58\n",
      "Epoch [1/1], Step [101100/274360], Loss: 0.0000, Train time: 117.70\n",
      "Epoch [1/1], Step [101200/274360], Loss: 0.0000, Train time: 117.77\n",
      "Epoch [1/1], Step [101300/274360], Loss: 0.0007, Train time: 117.83\n",
      "Epoch [1/1], Step [101400/274360], Loss: 0.0000, Train time: 117.91\n",
      "Epoch [1/1], Step [101500/274360], Loss: 0.0000, Train time: 118.18\n",
      "Epoch [1/1], Step [101600/274360], Loss: 0.0000, Train time: 118.27\n",
      "Epoch [1/1], Step [101700/274360], Loss: 0.0190, Train time: 118.33\n",
      "Epoch [1/1], Step [101800/274360], Loss: 0.0002, Train time: 118.49\n",
      "Epoch [1/1], Step [101900/274360], Loss: 0.0000, Train time: 118.55\n",
      "Epoch [1/1], Step [102000/274360], Loss: 0.0027, Train time: 118.61\n",
      "Epoch [1/1], Step [102100/274360], Loss: 0.0003, Train time: 118.95\n",
      "Epoch [1/1], Step [102200/274360], Loss: 0.0271, Train time: 118.99\n",
      "Epoch [1/1], Step [102300/274360], Loss: 0.0017, Train time: 119.09\n",
      "Epoch [1/1], Step [102400/274360], Loss: 0.0000, Train time: 119.31\n",
      "Epoch [1/1], Step [102500/274360], Loss: 0.0124, Train time: 119.39\n",
      "Epoch [1/1], Step [102600/274360], Loss: 0.0438, Train time: 119.46\n",
      "Epoch [1/1], Step [102700/274360], Loss: 0.0059, Train time: 119.52\n",
      "Epoch [1/1], Step [102800/274360], Loss: 0.0000, Train time: 119.60\n",
      "Epoch [1/1], Step [102900/274360], Loss: 0.0031, Train time: 119.78\n",
      "Epoch [1/1], Step [103000/274360], Loss: 0.0000, Train time: 120.00\n",
      "Epoch [1/1], Step [103100/274360], Loss: 0.0002, Train time: 120.05\n",
      "Epoch [1/1], Step [103200/274360], Loss: 0.0040, Train time: 120.11\n",
      "Epoch [1/1], Step [103300/274360], Loss: 0.0009, Train time: 120.21\n",
      "Epoch [1/1], Step [103400/274360], Loss: 0.0001, Train time: 120.34\n",
      "Epoch [1/1], Step [103500/274360], Loss: 0.0001, Train time: 120.67\n",
      "Epoch [1/1], Step [103600/274360], Loss: 0.0000, Train time: 120.78\n",
      "Epoch [1/1], Step [103700/274360], Loss: 0.0000, Train time: 120.84\n",
      "Epoch [1/1], Step [103800/274360], Loss: 0.0001, Train time: 120.95\n",
      "Epoch [1/1], Step [103900/274360], Loss: 0.0000, Train time: 121.07\n",
      "Epoch [1/1], Step [104000/274360], Loss: 0.0001, Train time: 121.17\n",
      "Epoch [1/1], Step [104100/274360], Loss: 0.0002, Train time: 121.47\n",
      "Epoch [1/1], Step [104200/274360], Loss: 0.0027, Train time: 121.52\n",
      "Epoch [1/1], Step [104300/274360], Loss: 0.0004, Train time: 121.58\n",
      "Epoch [1/1], Step [104400/274360], Loss: 0.0006, Train time: 121.63\n",
      "Epoch [1/1], Step [104500/274360], Loss: 0.0042, Train time: 121.90\n",
      "Epoch [1/1], Step [104600/274360], Loss: 0.0000, Train time: 121.96\n",
      "Epoch [1/1], Step [104700/274360], Loss: 0.0000, Train time: 122.02\n",
      "Epoch [1/1], Step [104800/274360], Loss: 0.0000, Train time: 122.27\n",
      "Epoch [1/1], Step [104900/274360], Loss: 0.0001, Train time: 122.33\n",
      "Epoch [1/1], Step [105000/274360], Loss: 0.0000, Train time: 122.48\n",
      "Epoch [1/1], Step [105100/274360], Loss: 0.0004, Train time: 122.61\n",
      "Epoch [1/1], Step [105200/274360], Loss: 0.0098, Train time: 122.71\n",
      "Epoch [1/1], Step [105300/274360], Loss: 0.0004, Train time: 122.95\n",
      "Epoch [1/1], Step [105400/274360], Loss: 0.0027, Train time: 123.01\n",
      "Epoch [1/1], Step [105500/274360], Loss: 0.0000, Train time: 123.05\n",
      "Epoch [1/1], Step [105600/274360], Loss: 0.0352, Train time: 123.17\n",
      "Epoch [1/1], Step [105700/274360], Loss: 0.0006, Train time: 123.33\n",
      "Epoch [1/1], Step [105800/274360], Loss: 0.0000, Train time: 123.58\n",
      "Epoch [1/1], Step [105900/274360], Loss: 0.0000, Train time: 123.71\n",
      "Epoch [1/1], Step [106000/274360], Loss: 0.0000, Train time: 123.79\n",
      "Epoch [1/1], Step [106100/274360], Loss: 0.0001, Train time: 123.88\n",
      "Epoch [1/1], Step [106200/274360], Loss: 0.0000, Train time: 124.13\n",
      "Epoch [1/1], Step [106300/274360], Loss: 0.0034, Train time: 124.22\n",
      "Epoch [1/1], Step [106400/274360], Loss: 0.0000, Train time: 124.35\n",
      "Epoch [1/1], Step [106500/274360], Loss: 0.0001, Train time: 124.46\n",
      "Epoch [1/1], Step [106600/274360], Loss: 0.0000, Train time: 124.60\n",
      "Epoch [1/1], Step [106700/274360], Loss: 0.0001, Train time: 124.66\n",
      "Epoch [1/1], Step [106800/274360], Loss: 0.0445, Train time: 124.81\n",
      "Epoch [1/1], Step [106900/274360], Loss: 0.0005, Train time: 125.08\n",
      "Epoch [1/1], Step [107000/274360], Loss: 0.0645, Train time: 125.16\n",
      "Epoch [1/1], Step [107100/274360], Loss: 0.0000, Train time: 125.26\n",
      "Epoch [1/1], Step [107200/274360], Loss: 0.0000, Train time: 125.35\n",
      "Epoch [1/1], Step [107300/274360], Loss: 0.0002, Train time: 125.46\n",
      "Epoch [1/1], Step [107400/274360], Loss: 0.0000, Train time: 125.65\n",
      "Epoch [1/1], Step [107500/274360], Loss: 0.0059, Train time: 125.82\n",
      "Epoch [1/1], Step [107600/274360], Loss: 0.0000, Train time: 125.92\n",
      "Epoch [1/1], Step [107700/274360], Loss: 0.0000, Train time: 125.99\n",
      "Epoch [1/1], Step [107800/274360], Loss: 0.0016, Train time: 126.07\n",
      "Epoch [1/1], Step [107900/274360], Loss: 0.0000, Train time: 126.15\n",
      "Epoch [1/1], Step [108000/274360], Loss: 0.0007, Train time: 126.24\n",
      "Epoch [1/1], Step [108100/274360], Loss: 0.0000, Train time: 126.57\n",
      "Epoch [1/1], Step [108200/274360], Loss: 0.0001, Train time: 126.74\n",
      "Epoch [1/1], Step [108300/274360], Loss: 0.0000, Train time: 126.79\n",
      "Epoch [1/1], Step [108400/274360], Loss: 0.0002, Train time: 126.86\n",
      "Epoch [1/1], Step [108500/274360], Loss: 0.0001, Train time: 126.95\n",
      "Epoch [1/1], Step [108600/274360], Loss: 0.0001, Train time: 127.15\n",
      "Epoch [1/1], Step [108700/274360], Loss: 0.0035, Train time: 127.19\n",
      "Epoch [1/1], Step [108800/274360], Loss: 0.0000, Train time: 127.26\n",
      "Epoch [1/1], Step [108900/274360], Loss: 0.0000, Train time: 127.42\n",
      "Epoch [1/1], Step [109000/274360], Loss: 0.0000, Train time: 127.53\n",
      "Epoch [1/1], Step [109100/274360], Loss: 0.0005, Train time: 127.85\n",
      "Epoch [1/1], Step [109200/274360], Loss: 0.0001, Train time: 127.93\n",
      "Epoch [1/1], Step [109300/274360], Loss: 0.0064, Train time: 127.99\n",
      "Epoch [1/1], Step [109400/274360], Loss: 0.0000, Train time: 128.08\n",
      "Epoch [1/1], Step [109500/274360], Loss: 0.0001, Train time: 128.19\n",
      "Epoch [1/1], Step [109600/274360], Loss: 0.0000, Train time: 128.52\n",
      "Epoch [1/1], Step [109700/274360], Loss: 0.0102, Train time: 128.59\n",
      "Epoch [1/1], Step [109800/274360], Loss: 0.0000, Train time: 128.66\n",
      "Epoch [1/1], Step [109900/274360], Loss: 0.0000, Train time: 128.73\n",
      "Epoch [1/1], Step [110000/274360], Loss: 0.0242, Train time: 128.93\n",
      "Epoch [1/1], Step [110100/274360], Loss: 0.0033, Train time: 129.05\n",
      "Epoch [1/1], Step [110200/274360], Loss: 0.0000, Train time: 129.12\n",
      "Epoch [1/1], Step [110300/274360], Loss: 0.0015, Train time: 129.16\n",
      "Epoch [1/1], Step [110400/274360], Loss: 0.0001, Train time: 129.22\n",
      "Epoch [1/1], Step [110500/274360], Loss: 0.0001, Train time: 129.49\n",
      "Epoch [1/1], Step [110600/274360], Loss: 0.0000, Train time: 129.67\n",
      "Epoch [1/1], Step [110700/274360], Loss: 0.0000, Train time: 129.86\n",
      "Epoch [1/1], Step [110800/274360], Loss: 0.0000, Train time: 130.03\n",
      "Epoch [1/1], Step [110900/274360], Loss: 0.0001, Train time: 130.07\n",
      "Epoch [1/1], Step [111000/274360], Loss: 0.0001, Train time: 130.34\n",
      "Epoch [1/1], Step [111100/274360], Loss: 0.0002, Train time: 130.40\n",
      "Epoch [1/1], Step [111200/274360], Loss: 0.0045, Train time: 130.48\n",
      "Epoch [1/1], Step [111300/274360], Loss: 0.0004, Train time: 130.61\n",
      "Epoch [1/1], Step [111400/274360], Loss: 0.0001, Train time: 130.74\n",
      "Epoch [1/1], Step [111500/274360], Loss: 0.0000, Train time: 130.80\n",
      "Epoch [1/1], Step [111600/274360], Loss: 0.0001, Train time: 130.86\n",
      "Epoch [1/1], Step [111700/274360], Loss: 0.0016, Train time: 131.06\n",
      "Epoch [1/1], Step [111800/274360], Loss: 0.0000, Train time: 131.14\n",
      "Epoch [1/1], Step [111900/274360], Loss: 0.0000, Train time: 131.20\n",
      "Epoch [1/1], Step [112000/274360], Loss: 0.0002, Train time: 131.44\n",
      "Epoch [1/1], Step [112100/274360], Loss: 0.0082, Train time: 131.61\n",
      "Epoch [1/1], Step [112200/274360], Loss: 0.0177, Train time: 131.71\n",
      "Epoch [1/1], Step [112300/274360], Loss: 0.0033, Train time: 131.75\n",
      "Epoch [1/1], Step [112400/274360], Loss: 0.0000, Train time: 131.88\n",
      "Epoch [1/1], Step [112500/274360], Loss: 0.0000, Train time: 132.13\n",
      "Epoch [1/1], Step [112600/274360], Loss: 0.0000, Train time: 132.20\n",
      "Epoch [1/1], Step [112700/274360], Loss: 0.0000, Train time: 132.29\n",
      "Epoch [1/1], Step [112800/274360], Loss: 0.0010, Train time: 132.38\n",
      "Epoch [1/1], Step [112900/274360], Loss: 0.0000, Train time: 132.51\n",
      "Epoch [1/1], Step [113000/274360], Loss: 0.0000, Train time: 132.59\n",
      "Epoch [1/1], Step [113100/274360], Loss: 0.0001, Train time: 132.84\n",
      "Epoch [1/1], Step [113200/274360], Loss: 0.0000, Train time: 132.91\n",
      "Epoch [1/1], Step [113300/274360], Loss: 0.0000, Train time: 132.97\n",
      "Epoch [1/1], Step [113400/274360], Loss: 0.0099, Train time: 133.05\n",
      "Epoch [1/1], Step [113500/274360], Loss: 0.0001, Train time: 133.32\n",
      "Epoch [1/1], Step [113600/274360], Loss: 0.0011, Train time: 133.39\n",
      "Epoch [1/1], Step [113700/274360], Loss: 0.0002, Train time: 133.50\n",
      "Epoch [1/1], Step [113800/274360], Loss: 0.0015, Train time: 133.59\n",
      "Epoch [1/1], Step [113900/274360], Loss: 0.0000, Train time: 133.89\n",
      "Epoch [1/1], Step [114000/274360], Loss: 0.0000, Train time: 133.97\n",
      "Epoch [1/1], Step [114100/274360], Loss: 0.0000, Train time: 134.05\n",
      "Epoch [1/1], Step [114200/274360], Loss: 0.0001, Train time: 134.22\n",
      "Epoch [1/1], Step [114300/274360], Loss: 0.0002, Train time: 134.35\n",
      "Epoch [1/1], Step [114400/274360], Loss: 0.0001, Train time: 134.54\n",
      "Epoch [1/1], Step [114500/274360], Loss: 0.0003, Train time: 134.58\n",
      "Epoch [1/1], Step [114600/274360], Loss: 0.0018, Train time: 134.67\n",
      "Epoch [1/1], Step [114700/274360], Loss: 0.0002, Train time: 134.77\n",
      "Epoch [1/1], Step [114800/274360], Loss: 0.0000, Train time: 134.89\n",
      "Epoch [1/1], Step [114900/274360], Loss: 0.0001, Train time: 135.00\n",
      "Epoch [1/1], Step [115000/274360], Loss: 0.0012, Train time: 135.29\n",
      "Epoch [1/1], Step [115100/274360], Loss: 0.0000, Train time: 135.37\n",
      "Epoch [1/1], Step [115200/274360], Loss: 0.0002, Train time: 135.43\n",
      "Epoch [1/1], Step [115300/274360], Loss: 0.0000, Train time: 135.54\n",
      "Epoch [1/1], Step [115400/274360], Loss: 0.0001, Train time: 135.69\n",
      "Epoch [1/1], Step [115500/274360], Loss: 0.0000, Train time: 135.89\n",
      "Epoch [1/1], Step [115600/274360], Loss: 0.0000, Train time: 135.98\n",
      "Epoch [1/1], Step [115700/274360], Loss: 0.0000, Train time: 136.02\n",
      "Epoch [1/1], Step [115800/274360], Loss: 0.0002, Train time: 136.07\n",
      "Epoch [1/1], Step [115900/274360], Loss: 0.0002, Train time: 136.30\n",
      "Epoch [1/1], Step [116000/274360], Loss: 0.0000, Train time: 136.36\n",
      "Epoch [1/1], Step [116100/274360], Loss: 0.0001, Train time: 136.46\n",
      "Epoch [1/1], Step [116200/274360], Loss: 0.0000, Train time: 136.60\n",
      "Epoch [1/1], Step [116300/274360], Loss: 0.0000, Train time: 136.72\n",
      "Epoch [1/1], Step [116400/274360], Loss: 0.0002, Train time: 136.79\n",
      "Epoch [1/1], Step [116500/274360], Loss: 0.0000, Train time: 136.84\n",
      "Epoch [1/1], Step [116600/274360], Loss: 0.0004, Train time: 136.95\n",
      "Epoch [1/1], Step [116700/274360], Loss: 0.0001, Train time: 137.26\n",
      "Epoch [1/1], Step [116800/274360], Loss: 0.0009, Train time: 137.31\n",
      "Epoch [1/1], Step [116900/274360], Loss: 0.0000, Train time: 137.39\n",
      "Epoch [1/1], Step [117000/274360], Loss: 0.0006, Train time: 137.46\n",
      "Epoch [1/1], Step [117100/274360], Loss: 0.0001, Train time: 137.60\n",
      "Epoch [1/1], Step [117200/274360], Loss: 0.0000, Train time: 137.87\n",
      "Epoch [1/1], Step [117300/274360], Loss: 0.0064, Train time: 137.92\n",
      "Epoch [1/1], Step [117400/274360], Loss: 0.0000, Train time: 137.98\n",
      "Epoch [1/1], Step [117500/274360], Loss: 0.0000, Train time: 138.04\n",
      "Epoch [1/1], Step [117600/274360], Loss: 0.0002, Train time: 138.23\n",
      "Epoch [1/1], Step [117700/274360], Loss: 0.0000, Train time: 138.48\n",
      "Epoch [1/1], Step [117800/274360], Loss: 0.0002, Train time: 138.52\n",
      "Epoch [1/1], Step [117900/274360], Loss: 0.0000, Train time: 138.63\n",
      "Epoch [1/1], Step [118000/274360], Loss: 0.0055, Train time: 138.71\n",
      "Epoch [1/1], Step [118100/274360], Loss: 0.0000, Train time: 138.95\n",
      "Epoch [1/1], Step [118200/274360], Loss: 0.0426, Train time: 139.03\n",
      "Epoch [1/1], Step [118300/274360], Loss: 0.0060, Train time: 139.09\n",
      "Epoch [1/1], Step [118400/274360], Loss: 0.0125, Train time: 139.27\n",
      "Epoch [1/1], Step [118500/274360], Loss: 0.0003, Train time: 139.39\n",
      "Epoch [1/1], Step [118600/274360], Loss: 0.0008, Train time: 139.64\n",
      "Epoch [1/1], Step [118700/274360], Loss: 0.0010, Train time: 139.74\n",
      "Epoch [1/1], Step [118800/274360], Loss: 0.0001, Train time: 139.81\n",
      "Epoch [1/1], Step [118900/274360], Loss: 0.0000, Train time: 140.07\n",
      "Epoch [1/1], Step [119000/274360], Loss: 0.0000, Train time: 140.15\n",
      "Epoch [1/1], Step [119100/274360], Loss: 0.0091, Train time: 140.20\n",
      "Epoch [1/1], Step [119200/274360], Loss: 0.0000, Train time: 140.34\n",
      "Epoch [1/1], Step [119300/274360], Loss: 0.0000, Train time: 140.46\n",
      "Epoch [1/1], Step [119400/274360], Loss: 0.0000, Train time: 140.52\n",
      "Epoch [1/1], Step [119500/274360], Loss: 0.0030, Train time: 140.74\n",
      "Epoch [1/1], Step [119600/274360], Loss: 0.0000, Train time: 140.89\n",
      "Epoch [1/1], Step [119700/274360], Loss: 0.0013, Train time: 140.94\n",
      "Epoch [1/1], Step [119800/274360], Loss: 0.0000, Train time: 141.01\n",
      "Epoch [1/1], Step [119900/274360], Loss: 0.0000, Train time: 141.24\n",
      "Epoch [1/1], Step [120000/274360], Loss: 0.0000, Train time: 141.38\n",
      "Epoch [1/1], Step [120100/274360], Loss: 0.0016, Train time: 141.49\n",
      "Epoch [1/1], Step [120200/274360], Loss: 0.0011, Train time: 141.55\n",
      "Epoch [1/1], Step [120300/274360], Loss: 0.0003, Train time: 141.67\n",
      "Epoch [1/1], Step [120400/274360], Loss: 0.0000, Train time: 141.94\n",
      "Epoch [1/1], Step [120500/274360], Loss: 0.0022, Train time: 141.99\n",
      "Epoch [1/1], Step [120600/274360], Loss: 0.0000, Train time: 142.04\n",
      "Epoch [1/1], Step [120700/274360], Loss: 10.1417, Train time: 142.13\n",
      "Epoch [1/1], Step [120800/274360], Loss: 0.0012, Train time: 142.34\n",
      "Epoch [1/1], Step [120900/274360], Loss: 0.0000, Train time: 142.42\n",
      "Epoch [1/1], Step [121000/274360], Loss: 0.0000, Train time: 142.52\n",
      "Epoch [1/1], Step [121100/274360], Loss: 0.0001, Train time: 142.60\n",
      "Epoch [1/1], Step [121200/274360], Loss: 0.0000, Train time: 142.83\n",
      "Epoch [1/1], Step [121300/274360], Loss: 0.0019, Train time: 142.90\n",
      "Epoch [1/1], Step [121400/274360], Loss: 0.0011, Train time: 142.97\n",
      "Epoch [1/1], Step [121500/274360], Loss: 0.0022, Train time: 143.03\n",
      "Epoch [1/1], Step [121600/274360], Loss: 0.0000, Train time: 143.30\n",
      "Epoch [1/1], Step [121700/274360], Loss: 0.0004, Train time: 143.43\n",
      "Epoch [1/1], Step [121800/274360], Loss: 0.0002, Train time: 143.48\n",
      "Epoch [1/1], Step [121900/274360], Loss: 0.0000, Train time: 143.54\n",
      "Epoch [1/1], Step [122000/274360], Loss: 0.0001, Train time: 143.69\n",
      "Epoch [1/1], Step [122100/274360], Loss: 0.0000, Train time: 144.00\n",
      "Epoch [1/1], Step [122200/274360], Loss: 0.0001, Train time: 144.17\n",
      "Epoch [1/1], Step [122300/274360], Loss: 0.0002, Train time: 144.26\n",
      "Epoch [1/1], Step [122400/274360], Loss: 0.0000, Train time: 144.35\n",
      "Epoch [1/1], Step [122500/274360], Loss: 0.0000, Train time: 144.55\n",
      "Epoch [1/1], Step [122600/274360], Loss: 0.0132, Train time: 144.66\n",
      "Epoch [1/1], Step [122700/274360], Loss: 0.0001, Train time: 144.73\n",
      "Epoch [1/1], Step [122800/274360], Loss: 0.0001, Train time: 144.80\n",
      "Epoch [1/1], Step [122900/274360], Loss: 0.0002, Train time: 145.03\n",
      "Epoch [1/1], Step [123000/274360], Loss: 0.0044, Train time: 145.08\n",
      "Epoch [1/1], Step [123100/274360], Loss: 0.0028, Train time: 145.17\n",
      "Epoch [1/1], Step [123200/274360], Loss: 0.0014, Train time: 145.30\n",
      "Epoch [1/1], Step [123300/274360], Loss: 0.0068, Train time: 145.51\n",
      "Epoch [1/1], Step [123400/274360], Loss: 0.0000, Train time: 145.58\n",
      "Epoch [1/1], Step [123500/274360], Loss: 0.0001, Train time: 145.64\n",
      "Epoch [1/1], Step [123600/274360], Loss: 0.0000, Train time: 145.80\n",
      "Epoch [1/1], Step [123700/274360], Loss: 0.0001, Train time: 145.93\n",
      "Epoch [1/1], Step [123800/274360], Loss: 0.1546, Train time: 146.07\n",
      "Epoch [1/1], Step [123900/274360], Loss: 0.0041, Train time: 146.17\n",
      "Epoch [1/1], Step [124000/274360], Loss: 0.0003, Train time: 146.26\n",
      "Epoch [1/1], Step [124100/274360], Loss: 0.0006, Train time: 146.46\n",
      "Epoch [1/1], Step [124200/274360], Loss: 0.0004, Train time: 146.81\n",
      "Epoch [1/1], Step [124300/274360], Loss: 0.0000, Train time: 146.95\n",
      "Epoch [1/1], Step [124400/274360], Loss: 0.0006, Train time: 147.00\n",
      "Epoch [1/1], Step [124500/274360], Loss: 0.0000, Train time: 147.07\n",
      "Epoch [1/1], Step [124600/274360], Loss: 0.0000, Train time: 147.27\n",
      "Epoch [1/1], Step [124700/274360], Loss: 0.0001, Train time: 147.32\n",
      "Epoch [1/1], Step [124800/274360], Loss: 0.0000, Train time: 147.46\n",
      "Epoch [1/1], Step [124900/274360], Loss: 0.0000, Train time: 147.55\n",
      "Epoch [1/1], Step [125000/274360], Loss: 0.0001, Train time: 147.69\n",
      "Epoch [1/1], Step [125100/274360], Loss: 0.0001, Train time: 147.78\n",
      "Epoch [1/1], Step [125200/274360], Loss: 0.0000, Train time: 147.85\n",
      "Epoch [1/1], Step [125300/274360], Loss: 0.0003, Train time: 148.01\n",
      "Epoch [1/1], Step [125400/274360], Loss: 0.0000, Train time: 148.06\n",
      "Epoch [1/1], Step [125500/274360], Loss: 0.0000, Train time: 148.15\n",
      "Epoch [1/1], Step [125600/274360], Loss: 0.0016, Train time: 148.38\n",
      "Epoch [1/1], Step [125700/274360], Loss: 0.0005, Train time: 148.52\n",
      "Epoch [1/1], Step [125800/274360], Loss: 0.0000, Train time: 148.62\n",
      "Epoch [1/1], Step [125900/274360], Loss: 0.0000, Train time: 148.71\n",
      "Epoch [1/1], Step [126000/274360], Loss: 0.0000, Train time: 148.83\n",
      "Epoch [1/1], Step [126100/274360], Loss: 0.0000, Train time: 149.03\n",
      "Epoch [1/1], Step [126200/274360], Loss: 0.0001, Train time: 149.12\n",
      "Epoch [1/1], Step [126300/274360], Loss: 0.0000, Train time: 149.22\n",
      "Epoch [1/1], Step [126400/274360], Loss: 0.0000, Train time: 149.32\n",
      "Epoch [1/1], Step [126500/274360], Loss: 0.0001, Train time: 149.42\n",
      "Epoch [1/1], Step [126600/274360], Loss: 0.0000, Train time: 149.65\n",
      "Epoch [1/1], Step [126700/274360], Loss: 0.0017, Train time: 149.75\n",
      "Epoch [1/1], Step [126800/274360], Loss: 0.0001, Train time: 149.82\n",
      "Epoch [1/1], Step [126900/274360], Loss: 0.0000, Train time: 149.88\n",
      "Epoch [1/1], Step [127000/274360], Loss: 0.0001, Train time: 150.08\n",
      "Epoch [1/1], Step [127100/274360], Loss: 0.0010, Train time: 150.14\n",
      "Epoch [1/1], Step [127200/274360], Loss: 0.0004, Train time: 150.19\n",
      "Epoch [1/1], Step [127300/274360], Loss: 0.0000, Train time: 150.33\n",
      "Epoch [1/1], Step [127400/274360], Loss: 0.0001, Train time: 150.42\n",
      "Epoch [1/1], Step [127500/274360], Loss: 0.0013, Train time: 150.49\n",
      "Epoch [1/1], Step [127600/274360], Loss: 0.0001, Train time: 150.77\n",
      "Epoch [1/1], Step [127700/274360], Loss: 0.0000, Train time: 150.93\n",
      "Epoch [1/1], Step [127800/274360], Loss: 0.0000, Train time: 150.99\n",
      "Epoch [1/1], Step [127900/274360], Loss: 0.0000, Train time: 151.06\n",
      "Epoch [1/1], Step [128000/274360], Loss: 0.0004, Train time: 151.31\n",
      "Epoch [1/1], Step [128100/274360], Loss: 0.0000, Train time: 151.41\n",
      "Epoch [1/1], Step [128200/274360], Loss: 0.0001, Train time: 151.46\n",
      "Epoch [1/1], Step [128300/274360], Loss: 0.0071, Train time: 151.56\n",
      "Epoch [1/1], Step [128400/274360], Loss: 0.0050, Train time: 151.61\n",
      "Epoch [1/1], Step [128500/274360], Loss: 0.0000, Train time: 151.91\n",
      "Epoch [1/1], Step [128600/274360], Loss: 0.0000, Train time: 151.98\n",
      "Epoch [1/1], Step [128700/274360], Loss: 0.0000, Train time: 152.05\n",
      "Epoch [1/1], Step [128800/274360], Loss: 0.0000, Train time: 152.10\n",
      "Epoch [1/1], Step [128900/274360], Loss: 0.0000, Train time: 152.28\n",
      "Epoch [1/1], Step [129000/274360], Loss: 0.0007, Train time: 152.34\n",
      "Epoch [1/1], Step [129100/274360], Loss: 0.0015, Train time: 152.43\n",
      "Epoch [1/1], Step [129200/274360], Loss: 0.0007, Train time: 152.59\n",
      "Epoch [1/1], Step [129300/274360], Loss: 0.0003, Train time: 152.70\n",
      "Epoch [1/1], Step [129400/274360], Loss: 0.0000, Train time: 152.94\n",
      "Epoch [1/1], Step [129500/274360], Loss: 0.0000, Train time: 153.19\n",
      "Epoch [1/1], Step [129600/274360], Loss: 0.0001, Train time: 153.26\n",
      "Epoch [1/1], Step [129700/274360], Loss: 0.0016, Train time: 153.32\n",
      "Epoch [1/1], Step [129800/274360], Loss: 0.0000, Train time: 153.42\n",
      "Epoch [1/1], Step [129900/274360], Loss: 0.0011, Train time: 153.53\n",
      "Epoch [1/1], Step [130000/274360], Loss: 0.0000, Train time: 153.63\n",
      "Epoch [1/1], Step [130100/274360], Loss: 0.0009, Train time: 153.96\n",
      "Epoch [1/1], Step [130200/274360], Loss: 1.5318, Train time: 154.00\n",
      "Epoch [1/1], Step [130300/274360], Loss: 0.0012, Train time: 154.06\n",
      "Epoch [1/1], Step [130400/274360], Loss: 0.0002, Train time: 154.12\n",
      "Epoch [1/1], Step [130500/274360], Loss: 0.0000, Train time: 154.35\n",
      "Epoch [1/1], Step [130600/274360], Loss: 0.0002, Train time: 154.39\n",
      "Epoch [1/1], Step [130700/274360], Loss: 0.0001, Train time: 154.51\n",
      "Epoch [1/1], Step [130800/274360], Loss: 0.0028, Train time: 154.63\n",
      "Epoch [1/1], Step [130900/274360], Loss: 0.0000, Train time: 154.79\n",
      "Epoch [1/1], Step [131000/274360], Loss: 0.0003, Train time: 154.86\n",
      "Epoch [1/1], Step [131100/274360], Loss: 0.0002, Train time: 154.93\n",
      "Epoch [1/1], Step [131200/274360], Loss: 0.0004, Train time: 155.00\n",
      "Epoch [1/1], Step [131300/274360], Loss: 0.0000, Train time: 155.27\n",
      "Epoch [1/1], Step [131400/274360], Loss: 0.0016, Train time: 155.32\n",
      "Epoch [1/1], Step [131500/274360], Loss: 0.0000, Train time: 155.37\n",
      "Epoch [1/1], Step [131600/274360], Loss: 0.0008, Train time: 155.48\n",
      "Epoch [1/1], Step [131700/274360], Loss: 0.0000, Train time: 155.69\n",
      "Epoch [1/1], Step [131800/274360], Loss: 0.0003, Train time: 155.84\n",
      "Epoch [1/1], Step [131900/274360], Loss: 0.0024, Train time: 155.93\n",
      "Epoch [1/1], Step [132000/274360], Loss: 0.0000, Train time: 156.12\n",
      "Epoch [1/1], Step [132100/274360], Loss: 0.0000, Train time: 156.17\n",
      "Epoch [1/1], Step [132200/274360], Loss: 0.0012, Train time: 156.29\n",
      "Epoch [1/1], Step [132300/274360], Loss: 0.0020, Train time: 156.44\n",
      "Epoch [1/1], Step [132400/274360], Loss: 0.0010, Train time: 156.52\n",
      "Epoch [1/1], Step [132500/274360], Loss: 0.0000, Train time: 156.57\n",
      "Epoch [1/1], Step [132600/274360], Loss: 0.0000, Train time: 156.67\n",
      "Epoch [1/1], Step [132700/274360], Loss: 0.0000, Train time: 156.87\n",
      "Epoch [1/1], Step [132800/274360], Loss: 0.0000, Train time: 156.96\n",
      "Epoch [1/1], Step [132900/274360], Loss: 0.0000, Train time: 157.04\n",
      "Epoch [1/1], Step [133000/274360], Loss: 0.0000, Train time: 157.10\n",
      "Epoch [1/1], Step [133100/274360], Loss: 0.0016, Train time: 157.34\n",
      "Epoch [1/1], Step [133200/274360], Loss: 0.0000, Train time: 157.42\n",
      "Epoch [1/1], Step [133300/274360], Loss: 0.0000, Train time: 157.51\n",
      "Epoch [1/1], Step [133400/274360], Loss: 0.0000, Train time: 157.61\n",
      "Epoch [1/1], Step [133500/274360], Loss: 0.0001, Train time: 157.85\n",
      "Epoch [1/1], Step [133600/274360], Loss: 0.0000, Train time: 157.93\n",
      "Epoch [1/1], Step [133700/274360], Loss: 0.0002, Train time: 158.06\n",
      "Epoch [1/1], Step [133800/274360], Loss: 0.0000, Train time: 158.17\n",
      "Epoch [1/1], Step [133900/274360], Loss: 0.0010, Train time: 158.24\n",
      "Epoch [1/1], Step [134000/274360], Loss: 0.0004, Train time: 158.53\n",
      "Epoch [1/1], Step [134100/274360], Loss: 0.0000, Train time: 158.60\n",
      "Epoch [1/1], Step [134200/274360], Loss: 0.0001, Train time: 158.65\n",
      "Epoch [1/1], Step [134300/274360], Loss: 0.0000, Train time: 158.70\n",
      "Epoch [1/1], Step [134400/274360], Loss: 0.0000, Train time: 158.88\n",
      "Epoch [1/1], Step [134500/274360], Loss: 0.0000, Train time: 158.95\n",
      "Epoch [1/1], Step [134600/274360], Loss: 0.0064, Train time: 159.08\n",
      "Epoch [1/1], Step [134700/274360], Loss: 0.0000, Train time: 159.21\n",
      "Epoch [1/1], Step [134800/274360], Loss: 0.0001, Train time: 159.46\n",
      "Epoch [1/1], Step [134900/274360], Loss: 0.0004, Train time: 159.50\n",
      "Epoch [1/1], Step [135000/274360], Loss: 0.0342, Train time: 159.69\n",
      "Epoch [1/1], Step [135100/274360], Loss: 0.0004, Train time: 159.88\n",
      "Epoch [1/1], Step [135200/274360], Loss: 0.0000, Train time: 159.92\n",
      "Epoch [1/1], Step [135300/274360], Loss: 0.0001, Train time: 159.98\n",
      "Epoch [1/1], Step [135400/274360], Loss: 0.0011, Train time: 160.03\n",
      "Epoch [1/1], Step [135500/274360], Loss: 0.0009, Train time: 160.17\n",
      "Epoch [1/1], Step [135600/274360], Loss: 0.0002, Train time: 160.42\n",
      "Epoch [1/1], Step [135700/274360], Loss: 0.0000, Train time: 160.47\n",
      "Epoch [1/1], Step [135800/274360], Loss: 0.0001, Train time: 160.53\n",
      "Epoch [1/1], Step [135900/274360], Loss: 0.0000, Train time: 160.76\n",
      "Epoch [1/1], Step [136000/274360], Loss: 0.0000, Train time: 160.90\n",
      "Epoch [1/1], Step [136100/274360], Loss: 0.0006, Train time: 160.95\n",
      "Epoch [1/1], Step [136200/274360], Loss: 0.0000, Train time: 161.07\n",
      "Epoch [1/1], Step [136300/274360], Loss: 0.0008, Train time: 161.23\n",
      "Epoch [1/1], Step [136400/274360], Loss: 0.0000, Train time: 161.30\n",
      "Epoch [1/1], Step [136500/274360], Loss: 0.0004, Train time: 161.36\n",
      "Epoch [1/1], Step [136600/274360], Loss: 0.0000, Train time: 161.62\n",
      "Epoch [1/1], Step [136700/274360], Loss: 0.0003, Train time: 161.72\n",
      "Epoch [1/1], Step [136800/274360], Loss: 0.0003, Train time: 161.89\n",
      "Epoch [1/1], Step [136900/274360], Loss: 0.0000, Train time: 161.99\n",
      "Epoch [1/1], Step [137000/274360], Loss: 0.0001, Train time: 162.10\n",
      "Epoch [1/1], Step [137100/274360], Loss: 0.0017, Train time: 162.18\n",
      "Epoch [1/1], Step [137200/274360], Loss: 0.0000, Train time: 162.29\n",
      "Epoch [1/1], Step [137300/274360], Loss: 0.0101, Train time: 162.55\n",
      "Epoch [1/1], Step [137400/274360], Loss: 0.0162, Train time: 162.61\n",
      "Epoch [1/1], Step [137500/274360], Loss: 0.0000, Train time: 162.67\n",
      "Epoch [1/1], Step [137600/274360], Loss: 0.0000, Train time: 162.73\n",
      "Epoch [1/1], Step [137700/274360], Loss: 0.0000, Train time: 162.96\n",
      "Epoch [1/1], Step [137800/274360], Loss: 0.0000, Train time: 163.03\n",
      "Epoch [1/1], Step [137900/274360], Loss: 0.0013, Train time: 163.12\n",
      "Epoch [1/1], Step [138000/274360], Loss: 0.0001, Train time: 163.34\n",
      "Epoch [1/1], Step [138100/274360], Loss: 0.0001, Train time: 163.46\n",
      "Epoch [1/1], Step [138200/274360], Loss: 0.0003, Train time: 163.55\n",
      "Epoch [1/1], Step [138300/274360], Loss: 0.0014, Train time: 163.60\n",
      "Epoch [1/1], Step [138400/274360], Loss: 0.0003, Train time: 163.82\n",
      "Epoch [1/1], Step [138500/274360], Loss: 0.0019, Train time: 163.88\n",
      "Epoch [1/1], Step [138600/274360], Loss: 0.0000, Train time: 163.95\n",
      "Epoch [1/1], Step [138700/274360], Loss: 0.0052, Train time: 164.03\n",
      "Epoch [1/1], Step [138800/274360], Loss: 0.0000, Train time: 164.25\n",
      "Epoch [1/1], Step [138900/274360], Loss: 0.0000, Train time: 164.33\n",
      "Epoch [1/1], Step [139000/274360], Loss: 0.0001, Train time: 164.39\n",
      "Epoch [1/1], Step [139100/274360], Loss: 0.0008, Train time: 164.55\n",
      "Epoch [1/1], Step [139200/274360], Loss: 0.0000, Train time: 164.75\n",
      "Epoch [1/1], Step [139300/274360], Loss: 0.0000, Train time: 164.81\n",
      "Epoch [1/1], Step [139400/274360], Loss: 0.0001, Train time: 164.91\n",
      "Epoch [1/1], Step [139500/274360], Loss: 0.0001, Train time: 165.05\n",
      "Epoch [1/1], Step [139600/274360], Loss: 0.0000, Train time: 165.26\n",
      "Epoch [1/1], Step [139700/274360], Loss: 0.0001, Train time: 165.35\n",
      "Epoch [1/1], Step [139800/274360], Loss: 0.0022, Train time: 165.39\n",
      "Epoch [1/1], Step [139900/274360], Loss: 0.0007, Train time: 165.64\n",
      "Epoch [1/1], Step [140000/274360], Loss: 0.0006, Train time: 165.69\n",
      "Epoch [1/1], Step [140100/274360], Loss: 0.0000, Train time: 165.81\n",
      "Epoch [1/1], Step [140200/274360], Loss: 0.0003, Train time: 165.89\n",
      "Epoch [1/1], Step [140300/274360], Loss: 0.0001, Train time: 166.15\n",
      "Epoch [1/1], Step [140400/274360], Loss: 0.0000, Train time: 166.23\n",
      "Epoch [1/1], Step [140500/274360], Loss: 0.0001, Train time: 166.42\n",
      "Epoch [1/1], Step [140600/274360], Loss: 0.0032, Train time: 166.65\n",
      "Epoch [1/1], Step [140700/274360], Loss: 0.0010, Train time: 166.74\n",
      "Epoch [1/1], Step [140800/274360], Loss: 0.0119, Train time: 166.79\n",
      "Epoch [1/1], Step [140900/274360], Loss: 0.0001, Train time: 166.95\n",
      "Epoch [1/1], Step [141000/274360], Loss: 0.0000, Train time: 167.05\n",
      "Epoch [1/1], Step [141100/274360], Loss: 0.0001, Train time: 167.36\n",
      "Epoch [1/1], Step [141200/274360], Loss: 0.0063, Train time: 167.40\n",
      "Epoch [1/1], Step [141300/274360], Loss: 0.0000, Train time: 167.46\n",
      "Epoch [1/1], Step [141400/274360], Loss: 0.0002, Train time: 167.54\n",
      "Epoch [1/1], Step [141500/274360], Loss: 0.0000, Train time: 167.62\n",
      "Epoch [1/1], Step [141600/274360], Loss: 0.0000, Train time: 167.73\n",
      "Epoch [1/1], Step [141700/274360], Loss: 0.0369, Train time: 167.95\n",
      "Epoch [1/1], Step [141800/274360], Loss: 0.0001, Train time: 168.00\n",
      "Epoch [1/1], Step [141900/274360], Loss: 0.0002, Train time: 168.10\n",
      "Epoch [1/1], Step [142000/274360], Loss: 0.0000, Train time: 168.18\n",
      "Epoch [1/1], Step [142100/274360], Loss: 0.0000, Train time: 168.26\n",
      "Epoch [1/1], Step [142200/274360], Loss: 0.0000, Train time: 168.54\n",
      "Epoch [1/1], Step [142300/274360], Loss: 0.0000, Train time: 168.58\n",
      "Epoch [1/1], Step [142400/274360], Loss: 0.0012, Train time: 168.66\n",
      "Epoch [1/1], Step [142500/274360], Loss: 0.0000, Train time: 168.76\n",
      "Epoch [1/1], Step [142600/274360], Loss: 0.0000, Train time: 168.95\n",
      "Epoch [1/1], Step [142700/274360], Loss: 0.0009, Train time: 169.02\n",
      "Epoch [1/1], Step [142800/274360], Loss: 0.0013, Train time: 169.15\n",
      "Epoch [1/1], Step [142900/274360], Loss: 0.0000, Train time: 169.26\n",
      "Epoch [1/1], Step [143000/274360], Loss: 0.0014, Train time: 169.43\n",
      "Epoch [1/1], Step [143100/274360], Loss: 0.0014, Train time: 169.57\n",
      "Epoch [1/1], Step [143200/274360], Loss: 0.0130, Train time: 169.79\n",
      "Epoch [1/1], Step [143300/274360], Loss: 0.0003, Train time: 169.84\n",
      "Epoch [1/1], Step [143400/274360], Loss: 0.0173, Train time: 169.88\n",
      "Epoch [1/1], Step [143500/274360], Loss: 0.0000, Train time: 169.93\n",
      "Epoch [1/1], Step [143600/274360], Loss: 0.0001, Train time: 170.19\n",
      "Epoch [1/1], Step [143700/274360], Loss: 0.0000, Train time: 170.25\n",
      "Epoch [1/1], Step [143800/274360], Loss: 0.0050, Train time: 170.30\n",
      "Epoch [1/1], Step [143900/274360], Loss: 0.0000, Train time: 170.39\n",
      "Epoch [1/1], Step [144000/274360], Loss: 0.0000, Train time: 170.58\n",
      "Epoch [1/1], Step [144100/274360], Loss: 0.0000, Train time: 170.74\n",
      "Epoch [1/1], Step [144200/274360], Loss: 0.0000, Train time: 170.82\n",
      "Epoch [1/1], Step [144300/274360], Loss: 0.0000, Train time: 170.87\n",
      "Epoch [1/1], Step [144400/274360], Loss: 0.0001, Train time: 170.99\n",
      "Epoch [1/1], Step [144500/274360], Loss: 0.0000, Train time: 171.34\n",
      "Epoch [1/1], Step [144600/274360], Loss: 0.0000, Train time: 171.41\n",
      "Epoch [1/1], Step [144700/274360], Loss: 0.0000, Train time: 171.47\n",
      "Epoch [1/1], Step [144800/274360], Loss: 0.0000, Train time: 171.64\n",
      "Epoch [1/1], Step [144900/274360], Loss: 0.0002, Train time: 171.79\n",
      "Epoch [1/1], Step [145000/274360], Loss: 0.0032, Train time: 171.83\n",
      "Epoch [1/1], Step [145100/274360], Loss: 0.0000, Train time: 171.95\n",
      "Epoch [1/1], Step [145200/274360], Loss: 0.0003, Train time: 172.10\n",
      "Epoch [1/1], Step [145300/274360], Loss: 0.0001, Train time: 172.16\n",
      "Epoch [1/1], Step [145400/274360], Loss: 0.0009, Train time: 172.21\n",
      "Epoch [1/1], Step [145500/274360], Loss: 0.0001, Train time: 172.32\n",
      "Epoch [1/1], Step [145600/274360], Loss: 0.0001, Train time: 172.41\n",
      "Epoch [1/1], Step [145700/274360], Loss: 0.0000, Train time: 172.73\n",
      "Epoch [1/1], Step [145800/274360], Loss: 0.0003, Train time: 172.80\n",
      "Epoch [1/1], Step [145900/274360], Loss: 0.0000, Train time: 172.84\n",
      "Epoch [1/1], Step [146000/274360], Loss: 0.0011, Train time: 172.90\n",
      "Epoch [1/1], Step [146100/274360], Loss: 0.0000, Train time: 173.11\n",
      "Epoch [1/1], Step [146200/274360], Loss: 0.2465, Train time: 173.22\n",
      "Epoch [1/1], Step [146300/274360], Loss: 0.0000, Train time: 173.32\n",
      "Epoch [1/1], Step [146400/274360], Loss: 0.0000, Train time: 173.40\n",
      "Epoch [1/1], Step [146500/274360], Loss: 0.0001, Train time: 173.52\n",
      "Epoch [1/1], Step [146600/274360], Loss: 0.0000, Train time: 173.76\n",
      "Epoch [1/1], Step [146700/274360], Loss: 0.0004, Train time: 173.87\n",
      "Epoch [1/1], Step [146800/274360], Loss: 0.0000, Train time: 173.95\n",
      "Epoch [1/1], Step [146900/274360], Loss: 0.0000, Train time: 174.17\n",
      "Epoch [1/1], Step [147000/274360], Loss: 0.0007, Train time: 174.27\n",
      "Epoch [1/1], Step [147100/274360], Loss: 0.0001, Train time: 174.34\n",
      "Epoch [1/1], Step [147200/274360], Loss: 0.0001, Train time: 174.55\n",
      "Epoch [1/1], Step [147300/274360], Loss: 0.0004, Train time: 174.61\n",
      "Epoch [1/1], Step [147400/274360], Loss: 0.0001, Train time: 174.68\n",
      "Epoch [1/1], Step [147500/274360], Loss: 0.0000, Train time: 174.80\n",
      "Epoch [1/1], Step [147600/274360], Loss: 0.0000, Train time: 174.96\n",
      "Epoch [1/1], Step [147700/274360], Loss: 0.0000, Train time: 175.05\n",
      "Epoch [1/1], Step [147800/274360], Loss: 0.0001, Train time: 175.26\n",
      "Epoch [1/1], Step [147900/274360], Loss: 0.0001, Train time: 175.31\n",
      "Epoch [1/1], Step [148000/274360], Loss: 0.0000, Train time: 175.54\n",
      "Epoch [1/1], Step [148100/274360], Loss: 0.0003, Train time: 175.61\n",
      "Epoch [1/1], Step [148200/274360], Loss: 0.0000, Train time: 175.75\n",
      "Epoch [1/1], Step [148300/274360], Loss: 0.0035, Train time: 175.96\n",
      "Epoch [1/1], Step [148400/274360], Loss: 0.0000, Train time: 176.00\n",
      "Epoch [1/1], Step [148500/274360], Loss: 0.0000, Train time: 176.16\n",
      "Epoch [1/1], Step [148600/274360], Loss: 0.0021, Train time: 176.27\n",
      "Epoch [1/1], Step [148700/274360], Loss: 0.0016, Train time: 176.36\n",
      "Epoch [1/1], Step [148800/274360], Loss: 0.0000, Train time: 176.53\n",
      "Epoch [1/1], Step [148900/274360], Loss: 0.0000, Train time: 176.58\n",
      "Epoch [1/1], Step [149000/274360], Loss: 0.0000, Train time: 176.67\n",
      "Epoch [1/1], Step [149100/274360], Loss: 0.0000, Train time: 176.94\n",
      "Epoch [1/1], Step [149200/274360], Loss: 0.0039, Train time: 177.13\n",
      "Epoch [1/1], Step [149300/274360], Loss: 0.0004, Train time: 177.19\n",
      "Epoch [1/1], Step [149400/274360], Loss: 0.0005, Train time: 177.27\n",
      "Epoch [1/1], Step [149500/274360], Loss: 0.0416, Train time: 177.35\n",
      "Epoch [1/1], Step [149600/274360], Loss: 0.0684, Train time: 177.64\n",
      "Epoch [1/1], Step [149700/274360], Loss: 0.0000, Train time: 177.70\n",
      "Epoch [1/1], Step [149800/274360], Loss: 0.0005, Train time: 177.75\n",
      "Epoch [1/1], Step [149900/274360], Loss: 0.0004, Train time: 177.84\n",
      "Epoch [1/1], Step [150000/274360], Loss: 0.0000, Train time: 178.07\n",
      "Epoch [1/1], Step [150100/274360], Loss: 0.0000, Train time: 178.11\n",
      "Epoch [1/1], Step [150200/274360], Loss: 0.0005, Train time: 178.27\n",
      "Epoch [1/1], Step [150300/274360], Loss: 0.0013, Train time: 178.44\n",
      "Epoch [1/1], Step [150400/274360], Loss: 0.0000, Train time: 178.49\n",
      "Epoch [1/1], Step [150500/274360], Loss: 0.0001, Train time: 178.56\n",
      "Epoch [1/1], Step [150600/274360], Loss: 0.0002, Train time: 178.66\n",
      "Epoch [1/1], Step [150700/274360], Loss: 0.0008, Train time: 178.89\n",
      "Epoch [1/1], Step [150800/274360], Loss: 0.0000, Train time: 178.96\n",
      "Epoch [1/1], Step [150900/274360], Loss: 0.0017, Train time: 179.04\n",
      "Epoch [1/1], Step [151000/274360], Loss: 0.0163, Train time: 179.18\n",
      "Epoch [1/1], Step [151100/274360], Loss: 0.0008, Train time: 179.30\n",
      "Epoch [1/1], Step [151200/274360], Loss: 0.0268, Train time: 179.43\n",
      "Epoch [1/1], Step [151300/274360], Loss: 0.0000, Train time: 179.52\n",
      "Epoch [1/1], Step [151400/274360], Loss: 0.0000, Train time: 179.77\n",
      "Epoch [1/1], Step [151500/274360], Loss: 0.0000, Train time: 179.82\n",
      "Epoch [1/1], Step [151600/274360], Loss: 0.0000, Train time: 179.87\n",
      "Epoch [1/1], Step [151700/274360], Loss: 0.0001, Train time: 179.93\n",
      "Epoch [1/1], Step [151800/274360], Loss: 0.0001, Train time: 180.13\n",
      "Epoch [1/1], Step [151900/274360], Loss: 0.0001, Train time: 180.33\n",
      "Epoch [1/1], Step [152000/274360], Loss: 0.0000, Train time: 180.41\n",
      "Epoch [1/1], Step [152100/274360], Loss: 0.0000, Train time: 180.53\n",
      "Epoch [1/1], Step [152200/274360], Loss: 0.0026, Train time: 180.59\n",
      "Epoch [1/1], Step [152300/274360], Loss: 0.0000, Train time: 180.67\n",
      "Epoch [1/1], Step [152400/274360], Loss: 0.0040, Train time: 180.92\n",
      "Epoch [1/1], Step [152500/274360], Loss: 0.0004, Train time: 181.22\n",
      "Epoch [1/1], Step [152600/274360], Loss: 0.0003, Train time: 181.31\n",
      "Epoch [1/1], Step [152700/274360], Loss: 0.0000, Train time: 181.35\n",
      "Epoch [1/1], Step [152800/274360], Loss: 0.0001, Train time: 181.40\n",
      "Epoch [1/1], Step [152900/274360], Loss: 0.0001, Train time: 181.48\n",
      "Epoch [1/1], Step [153000/274360], Loss: 0.0153, Train time: 181.74\n",
      "Epoch [1/1], Step [153100/274360], Loss: 0.0002, Train time: 181.79\n",
      "Epoch [1/1], Step [153200/274360], Loss: 0.0048, Train time: 181.88\n",
      "Epoch [1/1], Step [153300/274360], Loss: 0.0000, Train time: 181.96\n",
      "Epoch [1/1], Step [153400/274360], Loss: 0.0448, Train time: 182.14\n",
      "Epoch [1/1], Step [153500/274360], Loss: 0.0000, Train time: 182.25\n",
      "Epoch [1/1], Step [153600/274360], Loss: 0.0204, Train time: 182.39\n",
      "Epoch [1/1], Step [153700/274360], Loss: 0.0000, Train time: 182.63\n",
      "Epoch [1/1], Step [153800/274360], Loss: 0.0001, Train time: 182.70\n",
      "Epoch [1/1], Step [153900/274360], Loss: 0.0006, Train time: 182.77\n",
      "Epoch [1/1], Step [154000/274360], Loss: 0.0000, Train time: 182.84\n",
      "Epoch [1/1], Step [154100/274360], Loss: 0.0001, Train time: 182.97\n",
      "Epoch [1/1], Step [154200/274360], Loss: 0.0002, Train time: 183.08\n",
      "Epoch [1/1], Step [154300/274360], Loss: 0.0000, Train time: 183.20\n",
      "Epoch [1/1], Step [154400/274360], Loss: 0.0000, Train time: 183.45\n",
      "Epoch [1/1], Step [154500/274360], Loss: 0.0000, Train time: 183.51\n",
      "Epoch [1/1], Step [154600/274360], Loss: 0.0000, Train time: 183.59\n",
      "Epoch [1/1], Step [154700/274360], Loss: 0.0017, Train time: 183.64\n",
      "Epoch [1/1], Step [154800/274360], Loss: 0.0545, Train time: 183.90\n",
      "Epoch [1/1], Step [154900/274360], Loss: 0.0002, Train time: 183.98\n",
      "Epoch [1/1], Step [155000/274360], Loss: 0.0000, Train time: 184.06\n",
      "Epoch [1/1], Step [155100/274360], Loss: 0.0001, Train time: 184.12\n",
      "Epoch [1/1], Step [155200/274360], Loss: 0.0094, Train time: 184.32\n",
      "Epoch [1/1], Step [155300/274360], Loss: 0.0001, Train time: 184.36\n",
      "Epoch [1/1], Step [155400/274360], Loss: 0.0000, Train time: 184.41\n",
      "Epoch [1/1], Step [155500/274360], Loss: 0.0002, Train time: 184.52\n",
      "Epoch [1/1], Step [155600/274360], Loss: 0.0000, Train time: 184.91\n",
      "Epoch [1/1], Step [155700/274360], Loss: 0.0000, Train time: 185.05\n",
      "Epoch [1/1], Step [155800/274360], Loss: 0.0000, Train time: 185.15\n",
      "Epoch [1/1], Step [155900/274360], Loss: 0.0006, Train time: 185.24\n",
      "Epoch [1/1], Step [156000/274360], Loss: 0.0033, Train time: 185.36\n",
      "Epoch [1/1], Step [156100/274360], Loss: 0.0001, Train time: 185.54\n",
      "Epoch [1/1], Step [156200/274360], Loss: 0.0002, Train time: 185.60\n",
      "Epoch [1/1], Step [156300/274360], Loss: 0.0775, Train time: 185.70\n",
      "Epoch [1/1], Step [156400/274360], Loss: 0.0000, Train time: 185.92\n",
      "Epoch [1/1], Step [156500/274360], Loss: 0.0000, Train time: 185.97\n",
      "Epoch [1/1], Step [156600/274360], Loss: 0.0000, Train time: 186.02\n",
      "Epoch [1/1], Step [156700/274360], Loss: 0.0000, Train time: 186.12\n",
      "Epoch [1/1], Step [156800/274360], Loss: 0.0000, Train time: 186.29\n",
      "Epoch [1/1], Step [156900/274360], Loss: 0.0000, Train time: 186.47\n",
      "Epoch [1/1], Step [157000/274360], Loss: 0.0000, Train time: 186.53\n",
      "Epoch [1/1], Step [157100/274360], Loss: 0.0003, Train time: 186.58\n",
      "Epoch [1/1], Step [157200/274360], Loss: 0.0000, Train time: 186.67\n",
      "Epoch [1/1], Step [157300/274360], Loss: 0.0003, Train time: 186.88\n",
      "Epoch [1/1], Step [157400/274360], Loss: 0.0000, Train time: 186.92\n",
      "Epoch [1/1], Step [157500/274360], Loss: 0.0000, Train time: 187.02\n",
      "Epoch [1/1], Step [157600/274360], Loss: 0.0000, Train time: 187.08\n",
      "Epoch [1/1], Step [157700/274360], Loss: 0.0001, Train time: 187.32\n",
      "Epoch [1/1], Step [157800/274360], Loss: 0.0000, Train time: 187.46\n",
      "Epoch [1/1], Step [157900/274360], Loss: 0.0000, Train time: 187.61\n",
      "Epoch [1/1], Step [158000/274360], Loss: 0.0001, Train time: 187.68\n",
      "Epoch [1/1], Step [158100/274360], Loss: 0.0000, Train time: 187.87\n",
      "Epoch [1/1], Step [158200/274360], Loss: 0.0000, Train time: 188.04\n",
      "Epoch [1/1], Step [158300/274360], Loss: 0.0000, Train time: 188.18\n",
      "Epoch [1/1], Step [158400/274360], Loss: 0.0000, Train time: 188.29\n",
      "Epoch [1/1], Step [158500/274360], Loss: 0.0000, Train time: 188.35\n",
      "Epoch [1/1], Step [158600/274360], Loss: 0.0039, Train time: 188.53\n",
      "Epoch [1/1], Step [158700/274360], Loss: 0.0032, Train time: 188.67\n",
      "Epoch [1/1], Step [158800/274360], Loss: 0.0000, Train time: 188.73\n",
      "Epoch [1/1], Step [158900/274360], Loss: 0.0001, Train time: 189.01\n",
      "Epoch [1/1], Step [159000/274360], Loss: 0.0009, Train time: 189.18\n",
      "Epoch [1/1], Step [159100/274360], Loss: 0.0001, Train time: 189.22\n",
      "Epoch [1/1], Step [159200/274360], Loss: 0.0000, Train time: 189.31\n",
      "Epoch [1/1], Step [159300/274360], Loss: 0.0374, Train time: 189.52\n",
      "Epoch [1/1], Step [159400/274360], Loss: 0.0006, Train time: 189.56\n",
      "Epoch [1/1], Step [159500/274360], Loss: 0.0000, Train time: 189.60\n",
      "Epoch [1/1], Step [159600/274360], Loss: 0.0001, Train time: 189.77\n",
      "Epoch [1/1], Step [159700/274360], Loss: 0.0000, Train time: 189.83\n",
      "Epoch [1/1], Step [159800/274360], Loss: 0.0000, Train time: 189.98\n",
      "Epoch [1/1], Step [159900/274360], Loss: 0.0000, Train time: 190.08\n",
      "Epoch [1/1], Step [160000/274360], Loss: 0.0001, Train time: 190.31\n",
      "Epoch [1/1], Step [160100/274360], Loss: 0.0000, Train time: 190.39\n",
      "Epoch [1/1], Step [160200/274360], Loss: 0.0000, Train time: 190.44\n",
      "Epoch [1/1], Step [160300/274360], Loss: 0.0000, Train time: 190.57\n",
      "Epoch [1/1], Step [160400/274360], Loss: 0.0001, Train time: 190.72\n",
      "Epoch [1/1], Step [160500/274360], Loss: 0.0000, Train time: 190.78\n",
      "Epoch [1/1], Step [160600/274360], Loss: 0.0007, Train time: 191.19\n",
      "Epoch [1/1], Step [160700/274360], Loss: 0.0105, Train time: 191.23\n",
      "Epoch [1/1], Step [160800/274360], Loss: 0.0005, Train time: 191.28\n",
      "Epoch [1/1], Step [160900/274360], Loss: 0.0001, Train time: 191.38\n",
      "Epoch [1/1], Step [161000/274360], Loss: 0.0000, Train time: 191.43\n",
      "Epoch [1/1], Step [161100/274360], Loss: 0.0058, Train time: 191.82\n",
      "Epoch [1/1], Step [161200/274360], Loss: 0.0000, Train time: 191.87\n",
      "Epoch [1/1], Step [161300/274360], Loss: 0.0003, Train time: 191.93\n",
      "Epoch [1/1], Step [161400/274360], Loss: 0.0000, Train time: 191.99\n",
      "Epoch [1/1], Step [161500/274360], Loss: 0.0000, Train time: 192.29\n",
      "Epoch [1/1], Step [161600/274360], Loss: 0.0004, Train time: 192.36\n",
      "Epoch [1/1], Step [161700/274360], Loss: 0.0000, Train time: 192.45\n",
      "Epoch [1/1], Step [161800/274360], Loss: 0.0000, Train time: 192.51\n",
      "Epoch [1/1], Step [161900/274360], Loss: 0.0007, Train time: 192.77\n",
      "Epoch [1/1], Step [162000/274360], Loss: 0.0011, Train time: 192.83\n",
      "Epoch [1/1], Step [162100/274360], Loss: 0.0000, Train time: 192.89\n",
      "Epoch [1/1], Step [162200/274360], Loss: 0.0001, Train time: 193.08\n",
      "Epoch [1/1], Step [162300/274360], Loss: 0.0036, Train time: 193.41\n",
      "Epoch [1/1], Step [162400/274360], Loss: 0.0000, Train time: 193.46\n",
      "Epoch [1/1], Step [162500/274360], Loss: 0.0075, Train time: 193.71\n",
      "Epoch [1/1], Step [162600/274360], Loss: 0.0001, Train time: 194.01\n",
      "Epoch [1/1], Step [162700/274360], Loss: 0.0003, Train time: 194.09\n",
      "Epoch [1/1], Step [162800/274360], Loss: 0.0000, Train time: 194.16\n",
      "Epoch [1/1], Step [162900/274360], Loss: 0.0001, Train time: 194.26\n",
      "Epoch [1/1], Step [163000/274360], Loss: 0.0009, Train time: 194.47\n",
      "Epoch [1/1], Step [163100/274360], Loss: 0.0017, Train time: 194.53\n",
      "Epoch [1/1], Step [163200/274360], Loss: 0.0000, Train time: 194.60\n",
      "Epoch [1/1], Step [163300/274360], Loss: 0.0000, Train time: 194.68\n",
      "Epoch [1/1], Step [163400/274360], Loss: 0.0000, Train time: 194.80\n",
      "Epoch [1/1], Step [163500/274360], Loss: 0.0008, Train time: 195.05\n",
      "Epoch [1/1], Step [163600/274360], Loss: 0.0083, Train time: 195.09\n",
      "Epoch [1/1], Step [163700/274360], Loss: 0.0000, Train time: 195.20\n",
      "Epoch [1/1], Step [163800/274360], Loss: 0.0019, Train time: 195.26\n",
      "Epoch [1/1], Step [163900/274360], Loss: 0.0003, Train time: 195.33\n",
      "Epoch [1/1], Step [164000/274360], Loss: 0.0000, Train time: 195.60\n",
      "Epoch [1/1], Step [164100/274360], Loss: 0.0000, Train time: 195.67\n",
      "Epoch [1/1], Step [164200/274360], Loss: 0.0002, Train time: 195.78\n",
      "Epoch [1/1], Step [164300/274360], Loss: 0.0035, Train time: 195.83\n",
      "Epoch [1/1], Step [164400/274360], Loss: 0.0001, Train time: 196.15\n",
      "Epoch [1/1], Step [164500/274360], Loss: 0.0000, Train time: 196.28\n",
      "Epoch [1/1], Step [164600/274360], Loss: 0.0001, Train time: 196.36\n",
      "Epoch [1/1], Step [164700/274360], Loss: 0.0003, Train time: 196.46\n",
      "Epoch [1/1], Step [164800/274360], Loss: 0.0000, Train time: 196.58\n",
      "Epoch [1/1], Step [164900/274360], Loss: 0.0003, Train time: 196.69\n",
      "Epoch [1/1], Step [165000/274360], Loss: 0.0001, Train time: 196.93\n",
      "Epoch [1/1], Step [165100/274360], Loss: 0.0000, Train time: 196.99\n",
      "Epoch [1/1], Step [165200/274360], Loss: 0.0000, Train time: 197.03\n",
      "Epoch [1/1], Step [165300/274360], Loss: 0.0000, Train time: 197.15\n",
      "Epoch [1/1], Step [165400/274360], Loss: 0.0000, Train time: 197.21\n",
      "Epoch [1/1], Step [165500/274360], Loss: 0.0004, Train time: 197.40\n",
      "Epoch [1/1], Step [165600/274360], Loss: 0.0000, Train time: 197.61\n",
      "Epoch [1/1], Step [165700/274360], Loss: 0.0000, Train time: 197.68\n",
      "Epoch [1/1], Step [165800/274360], Loss: 0.0000, Train time: 197.73\n",
      "Epoch [1/1], Step [165900/274360], Loss: 0.0000, Train time: 197.78\n",
      "Epoch [1/1], Step [166000/274360], Loss: 0.0144, Train time: 197.87\n",
      "Epoch [1/1], Step [166100/274360], Loss: 0.0000, Train time: 198.05\n",
      "Epoch [1/1], Step [166200/274360], Loss: 0.0001, Train time: 198.14\n",
      "Epoch [1/1], Step [166300/274360], Loss: 0.0000, Train time: 198.50\n",
      "Epoch [1/1], Step [166400/274360], Loss: 0.0006, Train time: 198.55\n",
      "Epoch [1/1], Step [166500/274360], Loss: 0.0000, Train time: 198.63\n",
      "Epoch [1/1], Step [166600/274360], Loss: 0.0001, Train time: 198.67\n",
      "Epoch [1/1], Step [166700/274360], Loss: 0.0003, Train time: 198.90\n",
      "Epoch [1/1], Step [166800/274360], Loss: 0.0000, Train time: 198.98\n",
      "Epoch [1/1], Step [166900/274360], Loss: 0.0015, Train time: 199.03\n",
      "Epoch [1/1], Step [167000/274360], Loss: 0.0000, Train time: 199.12\n",
      "Epoch [1/1], Step [167100/274360], Loss: 0.0000, Train time: 199.32\n",
      "Epoch [1/1], Step [167200/274360], Loss: 0.0861, Train time: 199.36\n",
      "Epoch [1/1], Step [167300/274360], Loss: 0.0001, Train time: 199.76\n",
      "Epoch [1/1], Step [167400/274360], Loss: 0.0092, Train time: 199.81\n",
      "Epoch [1/1], Step [167500/274360], Loss: 0.0000, Train time: 200.05\n",
      "Epoch [1/1], Step [167600/274360], Loss: 0.0006, Train time: 200.13\n",
      "Epoch [1/1], Step [167700/274360], Loss: 0.0000, Train time: 200.18\n",
      "Epoch [1/1], Step [167800/274360], Loss: 0.0004, Train time: 200.26\n",
      "Epoch [1/1], Step [167900/274360], Loss: 0.0000, Train time: 200.51\n",
      "Epoch [1/1], Step [168000/274360], Loss: 0.0000, Train time: 200.60\n",
      "Epoch [1/1], Step [168100/274360], Loss: 0.0165, Train time: 200.67\n",
      "Epoch [1/1], Step [168200/274360], Loss: 0.0002, Train time: 200.77\n",
      "Epoch [1/1], Step [168300/274360], Loss: 0.0073, Train time: 201.02\n",
      "Epoch [1/1], Step [168400/274360], Loss: 0.0026, Train time: 201.17\n",
      "Epoch [1/1], Step [168500/274360], Loss: 0.0001, Train time: 201.36\n",
      "Epoch [1/1], Step [168600/274360], Loss: 0.0000, Train time: 201.40\n",
      "Epoch [1/1], Step [168700/274360], Loss: 0.0000, Train time: 201.56\n",
      "Epoch [1/1], Step [168800/274360], Loss: 0.0004, Train time: 201.63\n",
      "Epoch [1/1], Step [168900/274360], Loss: 0.0005, Train time: 201.73\n",
      "Epoch [1/1], Step [169000/274360], Loss: 0.0006, Train time: 201.85\n",
      "Epoch [1/1], Step [169100/274360], Loss: 0.0000, Train time: 202.19\n",
      "Epoch [1/1], Step [169200/274360], Loss: 0.0001, Train time: 202.35\n",
      "Epoch [1/1], Step [169300/274360], Loss: 0.0000, Train time: 202.40\n",
      "Epoch [1/1], Step [169400/274360], Loss: 0.0000, Train time: 202.44\n",
      "Epoch [1/1], Step [169500/274360], Loss: 0.0000, Train time: 202.68\n",
      "Epoch [1/1], Step [169600/274360], Loss: 0.0056, Train time: 202.73\n",
      "Epoch [1/1], Step [169700/274360], Loss: 0.0214, Train time: 202.78\n",
      "Epoch [1/1], Step [169800/274360], Loss: 0.0000, Train time: 202.89\n",
      "Epoch [1/1], Step [169900/274360], Loss: 0.0000, Train time: 203.03\n",
      "Epoch [1/1], Step [170000/274360], Loss: 0.0000, Train time: 203.11\n",
      "Epoch [1/1], Step [170100/274360], Loss: 0.0001, Train time: 203.16\n",
      "Epoch [1/1], Step [170200/274360], Loss: 0.0000, Train time: 203.27\n",
      "Epoch [1/1], Step [170300/274360], Loss: 0.0000, Train time: 203.37\n",
      "Epoch [1/1], Step [170400/274360], Loss: 0.0000, Train time: 203.51\n",
      "Epoch [1/1], Step [170500/274360], Loss: 0.0003, Train time: 203.70\n",
      "Epoch [1/1], Step [170600/274360], Loss: 0.0000, Train time: 203.76\n",
      "Epoch [1/1], Step [170700/274360], Loss: 0.0027, Train time: 203.83\n",
      "Epoch [1/1], Step [170800/274360], Loss: 0.0002, Train time: 203.90\n",
      "Epoch [1/1], Step [170900/274360], Loss: 0.0000, Train time: 204.13\n",
      "Epoch [1/1], Step [171000/274360], Loss: 0.0000, Train time: 204.17\n",
      "Epoch [1/1], Step [171100/274360], Loss: 0.0000, Train time: 204.23\n",
      "Epoch [1/1], Step [171200/274360], Loss: 0.0000, Train time: 204.30\n",
      "Epoch [1/1], Step [171300/274360], Loss: 0.0005, Train time: 204.40\n",
      "Epoch [1/1], Step [171400/274360], Loss: 0.0000, Train time: 204.47\n",
      "Epoch [1/1], Step [171500/274360], Loss: 0.0000, Train time: 204.74\n",
      "Epoch [1/1], Step [171600/274360], Loss: 0.0000, Train time: 204.82\n",
      "Epoch [1/1], Step [171700/274360], Loss: 0.0027, Train time: 204.89\n",
      "Epoch [1/1], Step [171800/274360], Loss: 0.0000, Train time: 205.05\n",
      "Epoch [1/1], Step [171900/274360], Loss: 0.0000, Train time: 205.15\n",
      "Epoch [1/1], Step [172000/274360], Loss: 0.0001, Train time: 205.27\n",
      "Epoch [1/1], Step [172100/274360], Loss: 0.0000, Train time: 205.35\n",
      "Epoch [1/1], Step [172200/274360], Loss: 0.0008, Train time: 205.62\n",
      "Epoch [1/1], Step [172300/274360], Loss: 0.0001, Train time: 205.69\n",
      "Epoch [1/1], Step [172400/274360], Loss: 0.0000, Train time: 205.78\n",
      "Epoch [1/1], Step [172500/274360], Loss: 0.0001, Train time: 205.92\n",
      "Epoch [1/1], Step [172600/274360], Loss: 0.0001, Train time: 205.99\n",
      "Epoch [1/1], Step [172700/274360], Loss: 0.0000, Train time: 206.07\n",
      "Epoch [1/1], Step [172800/274360], Loss: 0.0017, Train time: 206.21\n",
      "Epoch [1/1], Step [172900/274360], Loss: 0.0001, Train time: 206.30\n",
      "Epoch [1/1], Step [173000/274360], Loss: 0.0006, Train time: 206.40\n",
      "Epoch [1/1], Step [173100/274360], Loss: 0.0020, Train time: 206.52\n",
      "Epoch [1/1], Step [173200/274360], Loss: 0.0032, Train time: 206.59\n",
      "Epoch [1/1], Step [173300/274360], Loss: 0.0000, Train time: 206.84\n",
      "Epoch [1/1], Step [173400/274360], Loss: 0.0000, Train time: 206.88\n",
      "Epoch [1/1], Step [173500/274360], Loss: 0.0002, Train time: 206.95\n",
      "Epoch [1/1], Step [173600/274360], Loss: 0.0000, Train time: 207.07\n",
      "Epoch [1/1], Step [173700/274360], Loss: 0.0004, Train time: 207.15\n",
      "Epoch [1/1], Step [173800/274360], Loss: 0.0000, Train time: 207.31\n",
      "Epoch [1/1], Step [173900/274360], Loss: 0.0003, Train time: 207.39\n",
      "Epoch [1/1], Step [174000/274360], Loss: 0.0000, Train time: 207.47\n",
      "Epoch [1/1], Step [174100/274360], Loss: 0.0001, Train time: 207.68\n",
      "Epoch [1/1], Step [174200/274360], Loss: 0.0001, Train time: 207.76\n",
      "Epoch [1/1], Step [174300/274360], Loss: 0.0001, Train time: 207.80\n",
      "Epoch [1/1], Step [174400/274360], Loss: 0.0004, Train time: 207.88\n",
      "Epoch [1/1], Step [174500/274360], Loss: 0.0000, Train time: 207.97\n",
      "Epoch [1/1], Step [174600/274360], Loss: 0.0001, Train time: 208.13\n",
      "Epoch [1/1], Step [174700/274360], Loss: 0.0003, Train time: 208.42\n",
      "Epoch [1/1], Step [174800/274360], Loss: 0.0001, Train time: 208.46\n",
      "Epoch [1/1], Step [174900/274360], Loss: 0.0000, Train time: 208.51\n",
      "Epoch [1/1], Step [175000/274360], Loss: 0.0024, Train time: 208.57\n",
      "Epoch [1/1], Step [175100/274360], Loss: 0.0002, Train time: 208.76\n",
      "Epoch [1/1], Step [175200/274360], Loss: 0.0000, Train time: 208.87\n",
      "Epoch [1/1], Step [175300/274360], Loss: 0.0000, Train time: 208.97\n",
      "Epoch [1/1], Step [175400/274360], Loss: 0.0000, Train time: 209.20\n",
      "Epoch [1/1], Step [175500/274360], Loss: 0.0000, Train time: 209.29\n",
      "Epoch [1/1], Step [175600/274360], Loss: 0.0001, Train time: 209.54\n",
      "Epoch [1/1], Step [175700/274360], Loss: 0.0006, Train time: 209.61\n",
      "Epoch [1/1], Step [175800/274360], Loss: 0.0000, Train time: 209.69\n",
      "Epoch [1/1], Step [175900/274360], Loss: 0.0027, Train time: 209.78\n",
      "Epoch [1/1], Step [176000/274360], Loss: 0.0000, Train time: 209.98\n",
      "Epoch [1/1], Step [176100/274360], Loss: 0.0693, Train time: 210.04\n",
      "Epoch [1/1], Step [176200/274360], Loss: 1.7221, Train time: 210.12\n",
      "Epoch [1/1], Step [176300/274360], Loss: 0.0000, Train time: 210.17\n",
      "Epoch [1/1], Step [176400/274360], Loss: 0.0000, Train time: 210.39\n",
      "Epoch [1/1], Step [176500/274360], Loss: 0.0001, Train time: 210.46\n",
      "Epoch [1/1], Step [176600/274360], Loss: 0.0000, Train time: 210.55\n",
      "Epoch [1/1], Step [176700/274360], Loss: 0.0000, Train time: 210.60\n",
      "Epoch [1/1], Step [176800/274360], Loss: 0.0006, Train time: 210.69\n",
      "Epoch [1/1], Step [176900/274360], Loss: 0.0000, Train time: 210.80\n",
      "Epoch [1/1], Step [177000/274360], Loss: 0.0002, Train time: 210.90\n",
      "Epoch [1/1], Step [177100/274360], Loss: 0.0008, Train time: 211.12\n",
      "Epoch [1/1], Step [177200/274360], Loss: 0.0013, Train time: 211.21\n",
      "Epoch [1/1], Step [177300/274360], Loss: 0.0000, Train time: 211.33\n",
      "Epoch [1/1], Step [177400/274360], Loss: 0.0000, Train time: 211.44\n",
      "Epoch [1/1], Step [177500/274360], Loss: 0.0001, Train time: 211.60\n",
      "Epoch [1/1], Step [177600/274360], Loss: 0.0003, Train time: 211.66\n",
      "Epoch [1/1], Step [177700/274360], Loss: 0.0000, Train time: 211.70\n",
      "Epoch [1/1], Step [177800/274360], Loss: 0.0000, Train time: 211.89\n",
      "Epoch [1/1], Step [177900/274360], Loss: 0.0029, Train time: 211.94\n",
      "Epoch [1/1], Step [178000/274360], Loss: 0.0001, Train time: 212.02\n",
      "Epoch [1/1], Step [178100/274360], Loss: 0.0000, Train time: 212.23\n",
      "Epoch [1/1], Step [178200/274360], Loss: 0.0001, Train time: 212.37\n",
      "Epoch [1/1], Step [178300/274360], Loss: 0.0000, Train time: 212.44\n",
      "Epoch [1/1], Step [178400/274360], Loss: 0.0001, Train time: 212.52\n",
      "Epoch [1/1], Step [178500/274360], Loss: 0.0000, Train time: 212.67\n",
      "Epoch [1/1], Step [178600/274360], Loss: 0.0001, Train time: 212.74\n",
      "Epoch [1/1], Step [178700/274360], Loss: 0.0000, Train time: 212.87\n",
      "Epoch [1/1], Step [178800/274360], Loss: 0.0256, Train time: 213.00\n",
      "Epoch [1/1], Step [178900/274360], Loss: 0.0002, Train time: 213.10\n",
      "Epoch [1/1], Step [179000/274360], Loss: 0.0143, Train time: 213.17\n",
      "Epoch [1/1], Step [179100/274360], Loss: 0.0000, Train time: 213.27\n",
      "Epoch [1/1], Step [179200/274360], Loss: 0.0001, Train time: 213.41\n",
      "Epoch [1/1], Step [179300/274360], Loss: 0.0006, Train time: 213.61\n",
      "Epoch [1/1], Step [179400/274360], Loss: 0.0003, Train time: 213.65\n",
      "Epoch [1/1], Step [179500/274360], Loss: 0.0001, Train time: 213.71\n",
      "Epoch [1/1], Step [179600/274360], Loss: 0.0000, Train time: 213.79\n",
      "Epoch [1/1], Step [179700/274360], Loss: 0.0000, Train time: 213.88\n",
      "Epoch [1/1], Step [179800/274360], Loss: 0.0000, Train time: 214.18\n",
      "Epoch [1/1], Step [179900/274360], Loss: 0.0000, Train time: 214.25\n",
      "Epoch [1/1], Step [180000/274360], Loss: 0.0001, Train time: 214.40\n",
      "Epoch [1/1], Step [180100/274360], Loss: 0.0003, Train time: 214.51\n",
      "Epoch [1/1], Step [180200/274360], Loss: 0.0060, Train time: 214.58\n",
      "Epoch [1/1], Step [180300/274360], Loss: 0.0001, Train time: 214.83\n",
      "Epoch [1/1], Step [180400/274360], Loss: 0.0000, Train time: 214.90\n",
      "Epoch [1/1], Step [180500/274360], Loss: 0.0000, Train time: 215.01\n",
      "Epoch [1/1], Step [180600/274360], Loss: 0.0000, Train time: 215.11\n",
      "Epoch [1/1], Step [180700/274360], Loss: 0.0234, Train time: 215.19\n",
      "Epoch [1/1], Step [180800/274360], Loss: 0.0002, Train time: 215.33\n",
      "Epoch [1/1], Step [180900/274360], Loss: 0.0002, Train time: 215.41\n",
      "Epoch [1/1], Step [181000/274360], Loss: 0.0000, Train time: 215.57\n",
      "Epoch [1/1], Step [181100/274360], Loss: 0.0000, Train time: 215.71\n",
      "Epoch [1/1], Step [181200/274360], Loss: 0.0026, Train time: 215.88\n",
      "Epoch [1/1], Step [181300/274360], Loss: 0.0064, Train time: 215.94\n",
      "Epoch [1/1], Step [181400/274360], Loss: 0.0000, Train time: 215.99\n",
      "Epoch [1/1], Step [181500/274360], Loss: 0.0006, Train time: 216.28\n",
      "Epoch [1/1], Step [181600/274360], Loss: 0.0000, Train time: 216.34\n",
      "Epoch [1/1], Step [181700/274360], Loss: 0.0000, Train time: 216.44\n",
      "Epoch [1/1], Step [181800/274360], Loss: 0.0000, Train time: 216.52\n",
      "Epoch [1/1], Step [181900/274360], Loss: 0.0011, Train time: 216.66\n",
      "Epoch [1/1], Step [182000/274360], Loss: 0.0006, Train time: 216.85\n",
      "Epoch [1/1], Step [182100/274360], Loss: 0.0018, Train time: 216.90\n",
      "Epoch [1/1], Step [182200/274360], Loss: 0.0003, Train time: 216.97\n",
      "Epoch [1/1], Step [182300/274360], Loss: 0.0000, Train time: 217.24\n",
      "Epoch [1/1], Step [182400/274360], Loss: 0.0000, Train time: 217.32\n",
      "Epoch [1/1], Step [182500/274360], Loss: 0.0000, Train time: 217.38\n",
      "Epoch [1/1], Step [182600/274360], Loss: 0.0000, Train time: 217.43\n",
      "Epoch [1/1], Step [182700/274360], Loss: 0.0000, Train time: 217.68\n",
      "Epoch [1/1], Step [182800/274360], Loss: 0.0001, Train time: 217.75\n",
      "Epoch [1/1], Step [182900/274360], Loss: 0.0002, Train time: 217.80\n",
      "Epoch [1/1], Step [183000/274360], Loss: 0.0009, Train time: 217.95\n",
      "Epoch [1/1], Step [183100/274360], Loss: 0.0016, Train time: 218.14\n",
      "Epoch [1/1], Step [183200/274360], Loss: 0.0000, Train time: 218.34\n",
      "Epoch [1/1], Step [183300/274360], Loss: 0.0004, Train time: 218.38\n",
      "Epoch [1/1], Step [183400/274360], Loss: 0.0008, Train time: 218.45\n",
      "Epoch [1/1], Step [183500/274360], Loss: 0.0033, Train time: 218.52\n",
      "Epoch [1/1], Step [183600/274360], Loss: 0.0057, Train time: 218.81\n",
      "Epoch [1/1], Step [183700/274360], Loss: 0.0003, Train time: 218.87\n",
      "Epoch [1/1], Step [183800/274360], Loss: 0.0000, Train time: 218.94\n",
      "Epoch [1/1], Step [183900/274360], Loss: 0.0000, Train time: 219.04\n",
      "Epoch [1/1], Step [184000/274360], Loss: 0.0003, Train time: 219.11\n",
      "Epoch [1/1], Step [184100/274360], Loss: 0.0018, Train time: 219.36\n",
      "Epoch [1/1], Step [184200/274360], Loss: 0.0031, Train time: 219.40\n",
      "Epoch [1/1], Step [184300/274360], Loss: 0.0003, Train time: 219.44\n",
      "Epoch [1/1], Step [184400/274360], Loss: 0.0002, Train time: 219.53\n",
      "Epoch [1/1], Step [184500/274360], Loss: 0.0037, Train time: 219.79\n",
      "Epoch [1/1], Step [184600/274360], Loss: 0.0000, Train time: 219.98\n",
      "Epoch [1/1], Step [184700/274360], Loss: 0.0004, Train time: 220.18\n",
      "Epoch [1/1], Step [184800/274360], Loss: 0.0000, Train time: 220.22\n",
      "Epoch [1/1], Step [184900/274360], Loss: 0.0002, Train time: 220.30\n",
      "Epoch [1/1], Step [185000/274360], Loss: 0.0000, Train time: 220.44\n",
      "Epoch [1/1], Step [185100/274360], Loss: 0.0000, Train time: 220.57\n",
      "Epoch [1/1], Step [185200/274360], Loss: 0.0000, Train time: 220.64\n",
      "Epoch [1/1], Step [185300/274360], Loss: 0.0001, Train time: 220.68\n",
      "Epoch [1/1], Step [185400/274360], Loss: 0.0000, Train time: 220.91\n",
      "Epoch [1/1], Step [185500/274360], Loss: 0.0000, Train time: 221.22\n",
      "Epoch [1/1], Step [185600/274360], Loss: 0.0003, Train time: 221.27\n",
      "Epoch [1/1], Step [185700/274360], Loss: 0.0001, Train time: 221.31\n",
      "Epoch [1/1], Step [185800/274360], Loss: 0.0000, Train time: 221.53\n",
      "Epoch [1/1], Step [185900/274360], Loss: 0.0000, Train time: 221.62\n",
      "Epoch [1/1], Step [186000/274360], Loss: 0.0018, Train time: 221.67\n",
      "Epoch [1/1], Step [186100/274360], Loss: 0.0009, Train time: 221.78\n",
      "Epoch [1/1], Step [186200/274360], Loss: 0.0011, Train time: 221.94\n",
      "Epoch [1/1], Step [186300/274360], Loss: 0.0000, Train time: 222.04\n",
      "Epoch [1/1], Step [186400/274360], Loss: 0.0001, Train time: 222.15\n",
      "Epoch [1/1], Step [186500/274360], Loss: 0.0138, Train time: 222.34\n",
      "Epoch [1/1], Step [186600/274360], Loss: 0.0009, Train time: 222.43\n",
      "Epoch [1/1], Step [186700/274360], Loss: 0.0010, Train time: 222.56\n",
      "Epoch [1/1], Step [186800/274360], Loss: 0.0448, Train time: 222.64\n",
      "Epoch [1/1], Step [186900/274360], Loss: 0.0001, Train time: 222.89\n",
      "Epoch [1/1], Step [187000/274360], Loss: 0.0000, Train time: 222.94\n",
      "Epoch [1/1], Step [187100/274360], Loss: 0.0000, Train time: 223.02\n",
      "Epoch [1/1], Step [187200/274360], Loss: 0.0000, Train time: 223.15\n",
      "Epoch [1/1], Step [187300/274360], Loss: 0.0000, Train time: 223.32\n",
      "Epoch [1/1], Step [187400/274360], Loss: 0.0000, Train time: 223.42\n",
      "Epoch [1/1], Step [187500/274360], Loss: 0.0001, Train time: 223.46\n",
      "Epoch [1/1], Step [187600/274360], Loss: 0.0000, Train time: 223.53\n",
      "Epoch [1/1], Step [187700/274360], Loss: 0.0000, Train time: 223.80\n",
      "Epoch [1/1], Step [187800/274360], Loss: 0.0001, Train time: 223.99\n",
      "Epoch [1/1], Step [187900/274360], Loss: 0.0000, Train time: 224.04\n",
      "Epoch [1/1], Step [188000/274360], Loss: 0.0000, Train time: 224.13\n",
      "Epoch [1/1], Step [188100/274360], Loss: 0.0000, Train time: 224.18\n",
      "Epoch [1/1], Step [188200/274360], Loss: 0.0000, Train time: 224.51\n",
      "Epoch [1/1], Step [188300/274360], Loss: 0.0000, Train time: 224.58\n",
      "Epoch [1/1], Step [188400/274360], Loss: 0.0000, Train time: 224.66\n",
      "Epoch [1/1], Step [188500/274360], Loss: 0.0001, Train time: 224.80\n",
      "Epoch [1/1], Step [188600/274360], Loss: 0.0039, Train time: 224.89\n",
      "Epoch [1/1], Step [188700/274360], Loss: 0.0000, Train time: 225.12\n",
      "Epoch [1/1], Step [188800/274360], Loss: 0.0000, Train time: 225.20\n",
      "Epoch [1/1], Step [188900/274360], Loss: 0.0004, Train time: 225.26\n",
      "Epoch [1/1], Step [189000/274360], Loss: 0.0019, Train time: 225.32\n",
      "Epoch [1/1], Step [189100/274360], Loss: 0.0000, Train time: 225.61\n",
      "Epoch [1/1], Step [189200/274360], Loss: 0.0020, Train time: 225.71\n",
      "Epoch [1/1], Step [189300/274360], Loss: 0.0036, Train time: 225.76\n",
      "Epoch [1/1], Step [189400/274360], Loss: 0.0000, Train time: 225.85\n",
      "Epoch [1/1], Step [189500/274360], Loss: 0.0000, Train time: 226.07\n",
      "Epoch [1/1], Step [189600/274360], Loss: 0.0037, Train time: 226.13\n",
      "Epoch [1/1], Step [189700/274360], Loss: 0.0001, Train time: 226.18\n",
      "Epoch [1/1], Step [189800/274360], Loss: 0.0001, Train time: 226.30\n",
      "Epoch [1/1], Step [189900/274360], Loss: 0.0027, Train time: 226.50\n",
      "Epoch [1/1], Step [190000/274360], Loss: 0.0000, Train time: 226.64\n",
      "Epoch [1/1], Step [190100/274360], Loss: 0.0000, Train time: 226.70\n",
      "Epoch [1/1], Step [190200/274360], Loss: 0.0000, Train time: 226.79\n",
      "Epoch [1/1], Step [190300/274360], Loss: 0.0000, Train time: 227.08\n",
      "Epoch [1/1], Step [190400/274360], Loss: 0.0165, Train time: 227.14\n",
      "Epoch [1/1], Step [190500/274360], Loss: 0.0126, Train time: 227.24\n",
      "Epoch [1/1], Step [190600/274360], Loss: 0.0000, Train time: 227.34\n",
      "Epoch [1/1], Step [190700/274360], Loss: 0.0000, Train time: 227.54\n",
      "Epoch [1/1], Step [190800/274360], Loss: 0.0001, Train time: 227.60\n",
      "Epoch [1/1], Step [190900/274360], Loss: 0.0000, Train time: 227.70\n",
      "Epoch [1/1], Step [191000/274360], Loss: 0.0000, Train time: 227.79\n",
      "Epoch [1/1], Step [191100/274360], Loss: 0.0000, Train time: 228.00\n",
      "Epoch [1/1], Step [191200/274360], Loss: 0.2841, Train time: 228.07\n",
      "Epoch [1/1], Step [191300/274360], Loss: 0.0000, Train time: 228.19\n",
      "Epoch [1/1], Step [191400/274360], Loss: 0.0010, Train time: 228.24\n",
      "Epoch [1/1], Step [191500/274360], Loss: 0.0000, Train time: 228.32\n",
      "Epoch [1/1], Step [191600/274360], Loss: 0.0000, Train time: 228.55\n",
      "Epoch [1/1], Step [191700/274360], Loss: 0.0000, Train time: 228.63\n",
      "Epoch [1/1], Step [191800/274360], Loss: 0.0000, Train time: 228.70\n",
      "Epoch [1/1], Step [191900/274360], Loss: 0.0000, Train time: 228.79\n",
      "Epoch [1/1], Step [192000/274360], Loss: 0.0000, Train time: 228.90\n",
      "Epoch [1/1], Step [192100/274360], Loss: 0.0000, Train time: 229.11\n",
      "Epoch [1/1], Step [192200/274360], Loss: 0.0000, Train time: 229.19\n",
      "Epoch [1/1], Step [192300/274360], Loss: 0.0014, Train time: 229.25\n",
      "Epoch [1/1], Step [192400/274360], Loss: 0.0000, Train time: 229.35\n",
      "Epoch [1/1], Step [192500/274360], Loss: 0.0695, Train time: 229.52\n",
      "Epoch [1/1], Step [192600/274360], Loss: 0.0006, Train time: 229.79\n",
      "Epoch [1/1], Step [192700/274360], Loss: 0.0004, Train time: 229.89\n",
      "Epoch [1/1], Step [192800/274360], Loss: 0.0000, Train time: 229.96\n",
      "Epoch [1/1], Step [192900/274360], Loss: 0.0005, Train time: 230.00\n",
      "Epoch [1/1], Step [193000/274360], Loss: 0.0001, Train time: 230.13\n",
      "Epoch [1/1], Step [193100/274360], Loss: 0.0145, Train time: 230.25\n",
      "Epoch [1/1], Step [193200/274360], Loss: 0.0000, Train time: 230.48\n",
      "Epoch [1/1], Step [193300/274360], Loss: 0.0000, Train time: 230.54\n",
      "Epoch [1/1], Step [193400/274360], Loss: 0.0001, Train time: 230.58\n",
      "Epoch [1/1], Step [193500/274360], Loss: 0.0000, Train time: 230.64\n",
      "Epoch [1/1], Step [193600/274360], Loss: 0.0000, Train time: 230.80\n",
      "Epoch [1/1], Step [193700/274360], Loss: 0.0067, Train time: 230.95\n",
      "Epoch [1/1], Step [193800/274360], Loss: 0.0000, Train time: 231.29\n",
      "Epoch [1/1], Step [193900/274360], Loss: 0.0005, Train time: 231.36\n",
      "Epoch [1/1], Step [194000/274360], Loss: 0.0000, Train time: 231.43\n",
      "Epoch [1/1], Step [194100/274360], Loss: 0.0000, Train time: 231.52\n",
      "Epoch [1/1], Step [194200/274360], Loss: 0.0000, Train time: 231.59\n",
      "Epoch [1/1], Step [194300/274360], Loss: 0.0000, Train time: 231.70\n",
      "Epoch [1/1], Step [194400/274360], Loss: 0.0000, Train time: 231.84\n",
      "Epoch [1/1], Step [194500/274360], Loss: 0.0005, Train time: 231.93\n",
      "Epoch [1/1], Step [194600/274360], Loss: 0.0000, Train time: 232.22\n",
      "Epoch [1/1], Step [194700/274360], Loss: 0.0001, Train time: 232.27\n",
      "Epoch [1/1], Step [194800/274360], Loss: 0.0000, Train time: 232.31\n",
      "Epoch [1/1], Step [194900/274360], Loss: 0.0000, Train time: 232.35\n",
      "Epoch [1/1], Step [195000/274360], Loss: 0.0000, Train time: 232.46\n",
      "Epoch [1/1], Step [195100/274360], Loss: 0.0002, Train time: 232.75\n",
      "Epoch [1/1], Step [195200/274360], Loss: 0.0005, Train time: 232.82\n",
      "Epoch [1/1], Step [195300/274360], Loss: 0.0000, Train time: 233.02\n",
      "Epoch [1/1], Step [195400/274360], Loss: 0.0000, Train time: 233.28\n",
      "Epoch [1/1], Step [195500/274360], Loss: 0.0000, Train time: 233.39\n",
      "Epoch [1/1], Step [195600/274360], Loss: 0.0001, Train time: 233.45\n",
      "Epoch [1/1], Step [195700/274360], Loss: 0.0000, Train time: 233.58\n",
      "Epoch [1/1], Step [195800/274360], Loss: 0.0000, Train time: 233.64\n",
      "Epoch [1/1], Step [195900/274360], Loss: 0.0001, Train time: 233.90\n",
      "Epoch [1/1], Step [196000/274360], Loss: 0.0000, Train time: 233.96\n",
      "Epoch [1/1], Step [196100/274360], Loss: 0.0000, Train time: 234.06\n",
      "Epoch [1/1], Step [196200/274360], Loss: 0.0000, Train time: 234.13\n",
      "Epoch [1/1], Step [196300/274360], Loss: 0.0000, Train time: 234.29\n",
      "Epoch [1/1], Step [196400/274360], Loss: 0.0000, Train time: 234.59\n",
      "Epoch [1/1], Step [196500/274360], Loss: 0.5315, Train time: 234.64\n",
      "Epoch [1/1], Step [196600/274360], Loss: 0.0019, Train time: 234.68\n",
      "Epoch [1/1], Step [196700/274360], Loss: 0.0000, Train time: 234.71\n",
      "Epoch [1/1], Step [196800/274360], Loss: 0.0000, Train time: 234.92\n",
      "Epoch [1/1], Step [196900/274360], Loss: 0.0002, Train time: 235.15\n",
      "Epoch [1/1], Step [197000/274360], Loss: 0.0020, Train time: 235.24\n",
      "Epoch [1/1], Step [197100/274360], Loss: 0.0000, Train time: 235.35\n",
      "Epoch [1/1], Step [197200/274360], Loss: 0.0000, Train time: 235.42\n",
      "Epoch [1/1], Step [197300/274360], Loss: 0.0001, Train time: 235.50\n",
      "Epoch [1/1], Step [197400/274360], Loss: 0.0000, Train time: 235.72\n",
      "Epoch [1/1], Step [197500/274360], Loss: 0.0000, Train time: 235.87\n",
      "Epoch [1/1], Step [197600/274360], Loss: 0.0006, Train time: 235.92\n",
      "Epoch [1/1], Step [197700/274360], Loss: 0.0000, Train time: 235.98\n",
      "Epoch [1/1], Step [197800/274360], Loss: 0.0000, Train time: 236.16\n",
      "Epoch [1/1], Step [197900/274360], Loss: 0.0000, Train time: 236.33\n",
      "Epoch [1/1], Step [198000/274360], Loss: 0.0000, Train time: 236.38\n",
      "Epoch [1/1], Step [198100/274360], Loss: 0.0000, Train time: 236.46\n",
      "Epoch [1/1], Step [198200/274360], Loss: 0.0000, Train time: 236.71\n",
      "Epoch [1/1], Step [198300/274360], Loss: 0.0000, Train time: 236.79\n",
      "Epoch [1/1], Step [198400/274360], Loss: 0.0000, Train time: 236.89\n",
      "Epoch [1/1], Step [198500/274360], Loss: 0.0000, Train time: 236.97\n",
      "Epoch [1/1], Step [198600/274360], Loss: 0.0031, Train time: 237.30\n",
      "Epoch [1/1], Step [198700/274360], Loss: 0.0008, Train time: 237.49\n",
      "Epoch [1/1], Step [198800/274360], Loss: 0.0000, Train time: 237.53\n",
      "Epoch [1/1], Step [198900/274360], Loss: 0.0000, Train time: 237.64\n",
      "Epoch [1/1], Step [199000/274360], Loss: 0.0001, Train time: 237.76\n",
      "Epoch [1/1], Step [199100/274360], Loss: 0.0025, Train time: 237.86\n",
      "Epoch [1/1], Step [199200/274360], Loss: 0.1482, Train time: 237.95\n",
      "Epoch [1/1], Step [199300/274360], Loss: 0.0000, Train time: 238.06\n",
      "Epoch [1/1], Step [199400/274360], Loss: 0.0007, Train time: 238.15\n",
      "Epoch [1/1], Step [199500/274360], Loss: 0.0001, Train time: 238.37\n",
      "Epoch [1/1], Step [199600/274360], Loss: 0.0001, Train time: 238.44\n",
      "Epoch [1/1], Step [199700/274360], Loss: 0.0000, Train time: 238.54\n",
      "Epoch [1/1], Step [199800/274360], Loss: 0.0000, Train time: 238.68\n",
      "Epoch [1/1], Step [199900/274360], Loss: 0.0000, Train time: 238.93\n",
      "Epoch [1/1], Step [200000/274360], Loss: 0.0000, Train time: 239.00\n",
      "Epoch [1/1], Step [200100/274360], Loss: 0.0000, Train time: 239.10\n",
      "Epoch [1/1], Step [200200/274360], Loss: 0.0007, Train time: 239.30\n",
      "Epoch [1/1], Step [200300/274360], Loss: 0.0000, Train time: 239.36\n",
      "Epoch [1/1], Step [200400/274360], Loss: 0.0001, Train time: 239.48\n",
      "Epoch [1/1], Step [200500/274360], Loss: 0.0002, Train time: 239.57\n",
      "Epoch [1/1], Step [200600/274360], Loss: 0.0003, Train time: 239.79\n",
      "Epoch [1/1], Step [200700/274360], Loss: 1.6854, Train time: 239.85\n",
      "Epoch [1/1], Step [200800/274360], Loss: 0.0000, Train time: 239.95\n",
      "Epoch [1/1], Step [200900/274360], Loss: 0.0109, Train time: 240.09\n",
      "Epoch [1/1], Step [201000/274360], Loss: 0.0000, Train time: 240.27\n",
      "Epoch [1/1], Step [201100/274360], Loss: 0.0010, Train time: 240.31\n",
      "Epoch [1/1], Step [201200/274360], Loss: 0.0000, Train time: 240.51\n",
      "Epoch [1/1], Step [201300/274360], Loss: 0.0002, Train time: 240.59\n",
      "Epoch [1/1], Step [201400/274360], Loss: 0.0000, Train time: 240.65\n",
      "Epoch [1/1], Step [201500/274360], Loss: 0.0000, Train time: 240.77\n",
      "Epoch [1/1], Step [201600/274360], Loss: 0.0004, Train time: 240.81\n",
      "Epoch [1/1], Step [201700/274360], Loss: 0.0005, Train time: 241.18\n",
      "Epoch [1/1], Step [201800/274360], Loss: 0.0003, Train time: 241.26\n",
      "Epoch [1/1], Step [201900/274360], Loss: 0.0012, Train time: 241.34\n",
      "Epoch [1/1], Step [202000/274360], Loss: 0.0025, Train time: 241.41\n",
      "Epoch [1/1], Step [202100/274360], Loss: 0.0080, Train time: 241.53\n",
      "Epoch [1/1], Step [202200/274360], Loss: 0.0001, Train time: 241.71\n",
      "Epoch [1/1], Step [202300/274360], Loss: 0.0000, Train time: 242.09\n",
      "Epoch [1/1], Step [202400/274360], Loss: 0.0000, Train time: 242.15\n",
      "Epoch [1/1], Step [202500/274360], Loss: 0.0000, Train time: 242.26\n",
      "Epoch [1/1], Step [202600/274360], Loss: 0.0001, Train time: 242.41\n",
      "Epoch [1/1], Step [202700/274360], Loss: 0.0000, Train time: 242.46\n",
      "Epoch [1/1], Step [202800/274360], Loss: 0.0000, Train time: 242.55\n",
      "Epoch [1/1], Step [202900/274360], Loss: 0.0000, Train time: 242.67\n",
      "Epoch [1/1], Step [203000/274360], Loss: 0.0115, Train time: 242.79\n",
      "Epoch [1/1], Step [203100/274360], Loss: 0.0000, Train time: 243.00\n",
      "Epoch [1/1], Step [203200/274360], Loss: 0.0002, Train time: 243.11\n",
      "Epoch [1/1], Step [203300/274360], Loss: 0.0062, Train time: 243.15\n",
      "Epoch [1/1], Step [203400/274360], Loss: 0.0001, Train time: 243.26\n",
      "Epoch [1/1], Step [203500/274360], Loss: 0.0047, Train time: 243.33\n",
      "Epoch [1/1], Step [203600/274360], Loss: 0.0000, Train time: 243.41\n",
      "Epoch [1/1], Step [203700/274360], Loss: 0.0000, Train time: 243.73\n",
      "Epoch [1/1], Step [203800/274360], Loss: 0.0007, Train time: 243.81\n",
      "Epoch [1/1], Step [203900/274360], Loss: 0.0000, Train time: 243.91\n",
      "Epoch [1/1], Step [204000/274360], Loss: 0.0000, Train time: 243.95\n",
      "Epoch [1/1], Step [204100/274360], Loss: 0.0001, Train time: 244.06\n",
      "Epoch [1/1], Step [204200/274360], Loss: 0.0026, Train time: 244.17\n",
      "Epoch [1/1], Step [204300/274360], Loss: 0.0000, Train time: 244.29\n",
      "Epoch [1/1], Step [204400/274360], Loss: 0.0000, Train time: 244.38\n",
      "Epoch [1/1], Step [204500/274360], Loss: 0.0001, Train time: 244.65\n",
      "Epoch [1/1], Step [204600/274360], Loss: 0.0024, Train time: 244.69\n",
      "Epoch [1/1], Step [204700/274360], Loss: 0.0008, Train time: 244.78\n",
      "Epoch [1/1], Step [204800/274360], Loss: 0.0000, Train time: 244.85\n",
      "Epoch [1/1], Step [204900/274360], Loss: 0.0000, Train time: 245.10\n",
      "Epoch [1/1], Step [205000/274360], Loss: 0.0004, Train time: 245.30\n",
      "Epoch [1/1], Step [205100/274360], Loss: 0.0000, Train time: 245.36\n",
      "Epoch [1/1], Step [205200/274360], Loss: 0.0003, Train time: 245.46\n",
      "Epoch [1/1], Step [205300/274360], Loss: 0.0002, Train time: 245.55\n",
      "Epoch [1/1], Step [205400/274360], Loss: 0.0005, Train time: 245.63\n",
      "Epoch [1/1], Step [205500/274360], Loss: 0.0057, Train time: 245.87\n",
      "Epoch [1/1], Step [205600/274360], Loss: 0.0000, Train time: 245.92\n",
      "Epoch [1/1], Step [205700/274360], Loss: 0.0001, Train time: 245.98\n",
      "Epoch [1/1], Step [205800/274360], Loss: 0.0001, Train time: 246.06\n",
      "Epoch [1/1], Step [205900/274360], Loss: 0.0000, Train time: 246.24\n",
      "Epoch [1/1], Step [206000/274360], Loss: 0.0000, Train time: 246.31\n",
      "Epoch [1/1], Step [206100/274360], Loss: 0.0000, Train time: 246.56\n",
      "Epoch [1/1], Step [206200/274360], Loss: 0.0001, Train time: 246.61\n",
      "Epoch [1/1], Step [206300/274360], Loss: 0.0000, Train time: 246.72\n",
      "Epoch [1/1], Step [206400/274360], Loss: 0.0001, Train time: 246.92\n",
      "Epoch [1/1], Step [206500/274360], Loss: 0.0015, Train time: 246.98\n",
      "Epoch [1/1], Step [206600/274360], Loss: 0.0001, Train time: 247.08\n",
      "Epoch [1/1], Step [206700/274360], Loss: 0.0005, Train time: 247.19\n",
      "Epoch [1/1], Step [206800/274360], Loss: 0.0002, Train time: 247.44\n",
      "Epoch [1/1], Step [206900/274360], Loss: 0.0000, Train time: 247.51\n",
      "Epoch [1/1], Step [207000/274360], Loss: 0.0003, Train time: 247.58\n",
      "Epoch [1/1], Step [207100/274360], Loss: 0.0000, Train time: 247.65\n",
      "Epoch [1/1], Step [207200/274360], Loss: 0.0000, Train time: 247.99\n",
      "Epoch [1/1], Step [207300/274360], Loss: 0.0000, Train time: 248.14\n",
      "Epoch [1/1], Step [207400/274360], Loss: 0.0009, Train time: 248.23\n",
      "Epoch [1/1], Step [207500/274360], Loss: 0.0000, Train time: 248.38\n",
      "Epoch [1/1], Step [207600/274360], Loss: 0.0001, Train time: 248.46\n",
      "Epoch [1/1], Step [207700/274360], Loss: 0.0000, Train time: 248.51\n",
      "Epoch [1/1], Step [207800/274360], Loss: 0.0000, Train time: 248.57\n",
      "Epoch [1/1], Step [207900/274360], Loss: 0.0002, Train time: 248.69\n",
      "Epoch [1/1], Step [208000/274360], Loss: 0.0422, Train time: 248.93\n",
      "Epoch [1/1], Step [208100/274360], Loss: 0.0004, Train time: 248.97\n",
      "Epoch [1/1], Step [208200/274360], Loss: 0.0002, Train time: 249.09\n",
      "Epoch [1/1], Step [208300/274360], Loss: 0.0001, Train time: 249.26\n",
      "Epoch [1/1], Step [208400/274360], Loss: 0.0002, Train time: 249.35\n",
      "Epoch [1/1], Step [208500/274360], Loss: 0.0000, Train time: 249.44\n",
      "Epoch [1/1], Step [208600/274360], Loss: 0.0000, Train time: 249.79\n",
      "Epoch [1/1], Step [208700/274360], Loss: 0.0166, Train time: 249.86\n",
      "Epoch [1/1], Step [208800/274360], Loss: 0.0034, Train time: 249.96\n",
      "Epoch [1/1], Step [208900/274360], Loss: 0.0013, Train time: 250.05\n",
      "Epoch [1/1], Step [209000/274360], Loss: 0.0003, Train time: 250.17\n",
      "Epoch [1/1], Step [209100/274360], Loss: 0.0001, Train time: 250.47\n",
      "Epoch [1/1], Step [209200/274360], Loss: 0.0005, Train time: 250.53\n",
      "Epoch [1/1], Step [209300/274360], Loss: 0.0000, Train time: 250.58\n",
      "Epoch [1/1], Step [209400/274360], Loss: 0.0000, Train time: 250.79\n",
      "Epoch [1/1], Step [209500/274360], Loss: 0.0000, Train time: 250.90\n",
      "Epoch [1/1], Step [209600/274360], Loss: 0.0000, Train time: 250.97\n",
      "Epoch [1/1], Step [209700/274360], Loss: 0.0000, Train time: 251.02\n",
      "Epoch [1/1], Step [209800/274360], Loss: 0.0001, Train time: 251.12\n",
      "Epoch [1/1], Step [209900/274360], Loss: 0.0000, Train time: 251.23\n",
      "Epoch [1/1], Step [210000/274360], Loss: 0.0000, Train time: 251.50\n",
      "Epoch [1/1], Step [210100/274360], Loss: 0.0000, Train time: 251.56\n",
      "Epoch [1/1], Step [210200/274360], Loss: 0.0000, Train time: 251.66\n",
      "Epoch [1/1], Step [210300/274360], Loss: 0.0064, Train time: 251.77\n",
      "Epoch [1/1], Step [210400/274360], Loss: 0.0000, Train time: 251.84\n",
      "Epoch [1/1], Step [210500/274360], Loss: 0.0000, Train time: 251.97\n",
      "Epoch [1/1], Step [210600/274360], Loss: 0.0000, Train time: 252.16\n",
      "Epoch [1/1], Step [210700/274360], Loss: 0.0000, Train time: 252.23\n",
      "Epoch [1/1], Step [210800/274360], Loss: 0.0000, Train time: 252.32\n",
      "Epoch [1/1], Step [210900/274360], Loss: 0.0000, Train time: 252.55\n",
      "Epoch [1/1], Step [211000/274360], Loss: 0.0000, Train time: 252.65\n",
      "Epoch [1/1], Step [211100/274360], Loss: 0.0000, Train time: 252.74\n",
      "Epoch [1/1], Step [211200/274360], Loss: 0.0000, Train time: 252.96\n",
      "Epoch [1/1], Step [211300/274360], Loss: 0.0003, Train time: 253.01\n",
      "Epoch [1/1], Step [211400/274360], Loss: 0.0000, Train time: 253.12\n",
      "Epoch [1/1], Step [211500/274360], Loss: 0.0000, Train time: 253.27\n",
      "Epoch [1/1], Step [211600/274360], Loss: 0.0002, Train time: 253.35\n",
      "Epoch [1/1], Step [211700/274360], Loss: 0.0002, Train time: 253.62\n",
      "Epoch [1/1], Step [211800/274360], Loss: 0.0091, Train time: 253.68\n",
      "Epoch [1/1], Step [211900/274360], Loss: 0.0000, Train time: 253.76\n",
      "Epoch [1/1], Step [212000/274360], Loss: 0.0002, Train time: 253.81\n",
      "Epoch [1/1], Step [212100/274360], Loss: 0.0001, Train time: 253.93\n",
      "Epoch [1/1], Step [212200/274360], Loss: 0.0000, Train time: 254.09\n",
      "Epoch [1/1], Step [212300/274360], Loss: 0.0000, Train time: 254.13\n",
      "Epoch [1/1], Step [212400/274360], Loss: 0.0000, Train time: 254.22\n",
      "Epoch [1/1], Step [212500/274360], Loss: 0.0017, Train time: 254.43\n",
      "Epoch [1/1], Step [212600/274360], Loss: 0.0018, Train time: 254.47\n",
      "Epoch [1/1], Step [212700/274360], Loss: 0.0422, Train time: 254.59\n",
      "Epoch [1/1], Step [212800/274360], Loss: 0.0278, Train time: 254.73\n",
      "Epoch [1/1], Step [212900/274360], Loss: 0.0005, Train time: 254.89\n",
      "Epoch [1/1], Step [213000/274360], Loss: 0.0000, Train time: 255.00\n",
      "Epoch [1/1], Step [213100/274360], Loss: 0.0000, Train time: 255.07\n",
      "Epoch [1/1], Step [213200/274360], Loss: 0.0000, Train time: 255.20\n",
      "Epoch [1/1], Step [213300/274360], Loss: 0.0000, Train time: 255.36\n",
      "Epoch [1/1], Step [213400/274360], Loss: 0.0008, Train time: 255.54\n",
      "Epoch [1/1], Step [213500/274360], Loss: 0.0441, Train time: 255.64\n",
      "Epoch [1/1], Step [213600/274360], Loss: 0.0000, Train time: 255.69\n",
      "Epoch [1/1], Step [213700/274360], Loss: 0.0002, Train time: 255.84\n",
      "Epoch [1/1], Step [213800/274360], Loss: 0.0001, Train time: 256.08\n",
      "Epoch [1/1], Step [213900/274360], Loss: 0.0001, Train time: 256.17\n",
      "Epoch [1/1], Step [214000/274360], Loss: 0.0019, Train time: 256.21\n",
      "Epoch [1/1], Step [214100/274360], Loss: 0.0001, Train time: 256.26\n",
      "Epoch [1/1], Step [214200/274360], Loss: 0.0000, Train time: 256.53\n",
      "Epoch [1/1], Step [214300/274360], Loss: 0.0000, Train time: 256.60\n",
      "Epoch [1/1], Step [214400/274360], Loss: 0.0052, Train time: 256.68\n",
      "Epoch [1/1], Step [214500/274360], Loss: 0.0001, Train time: 256.73\n",
      "Epoch [1/1], Step [214600/274360], Loss: 0.0462, Train time: 256.87\n",
      "Epoch [1/1], Step [214700/274360], Loss: 0.0066, Train time: 257.14\n",
      "Epoch [1/1], Step [214800/274360], Loss: 0.0000, Train time: 257.26\n",
      "Epoch [1/1], Step [214900/274360], Loss: 0.0001, Train time: 257.35\n",
      "Epoch [1/1], Step [215000/274360], Loss: 0.0000, Train time: 257.48\n",
      "Epoch [1/1], Step [215100/274360], Loss: 0.0001, Train time: 257.58\n",
      "Epoch [1/1], Step [215200/274360], Loss: 0.0000, Train time: 257.72\n",
      "Epoch [1/1], Step [215300/274360], Loss: 0.0067, Train time: 257.83\n",
      "Epoch [1/1], Step [215400/274360], Loss: 0.0001, Train time: 257.92\n",
      "Epoch [1/1], Step [215500/274360], Loss: 0.0031, Train time: 258.02\n",
      "Epoch [1/1], Step [215600/274360], Loss: 0.0102, Train time: 258.25\n",
      "Epoch [1/1], Step [215700/274360], Loss: 0.0000, Train time: 258.29\n",
      "Epoch [1/1], Step [215800/274360], Loss: 0.0001, Train time: 258.42\n",
      "Epoch [1/1], Step [215900/274360], Loss: 0.0004, Train time: 258.58\n",
      "Epoch [1/1], Step [216000/274360], Loss: 0.0000, Train time: 258.63\n",
      "Epoch [1/1], Step [216100/274360], Loss: 0.0002, Train time: 258.70\n",
      "Epoch [1/1], Step [216200/274360], Loss: 0.0001, Train time: 258.87\n",
      "Epoch [1/1], Step [216300/274360], Loss: 0.0000, Train time: 259.04\n",
      "Epoch [1/1], Step [216400/274360], Loss: 0.0000, Train time: 259.19\n",
      "Epoch [1/1], Step [216500/274360], Loss: 0.0128, Train time: 259.30\n",
      "Epoch [1/1], Step [216600/274360], Loss: 0.0000, Train time: 259.50\n",
      "Epoch [1/1], Step [216700/274360], Loss: 0.0000, Train time: 259.65\n",
      "Epoch [1/1], Step [216800/274360], Loss: 0.0000, Train time: 259.72\n",
      "Epoch [1/1], Step [216900/274360], Loss: 0.0003, Train time: 259.77\n",
      "Epoch [1/1], Step [217000/274360], Loss: 0.0000, Train time: 259.87\n",
      "Epoch [1/1], Step [217100/274360], Loss: 0.0000, Train time: 260.04\n",
      "Epoch [1/1], Step [217200/274360], Loss: 0.0002, Train time: 260.27\n",
      "Epoch [1/1], Step [217300/274360], Loss: 0.0000, Train time: 260.32\n",
      "Epoch [1/1], Step [217400/274360], Loss: 0.0002, Train time: 260.54\n",
      "Epoch [1/1], Step [217500/274360], Loss: 0.1173, Train time: 260.66\n",
      "Epoch [1/1], Step [217600/274360], Loss: 0.0000, Train time: 260.74\n",
      "Epoch [1/1], Step [217700/274360], Loss: 0.0003, Train time: 260.81\n",
      "Epoch [1/1], Step [217800/274360], Loss: 0.0018, Train time: 261.15\n",
      "Epoch [1/1], Step [217900/274360], Loss: 0.0000, Train time: 261.42\n",
      "Epoch [1/1], Step [218000/274360], Loss: 0.0001, Train time: 261.49\n",
      "Epoch [1/1], Step [218100/274360], Loss: 0.0000, Train time: 261.57\n",
      "Epoch [1/1], Step [218200/274360], Loss: 0.0000, Train time: 261.73\n",
      "Epoch [1/1], Step [218300/274360], Loss: 0.0000, Train time: 261.78\n",
      "Epoch [1/1], Step [218400/274360], Loss: 0.0000, Train time: 261.84\n",
      "Epoch [1/1], Step [218500/274360], Loss: 0.0000, Train time: 262.00\n",
      "Epoch [1/1], Step [218600/274360], Loss: 0.0000, Train time: 262.05\n",
      "Epoch [1/1], Step [218700/274360], Loss: 0.0025, Train time: 262.13\n",
      "Epoch [1/1], Step [218800/274360], Loss: 0.0000, Train time: 262.22\n",
      "Epoch [1/1], Step [218900/274360], Loss: 0.0006, Train time: 262.50\n",
      "Epoch [1/1], Step [219000/274360], Loss: 0.0004, Train time: 262.56\n",
      "Epoch [1/1], Step [219100/274360], Loss: 0.0000, Train time: 262.67\n",
      "Epoch [1/1], Step [219200/274360], Loss: 0.0002, Train time: 262.89\n",
      "Epoch [1/1], Step [219300/274360], Loss: 0.0000, Train time: 263.04\n",
      "Epoch [1/1], Step [219400/274360], Loss: 0.9153, Train time: 263.08\n",
      "Epoch [1/1], Step [219500/274360], Loss: 0.0000, Train time: 263.16\n",
      "Epoch [1/1], Step [219600/274360], Loss: 0.0000, Train time: 263.24\n",
      "Epoch [1/1], Step [219700/274360], Loss: 0.0000, Train time: 263.50\n",
      "Epoch [1/1], Step [219800/274360], Loss: 0.0000, Train time: 263.64\n",
      "Epoch [1/1], Step [219900/274360], Loss: 0.0001, Train time: 263.68\n",
      "Epoch [1/1], Step [220000/274360], Loss: 0.0000, Train time: 263.76\n",
      "Epoch [1/1], Step [220100/274360], Loss: 0.0000, Train time: 263.82\n",
      "Epoch [1/1], Step [220200/274360], Loss: 0.0000, Train time: 263.99\n",
      "Epoch [1/1], Step [220300/274360], Loss: 0.0001, Train time: 264.05\n",
      "Epoch [1/1], Step [220400/274360], Loss: 0.0001, Train time: 264.10\n",
      "Epoch [1/1], Step [220500/274360], Loss: 0.0000, Train time: 264.28\n",
      "Epoch [1/1], Step [220600/274360], Loss: 0.0001, Train time: 264.37\n",
      "Epoch [1/1], Step [220700/274360], Loss: 0.0000, Train time: 264.45\n",
      "Epoch [1/1], Step [220800/274360], Loss: 0.0001, Train time: 264.55\n",
      "Epoch [1/1], Step [220900/274360], Loss: 0.0001, Train time: 264.76\n",
      "Epoch [1/1], Step [221000/274360], Loss: 0.0000, Train time: 264.82\n",
      "Epoch [1/1], Step [221100/274360], Loss: 0.0000, Train time: 264.92\n",
      "Epoch [1/1], Step [221200/274360], Loss: 0.0000, Train time: 265.12\n",
      "Epoch [1/1], Step [221300/274360], Loss: 0.0002, Train time: 265.34\n",
      "Epoch [1/1], Step [221400/274360], Loss: 0.0000, Train time: 265.38\n",
      "Epoch [1/1], Step [221500/274360], Loss: 0.0002, Train time: 265.42\n",
      "Epoch [1/1], Step [221600/274360], Loss: 0.0021, Train time: 265.59\n",
      "Epoch [1/1], Step [221700/274360], Loss: 0.0061, Train time: 265.75\n",
      "Epoch [1/1], Step [221800/274360], Loss: 0.0000, Train time: 265.83\n",
      "Epoch [1/1], Step [221900/274360], Loss: 0.0039, Train time: 265.87\n",
      "Epoch [1/1], Step [222000/274360], Loss: 0.0000, Train time: 266.16\n",
      "Epoch [1/1], Step [222100/274360], Loss: 0.0001, Train time: 266.22\n",
      "Epoch [1/1], Step [222200/274360], Loss: 0.0000, Train time: 266.30\n",
      "Epoch [1/1], Step [222300/274360], Loss: 0.0011, Train time: 266.41\n",
      "Epoch [1/1], Step [222400/274360], Loss: 0.0000, Train time: 266.50\n",
      "Epoch [1/1], Step [222500/274360], Loss: 0.0000, Train time: 266.82\n",
      "Epoch [1/1], Step [222600/274360], Loss: 0.0000, Train time: 266.88\n",
      "Epoch [1/1], Step [222700/274360], Loss: 0.0000, Train time: 266.92\n",
      "Epoch [1/1], Step [222800/274360], Loss: 0.0001, Train time: 267.06\n",
      "Epoch [1/1], Step [222900/274360], Loss: 0.0001, Train time: 267.23\n",
      "Epoch [1/1], Step [223000/274360], Loss: 0.0000, Train time: 267.47\n",
      "Epoch [1/1], Step [223100/274360], Loss: 0.0001, Train time: 267.58\n",
      "Epoch [1/1], Step [223200/274360], Loss: 0.0009, Train time: 267.64\n",
      "Epoch [1/1], Step [223300/274360], Loss: 0.0001, Train time: 267.90\n",
      "Epoch [1/1], Step [223400/274360], Loss: 0.0000, Train time: 268.04\n",
      "Epoch [1/1], Step [223500/274360], Loss: 0.0003, Train time: 268.13\n",
      "Epoch [1/1], Step [223600/274360], Loss: 0.0000, Train time: 268.20\n",
      "Epoch [1/1], Step [223700/274360], Loss: 0.0000, Train time: 268.32\n",
      "Epoch [1/1], Step [223800/274360], Loss: 0.0000, Train time: 268.50\n",
      "Epoch [1/1], Step [223900/274360], Loss: 0.0000, Train time: 268.57\n",
      "Epoch [1/1], Step [224000/274360], Loss: 0.0000, Train time: 268.62\n",
      "Epoch [1/1], Step [224100/274360], Loss: 0.0000, Train time: 268.69\n",
      "Epoch [1/1], Step [224200/274360], Loss: 0.0000, Train time: 268.91\n",
      "Epoch [1/1], Step [224300/274360], Loss: 0.0001, Train time: 268.95\n",
      "Epoch [1/1], Step [224400/274360], Loss: 0.0000, Train time: 269.09\n",
      "Epoch [1/1], Step [224500/274360], Loss: 0.0001, Train time: 269.17\n",
      "Epoch [1/1], Step [224600/274360], Loss: 0.0000, Train time: 269.37\n",
      "Epoch [1/1], Step [224700/274360], Loss: 0.0005, Train time: 269.47\n",
      "Epoch [1/1], Step [224800/274360], Loss: 0.0001, Train time: 269.59\n",
      "Epoch [1/1], Step [224900/274360], Loss: 0.0000, Train time: 269.64\n",
      "Epoch [1/1], Step [225000/274360], Loss: 0.0004, Train time: 269.71\n",
      "Epoch [1/1], Step [225100/274360], Loss: 0.0000, Train time: 269.85\n",
      "Epoch [1/1], Step [225200/274360], Loss: 0.0001, Train time: 269.95\n",
      "Epoch [1/1], Step [225300/274360], Loss: 0.0167, Train time: 270.09\n",
      "Epoch [1/1], Step [225400/274360], Loss: 0.0087, Train time: 270.19\n",
      "Epoch [1/1], Step [225500/274360], Loss: 0.0033, Train time: 270.49\n",
      "Epoch [1/1], Step [225600/274360], Loss: 0.0000, Train time: 270.54\n",
      "Epoch [1/1], Step [225700/274360], Loss: 0.0000, Train time: 270.64\n",
      "Epoch [1/1], Step [225800/274360], Loss: 0.0001, Train time: 270.75\n",
      "Epoch [1/1], Step [225900/274360], Loss: 0.0004, Train time: 270.91\n",
      "Epoch [1/1], Step [226000/274360], Loss: 0.0001, Train time: 270.97\n",
      "Epoch [1/1], Step [226100/274360], Loss: 0.0000, Train time: 271.15\n",
      "Epoch [1/1], Step [226200/274360], Loss: 0.0000, Train time: 271.24\n",
      "Epoch [1/1], Step [226300/274360], Loss: 0.0000, Train time: 271.42\n",
      "Epoch [1/1], Step [226400/274360], Loss: 0.0000, Train time: 271.57\n",
      "Epoch [1/1], Step [226500/274360], Loss: 0.0001, Train time: 271.64\n",
      "Epoch [1/1], Step [226600/274360], Loss: 0.0033, Train time: 271.71\n",
      "Epoch [1/1], Step [226700/274360], Loss: 0.0000, Train time: 271.85\n",
      "Epoch [1/1], Step [226800/274360], Loss: 0.0017, Train time: 272.03\n",
      "Epoch [1/1], Step [226900/274360], Loss: 0.0000, Train time: 272.15\n",
      "Epoch [1/1], Step [227000/274360], Loss: 0.0019, Train time: 272.22\n",
      "Epoch [1/1], Step [227100/274360], Loss: 0.0000, Train time: 272.31\n",
      "Epoch [1/1], Step [227200/274360], Loss: 0.0001, Train time: 272.49\n",
      "Epoch [1/1], Step [227300/274360], Loss: 0.0000, Train time: 272.59\n",
      "Epoch [1/1], Step [227400/274360], Loss: 0.0000, Train time: 272.65\n",
      "Epoch [1/1], Step [227500/274360], Loss: 0.0000, Train time: 272.82\n",
      "Epoch [1/1], Step [227600/274360], Loss: 0.0032, Train time: 272.89\n",
      "Epoch [1/1], Step [227700/274360], Loss: 0.0001, Train time: 273.14\n",
      "Epoch [1/1], Step [227800/274360], Loss: 0.0000, Train time: 273.33\n",
      "Epoch [1/1], Step [227900/274360], Loss: 0.0001, Train time: 273.41\n",
      "Epoch [1/1], Step [228000/274360], Loss: 0.0006, Train time: 273.47\n",
      "Epoch [1/1], Step [228100/274360], Loss: 0.0000, Train time: 273.52\n",
      "Epoch [1/1], Step [228200/274360], Loss: 0.0000, Train time: 273.63\n",
      "Epoch [1/1], Step [228300/274360], Loss: 0.0000, Train time: 273.79\n",
      "Epoch [1/1], Step [228400/274360], Loss: 0.0003, Train time: 274.06\n",
      "Epoch [1/1], Step [228500/274360], Loss: 0.0000, Train time: 274.12\n",
      "Epoch [1/1], Step [228600/274360], Loss: 0.0001, Train time: 274.21\n",
      "Epoch [1/1], Step [228700/274360], Loss: 0.0000, Train time: 274.47\n",
      "Epoch [1/1], Step [228800/274360], Loss: 0.0023, Train time: 274.55\n",
      "Epoch [1/1], Step [228900/274360], Loss: 0.0010, Train time: 274.82\n",
      "Epoch [1/1], Step [229000/274360], Loss: 0.0004, Train time: 274.91\n",
      "Epoch [1/1], Step [229100/274360], Loss: 0.0000, Train time: 274.96\n",
      "Epoch [1/1], Step [229200/274360], Loss: 0.0003, Train time: 275.04\n",
      "Epoch [1/1], Step [229300/274360], Loss: 0.0001, Train time: 275.21\n",
      "Epoch [1/1], Step [229400/274360], Loss: 0.0000, Train time: 275.27\n",
      "Epoch [1/1], Step [229500/274360], Loss: 0.0000, Train time: 275.36\n",
      "Epoch [1/1], Step [229600/274360], Loss: 0.0000, Train time: 275.46\n",
      "Epoch [1/1], Step [229700/274360], Loss: 0.0001, Train time: 275.80\n",
      "Epoch [1/1], Step [229800/274360], Loss: 0.0003, Train time: 275.85\n",
      "Epoch [1/1], Step [229900/274360], Loss: 0.0000, Train time: 275.91\n",
      "Epoch [1/1], Step [230000/274360], Loss: 0.0000, Train time: 275.99\n",
      "Epoch [1/1], Step [230100/274360], Loss: 0.0000, Train time: 276.16\n",
      "Epoch [1/1], Step [230200/274360], Loss: 0.0004, Train time: 276.29\n",
      "Epoch [1/1], Step [230300/274360], Loss: 0.0202, Train time: 276.38\n",
      "Epoch [1/1], Step [230400/274360], Loss: 0.0868, Train time: 276.48\n",
      "Epoch [1/1], Step [230500/274360], Loss: 0.0007, Train time: 276.57\n",
      "Epoch [1/1], Step [230600/274360], Loss: 0.0000, Train time: 276.65\n",
      "Epoch [1/1], Step [230700/274360], Loss: 0.0000, Train time: 276.73\n",
      "Epoch [1/1], Step [230800/274360], Loss: 0.0000, Train time: 277.03\n",
      "Epoch [1/1], Step [230900/274360], Loss: 0.0001, Train time: 277.12\n",
      "Epoch [1/1], Step [231000/274360], Loss: 0.0000, Train time: 277.19\n",
      "Epoch [1/1], Step [231100/274360], Loss: 0.0000, Train time: 277.29\n",
      "Epoch [1/1], Step [231200/274360], Loss: 0.0000, Train time: 277.57\n",
      "Epoch [1/1], Step [231300/274360], Loss: 0.0000, Train time: 277.68\n",
      "Epoch [1/1], Step [231400/274360], Loss: 0.0000, Train time: 277.73\n",
      "Epoch [1/1], Step [231500/274360], Loss: 0.0000, Train time: 277.89\n",
      "Epoch [1/1], Step [231600/274360], Loss: 0.0000, Train time: 278.10\n",
      "Epoch [1/1], Step [231700/274360], Loss: 0.0000, Train time: 278.19\n",
      "Epoch [1/1], Step [231800/274360], Loss: 0.0001, Train time: 278.36\n",
      "Epoch [1/1], Step [231900/274360], Loss: 0.0000, Train time: 278.44\n",
      "Epoch [1/1], Step [232000/274360], Loss: 0.0000, Train time: 278.70\n",
      "Epoch [1/1], Step [232100/274360], Loss: 0.0000, Train time: 278.74\n",
      "Epoch [1/1], Step [232200/274360], Loss: 0.0000, Train time: 278.80\n",
      "Epoch [1/1], Step [232300/274360], Loss: 0.0000, Train time: 279.01\n",
      "Epoch [1/1], Step [232400/274360], Loss: 0.0000, Train time: 279.15\n",
      "Epoch [1/1], Step [232500/274360], Loss: 0.0000, Train time: 279.27\n",
      "Epoch [1/1], Step [232600/274360], Loss: 0.0002, Train time: 279.38\n",
      "Epoch [1/1], Step [232700/274360], Loss: 0.0000, Train time: 279.52\n",
      "Epoch [1/1], Step [232800/274360], Loss: 0.0000, Train time: 279.57\n",
      "Epoch [1/1], Step [232900/274360], Loss: 0.0005, Train time: 279.74\n",
      "Epoch [1/1], Step [233000/274360], Loss: 0.0000, Train time: 279.92\n",
      "Epoch [1/1], Step [233100/274360], Loss: 0.0003, Train time: 280.01\n",
      "Epoch [1/1], Step [233200/274360], Loss: 0.0000, Train time: 280.06\n",
      "Epoch [1/1], Step [233300/274360], Loss: 0.0000, Train time: 280.15\n",
      "Epoch [1/1], Step [233400/274360], Loss: 0.0011, Train time: 280.36\n",
      "Epoch [1/1], Step [233500/274360], Loss: 0.0000, Train time: 280.41\n",
      "Epoch [1/1], Step [233600/274360], Loss: 0.0000, Train time: 280.50\n",
      "Epoch [1/1], Step [233700/274360], Loss: 0.0000, Train time: 280.57\n",
      "Epoch [1/1], Step [233800/274360], Loss: 0.0001, Train time: 280.81\n",
      "Epoch [1/1], Step [233900/274360], Loss: 0.0000, Train time: 280.89\n",
      "Epoch [1/1], Step [234000/274360], Loss: 0.0001, Train time: 280.96\n",
      "Epoch [1/1], Step [234100/274360], Loss: 0.0000, Train time: 281.05\n",
      "Epoch [1/1], Step [234200/274360], Loss: 0.0000, Train time: 281.17\n",
      "Epoch [1/1], Step [234300/274360], Loss: 0.0003, Train time: 281.24\n",
      "Epoch [1/1], Step [234400/274360], Loss: 0.0197, Train time: 281.58\n",
      "Epoch [1/1], Step [234500/274360], Loss: 0.0000, Train time: 281.77\n",
      "Epoch [1/1], Step [234600/274360], Loss: 0.0000, Train time: 281.82\n",
      "Epoch [1/1], Step [234700/274360], Loss: 0.0000, Train time: 281.95\n",
      "Epoch [1/1], Step [234800/274360], Loss: 0.0018, Train time: 282.07\n",
      "Epoch [1/1], Step [234900/274360], Loss: 0.0000, Train time: 282.19\n",
      "Epoch [1/1], Step [235000/274360], Loss: 0.0000, Train time: 282.33\n",
      "Epoch [1/1], Step [235100/274360], Loss: 0.0000, Train time: 282.40\n",
      "Epoch [1/1], Step [235200/274360], Loss: 0.0000, Train time: 282.44\n",
      "Epoch [1/1], Step [235300/274360], Loss: 0.0008, Train time: 282.53\n",
      "Epoch [1/1], Step [235400/274360], Loss: 0.0000, Train time: 282.78\n",
      "Epoch [1/1], Step [235500/274360], Loss: 0.0000, Train time: 282.83\n",
      "Epoch [1/1], Step [235600/274360], Loss: 0.0000, Train time: 282.89\n",
      "Epoch [1/1], Step [235700/274360], Loss: 0.0019, Train time: 283.02\n",
      "Epoch [1/1], Step [235800/274360], Loss: 0.0000, Train time: 283.26\n",
      "Epoch [1/1], Step [235900/274360], Loss: 0.0006, Train time: 283.32\n",
      "Epoch [1/1], Step [236000/274360], Loss: 0.0025, Train time: 283.44\n",
      "Epoch [1/1], Step [236100/274360], Loss: 0.0000, Train time: 283.50\n",
      "Epoch [1/1], Step [236200/274360], Loss: 0.0000, Train time: 283.65\n",
      "Epoch [1/1], Step [236300/274360], Loss: 0.0002, Train time: 283.75\n",
      "Epoch [1/1], Step [236400/274360], Loss: 0.0001, Train time: 283.82\n",
      "Epoch [1/1], Step [236500/274360], Loss: 0.0000, Train time: 284.07\n",
      "Epoch [1/1], Step [236600/274360], Loss: 0.0003, Train time: 284.12\n",
      "Epoch [1/1], Step [236700/274360], Loss: 0.0061, Train time: 284.18\n",
      "Epoch [1/1], Step [236800/274360], Loss: 0.0044, Train time: 284.24\n",
      "Epoch [1/1], Step [236900/274360], Loss: 0.2987, Train time: 284.45\n",
      "Epoch [1/1], Step [237000/274360], Loss: 0.0020, Train time: 284.70\n",
      "Epoch [1/1], Step [237100/274360], Loss: 0.0000, Train time: 284.75\n",
      "Epoch [1/1], Step [237200/274360], Loss: 0.0000, Train time: 284.81\n",
      "Epoch [1/1], Step [237300/274360], Loss: 0.0021, Train time: 284.89\n",
      "Epoch [1/1], Step [237400/274360], Loss: 0.0000, Train time: 285.08\n",
      "Epoch [1/1], Step [237500/274360], Loss: 0.0000, Train time: 285.17\n",
      "Epoch [1/1], Step [237600/274360], Loss: 0.0001, Train time: 285.29\n",
      "Epoch [1/1], Step [237700/274360], Loss: 0.0004, Train time: 285.51\n",
      "Epoch [1/1], Step [237800/274360], Loss: 0.0001, Train time: 285.62\n",
      "Epoch [1/1], Step [237900/274360], Loss: 0.0004, Train time: 285.71\n",
      "Epoch [1/1], Step [238000/274360], Loss: 0.0002, Train time: 285.77\n",
      "Epoch [1/1], Step [238100/274360], Loss: 0.0000, Train time: 285.90\n",
      "Epoch [1/1], Step [238200/274360], Loss: 0.0000, Train time: 286.01\n",
      "Epoch [1/1], Step [238300/274360], Loss: 0.0003, Train time: 286.23\n",
      "Epoch [1/1], Step [238400/274360], Loss: 0.0000, Train time: 286.31\n",
      "Epoch [1/1], Step [238500/274360], Loss: 0.0000, Train time: 286.36\n",
      "Epoch [1/1], Step [238600/274360], Loss: 0.0005, Train time: 286.43\n",
      "Epoch [1/1], Step [238700/274360], Loss: 0.0001, Train time: 286.69\n",
      "Epoch [1/1], Step [238800/274360], Loss: 0.0000, Train time: 286.76\n",
      "Epoch [1/1], Step [238900/274360], Loss: 0.0117, Train time: 286.84\n",
      "Epoch [1/1], Step [239000/274360], Loss: 0.0000, Train time: 286.94\n",
      "Epoch [1/1], Step [239100/274360], Loss: 0.0000, Train time: 287.09\n",
      "Epoch [1/1], Step [239200/274360], Loss: 0.0000, Train time: 287.18\n",
      "Epoch [1/1], Step [239300/274360], Loss: 0.0000, Train time: 287.23\n",
      "Epoch [1/1], Step [239400/274360], Loss: 0.0003, Train time: 287.34\n",
      "Epoch [1/1], Step [239500/274360], Loss: 0.0001, Train time: 287.69\n",
      "Epoch [1/1], Step [239600/274360], Loss: 0.0076, Train time: 287.80\n",
      "Epoch [1/1], Step [239700/274360], Loss: 0.1401, Train time: 287.86\n",
      "Epoch [1/1], Step [239800/274360], Loss: 0.0031, Train time: 287.90\n",
      "Epoch [1/1], Step [239900/274360], Loss: 0.0000, Train time: 288.02\n",
      "Epoch [1/1], Step [240000/274360], Loss: 0.0006, Train time: 288.10\n",
      "Epoch [1/1], Step [240100/274360], Loss: 0.0022, Train time: 288.30\n",
      "Epoch [1/1], Step [240200/274360], Loss: 0.0000, Train time: 288.34\n",
      "Epoch [1/1], Step [240300/274360], Loss: 0.0000, Train time: 288.45\n",
      "Epoch [1/1], Step [240400/274360], Loss: 0.0000, Train time: 288.68\n",
      "Epoch [1/1], Step [240500/274360], Loss: 0.0001, Train time: 288.73\n",
      "Epoch [1/1], Step [240600/274360], Loss: 0.0004, Train time: 288.79\n",
      "Epoch [1/1], Step [240700/274360], Loss: 0.0000, Train time: 288.91\n",
      "Epoch [1/1], Step [240800/274360], Loss: 0.0006, Train time: 289.21\n",
      "Epoch [1/1], Step [240900/274360], Loss: 0.0008, Train time: 289.26\n",
      "Epoch [1/1], Step [241000/274360], Loss: 0.0000, Train time: 289.31\n",
      "Epoch [1/1], Step [241100/274360], Loss: 0.0000, Train time: 289.40\n",
      "Epoch [1/1], Step [241200/274360], Loss: 0.0003, Train time: 289.64\n",
      "Epoch [1/1], Step [241300/274360], Loss: 0.0000, Train time: 289.72\n",
      "Epoch [1/1], Step [241400/274360], Loss: 0.0014, Train time: 289.79\n",
      "Epoch [1/1], Step [241500/274360], Loss: 0.0000, Train time: 289.89\n",
      "Epoch [1/1], Step [241600/274360], Loss: 0.0000, Train time: 290.05\n",
      "Epoch [1/1], Step [241700/274360], Loss: 0.0008, Train time: 290.16\n",
      "Epoch [1/1], Step [241800/274360], Loss: 0.0003, Train time: 290.47\n",
      "Epoch [1/1], Step [241900/274360], Loss: 0.0000, Train time: 290.70\n",
      "Epoch [1/1], Step [242000/274360], Loss: 0.0000, Train time: 290.73\n",
      "Epoch [1/1], Step [242100/274360], Loss: 0.0010, Train time: 290.79\n",
      "Epoch [1/1], Step [242200/274360], Loss: 0.0001, Train time: 290.92\n",
      "Epoch [1/1], Step [242300/274360], Loss: 0.0002, Train time: 291.13\n",
      "Epoch [1/1], Step [242400/274360], Loss: 0.0003, Train time: 291.22\n",
      "Epoch [1/1], Step [242500/274360], Loss: 0.0001, Train time: 291.30\n",
      "Epoch [1/1], Step [242600/274360], Loss: 0.0007, Train time: 291.36\n",
      "Epoch [1/1], Step [242700/274360], Loss: 0.0000, Train time: 291.44\n",
      "Epoch [1/1], Step [242800/274360], Loss: 0.0000, Train time: 291.69\n",
      "Epoch [1/1], Step [242900/274360], Loss: 0.0502, Train time: 291.73\n",
      "Epoch [1/1], Step [243000/274360], Loss: 0.0000, Train time: 291.86\n",
      "Epoch [1/1], Step [243100/274360], Loss: 0.0002, Train time: 291.97\n",
      "Epoch [1/1], Step [243200/274360], Loss: 0.0000, Train time: 292.20\n",
      "Epoch [1/1], Step [243300/274360], Loss: 0.0002, Train time: 292.25\n",
      "Epoch [1/1], Step [243400/274360], Loss: 0.0000, Train time: 292.36\n",
      "Epoch [1/1], Step [243500/274360], Loss: 0.0000, Train time: 292.45\n",
      "Epoch [1/1], Step [243600/274360], Loss: 0.0004, Train time: 292.64\n",
      "Epoch [1/1], Step [243700/274360], Loss: 0.0000, Train time: 292.72\n",
      "Epoch [1/1], Step [243800/274360], Loss: 0.0000, Train time: 292.85\n",
      "Epoch [1/1], Step [243900/274360], Loss: 0.0000, Train time: 292.95\n",
      "Epoch [1/1], Step [244000/274360], Loss: 0.0000, Train time: 293.16\n",
      "Epoch [1/1], Step [244100/274360], Loss: 0.0000, Train time: 293.26\n",
      "Epoch [1/1], Step [244200/274360], Loss: 0.0005, Train time: 293.32\n",
      "Epoch [1/1], Step [244300/274360], Loss: 0.0000, Train time: 293.37\n",
      "Epoch [1/1], Step [244400/274360], Loss: 0.0021, Train time: 293.54\n",
      "Epoch [1/1], Step [244500/274360], Loss: 0.0010, Train time: 293.72\n",
      "Epoch [1/1], Step [244600/274360], Loss: 0.0000, Train time: 293.77\n",
      "Epoch [1/1], Step [244700/274360], Loss: 0.0002, Train time: 293.80\n",
      "Epoch [1/1], Step [244800/274360], Loss: 0.0000, Train time: 293.88\n",
      "Epoch [1/1], Step [244900/274360], Loss: 0.0001, Train time: 294.07\n",
      "Epoch [1/1], Step [245000/274360], Loss: 0.0001, Train time: 294.14\n",
      "Epoch [1/1], Step [245100/274360], Loss: 0.0000, Train time: 294.22\n",
      "Epoch [1/1], Step [245200/274360], Loss: 0.0000, Train time: 294.47\n",
      "Epoch [1/1], Step [245300/274360], Loss: 0.0000, Train time: 294.64\n",
      "Epoch [1/1], Step [245400/274360], Loss: 0.0047, Train time: 294.85\n",
      "Epoch [1/1], Step [245500/274360], Loss: 0.0000, Train time: 294.92\n",
      "Epoch [1/1], Step [245600/274360], Loss: 0.0012, Train time: 294.96\n",
      "Epoch [1/1], Step [245700/274360], Loss: 0.0000, Train time: 295.05\n",
      "Epoch [1/1], Step [245800/274360], Loss: 0.0000, Train time: 295.16\n",
      "Epoch [1/1], Step [245900/274360], Loss: 0.0000, Train time: 295.36\n",
      "Epoch [1/1], Step [246000/274360], Loss: 0.0021, Train time: 295.70\n",
      "Epoch [1/1], Step [246100/274360], Loss: 0.0001, Train time: 295.83\n",
      "Epoch [1/1], Step [246200/274360], Loss: 0.0001, Train time: 296.00\n",
      "Epoch [1/1], Step [246300/274360], Loss: 0.0012, Train time: 296.06\n",
      "Epoch [1/1], Step [246400/274360], Loss: 0.0001, Train time: 296.13\n",
      "Epoch [1/1], Step [246500/274360], Loss: 0.0007, Train time: 296.42\n",
      "Epoch [1/1], Step [246600/274360], Loss: 0.0000, Train time: 296.48\n",
      "Epoch [1/1], Step [246700/274360], Loss: 0.0000, Train time: 296.54\n",
      "Epoch [1/1], Step [246800/274360], Loss: 0.0017, Train time: 296.76\n",
      "Epoch [1/1], Step [246900/274360], Loss: 0.0000, Train time: 296.88\n",
      "Epoch [1/1], Step [247000/274360], Loss: 0.0000, Train time: 296.94\n",
      "Epoch [1/1], Step [247100/274360], Loss: 0.0000, Train time: 297.04\n",
      "Epoch [1/1], Step [247200/274360], Loss: 0.0000, Train time: 297.20\n",
      "Epoch [1/1], Step [247300/274360], Loss: 0.0000, Train time: 297.24\n",
      "Epoch [1/1], Step [247400/274360], Loss: 0.0000, Train time: 297.33\n",
      "Epoch [1/1], Step [247500/274360], Loss: 0.0000, Train time: 297.39\n",
      "Epoch [1/1], Step [247600/274360], Loss: 0.0000, Train time: 297.65\n",
      "Epoch [1/1], Step [247700/274360], Loss: 0.0000, Train time: 297.75\n",
      "Epoch [1/1], Step [247800/274360], Loss: 0.0000, Train time: 297.84\n",
      "Epoch [1/1], Step [247900/274360], Loss: 0.0000, Train time: 297.88\n",
      "Epoch [1/1], Step [248000/274360], Loss: 0.0081, Train time: 298.12\n",
      "Epoch [1/1], Step [248100/274360], Loss: 0.0000, Train time: 298.26\n",
      "Epoch [1/1], Step [248200/274360], Loss: 0.0000, Train time: 298.31\n",
      "Epoch [1/1], Step [248300/274360], Loss: 0.0000, Train time: 298.52\n",
      "Epoch [1/1], Step [248400/274360], Loss: 0.0000, Train time: 298.61\n",
      "Epoch [1/1], Step [248500/274360], Loss: 0.0000, Train time: 298.73\n",
      "Epoch [1/1], Step [248600/274360], Loss: 0.0000, Train time: 298.78\n",
      "Epoch [1/1], Step [248700/274360], Loss: 0.0001, Train time: 298.99\n",
      "Epoch [1/1], Step [248800/274360], Loss: 0.0000, Train time: 299.03\n",
      "Epoch [1/1], Step [248900/274360], Loss: 0.0000, Train time: 299.08\n",
      "Epoch [1/1], Step [249000/274360], Loss: 0.0002, Train time: 299.14\n",
      "Epoch [1/1], Step [249100/274360], Loss: 0.0001, Train time: 299.25\n",
      "Epoch [1/1], Step [249200/274360], Loss: 0.0003, Train time: 299.41\n",
      "Epoch [1/1], Step [249300/274360], Loss: 0.0000, Train time: 299.50\n",
      "Epoch [1/1], Step [249400/274360], Loss: 0.0000, Train time: 299.56\n",
      "Epoch [1/1], Step [249500/274360], Loss: 0.0002, Train time: 299.62\n",
      "Epoch [1/1], Step [249600/274360], Loss: 0.0007, Train time: 299.74\n",
      "Epoch [1/1], Step [249700/274360], Loss: 0.0002, Train time: 299.78\n",
      "Epoch [1/1], Step [249800/274360], Loss: 0.0000, Train time: 300.09\n",
      "Epoch [1/1], Step [249900/274360], Loss: 0.0045, Train time: 300.14\n",
      "Epoch [1/1], Step [250000/274360], Loss: 0.0000, Train time: 300.18\n",
      "Epoch [1/1], Step [250100/274360], Loss: 0.0003, Train time: 300.25\n",
      "Epoch [1/1], Step [250200/274360], Loss: 0.0002, Train time: 300.46\n",
      "Epoch [1/1], Step [250300/274360], Loss: 0.0000, Train time: 300.62\n",
      "Epoch [1/1], Step [250400/274360], Loss: 0.0000, Train time: 300.69\n",
      "Epoch [1/1], Step [250500/274360], Loss: 0.0005, Train time: 300.76\n",
      "Epoch [1/1], Step [250600/274360], Loss: 0.0000, Train time: 300.90\n",
      "Epoch [1/1], Step [250700/274360], Loss: 0.0002, Train time: 300.98\n",
      "Epoch [1/1], Step [250800/274360], Loss: 0.0000, Train time: 301.18\n",
      "Epoch [1/1], Step [250900/274360], Loss: 0.0000, Train time: 301.23\n",
      "Epoch [1/1], Step [251000/274360], Loss: 0.0000, Train time: 301.30\n",
      "Epoch [1/1], Step [251100/274360], Loss: 0.0607, Train time: 301.36\n",
      "Epoch [1/1], Step [251200/274360], Loss: 0.0000, Train time: 301.64\n",
      "Epoch [1/1], Step [251300/274360], Loss: 0.0000, Train time: 301.70\n",
      "Epoch [1/1], Step [251400/274360], Loss: 0.0000, Train time: 301.80\n",
      "Epoch [1/1], Step [251500/274360], Loss: 0.0012, Train time: 301.97\n",
      "Epoch [1/1], Step [251600/274360], Loss: 0.0003, Train time: 302.19\n",
      "Epoch [1/1], Step [251700/274360], Loss: 0.0000, Train time: 302.25\n",
      "Epoch [1/1], Step [251800/274360], Loss: 0.0003, Train time: 302.34\n",
      "Epoch [1/1], Step [251900/274360], Loss: 0.0000, Train time: 302.45\n",
      "Epoch [1/1], Step [252000/274360], Loss: 0.0000, Train time: 302.67\n",
      "Epoch [1/1], Step [252100/274360], Loss: 0.0000, Train time: 302.92\n",
      "Epoch [1/1], Step [252200/274360], Loss: 0.0000, Train time: 303.04\n",
      "Epoch [1/1], Step [252300/274360], Loss: 0.0000, Train time: 303.09\n",
      "Epoch [1/1], Step [252400/274360], Loss: 0.0000, Train time: 303.14\n",
      "Epoch [1/1], Step [252500/274360], Loss: 0.0002, Train time: 303.26\n",
      "Epoch [1/1], Step [252600/274360], Loss: 0.0001, Train time: 303.49\n",
      "Epoch [1/1], Step [252700/274360], Loss: 0.0082, Train time: 303.54\n",
      "Epoch [1/1], Step [252800/274360], Loss: 0.0000, Train time: 303.63\n",
      "Epoch [1/1], Step [252900/274360], Loss: 0.0000, Train time: 303.78\n",
      "Epoch [1/1], Step [253000/274360], Loss: 0.0000, Train time: 303.86\n",
      "Epoch [1/1], Step [253100/274360], Loss: 0.0006, Train time: 303.97\n",
      "Epoch [1/1], Step [253200/274360], Loss: 0.0000, Train time: 304.16\n",
      "Epoch [1/1], Step [253300/274360], Loss: 0.0000, Train time: 304.27\n",
      "Epoch [1/1], Step [253400/274360], Loss: 0.0000, Train time: 304.35\n",
      "Epoch [1/1], Step [253500/274360], Loss: 0.0089, Train time: 304.45\n",
      "Epoch [1/1], Step [253600/274360], Loss: 0.0001, Train time: 304.55\n",
      "Epoch [1/1], Step [253700/274360], Loss: 0.0000, Train time: 304.63\n",
      "Epoch [1/1], Step [253800/274360], Loss: 0.0000, Train time: 304.96\n",
      "Epoch [1/1], Step [253900/274360], Loss: 0.0000, Train time: 305.01\n",
      "Epoch [1/1], Step [254000/274360], Loss: 0.0000, Train time: 305.06\n",
      "Epoch [1/1], Step [254100/274360], Loss: 0.0000, Train time: 305.14\n",
      "Epoch [1/1], Step [254200/274360], Loss: 0.0001, Train time: 305.37\n",
      "Epoch [1/1], Step [254300/274360], Loss: 0.0012, Train time: 305.45\n",
      "Epoch [1/1], Step [254400/274360], Loss: 0.0000, Train time: 305.52\n",
      "Epoch [1/1], Step [254500/274360], Loss: 0.0011, Train time: 305.69\n",
      "Epoch [1/1], Step [254600/274360], Loss: 0.0000, Train time: 305.79\n",
      "Epoch [1/1], Step [254700/274360], Loss: 0.0000, Train time: 306.14\n",
      "Epoch [1/1], Step [254800/274360], Loss: 0.0000, Train time: 306.22\n",
      "Epoch [1/1], Step [254900/274360], Loss: 0.0000, Train time: 306.32\n",
      "Epoch [1/1], Step [255000/274360], Loss: 0.0000, Train time: 306.40\n",
      "Epoch [1/1], Step [255100/274360], Loss: 0.0001, Train time: 306.61\n",
      "Epoch [1/1], Step [255200/274360], Loss: 0.0055, Train time: 306.69\n",
      "Epoch [1/1], Step [255300/274360], Loss: 0.0025, Train time: 306.97\n",
      "Epoch [1/1], Step [255400/274360], Loss: 0.0000, Train time: 307.06\n",
      "Epoch [1/1], Step [255500/274360], Loss: 0.0011, Train time: 307.16\n",
      "Epoch [1/1], Step [255600/274360], Loss: 0.0000, Train time: 307.22\n",
      "Epoch [1/1], Step [255700/274360], Loss: 0.0000, Train time: 307.44\n",
      "Epoch [1/1], Step [255800/274360], Loss: 0.0192, Train time: 307.59\n",
      "Epoch [1/1], Step [255900/274360], Loss: 0.0000, Train time: 307.68\n",
      "Epoch [1/1], Step [256000/274360], Loss: 0.0236, Train time: 307.82\n",
      "Epoch [1/1], Step [256100/274360], Loss: 0.0000, Train time: 308.07\n",
      "Epoch [1/1], Step [256200/274360], Loss: 0.0000, Train time: 308.20\n",
      "Epoch [1/1], Step [256300/274360], Loss: 0.0004, Train time: 308.26\n",
      "Epoch [1/1], Step [256400/274360], Loss: 0.0000, Train time: 308.35\n",
      "Epoch [1/1], Step [256500/274360], Loss: 0.0000, Train time: 308.51\n",
      "Epoch [1/1], Step [256600/274360], Loss: 0.0000, Train time: 308.64\n",
      "Epoch [1/1], Step [256700/274360], Loss: 0.0000, Train time: 308.74\n",
      "Epoch [1/1], Step [256800/274360], Loss: 0.0003, Train time: 308.84\n",
      "Epoch [1/1], Step [256900/274360], Loss: 0.0000, Train time: 309.00\n",
      "Epoch [1/1], Step [257000/274360], Loss: 0.0000, Train time: 309.16\n",
      "Epoch [1/1], Step [257100/274360], Loss: 0.0000, Train time: 309.33\n",
      "Epoch [1/1], Step [257200/274360], Loss: 0.0000, Train time: 309.39\n",
      "Epoch [1/1], Step [257300/274360], Loss: 0.0001, Train time: 309.51\n",
      "Epoch [1/1], Step [257400/274360], Loss: 0.0000, Train time: 309.61\n",
      "Epoch [1/1], Step [257500/274360], Loss: 0.0009, Train time: 309.80\n",
      "Epoch [1/1], Step [257600/274360], Loss: 0.0000, Train time: 309.88\n",
      "Epoch [1/1], Step [257700/274360], Loss: 0.0000, Train time: 309.94\n",
      "Epoch [1/1], Step [257800/274360], Loss: 0.0000, Train time: 310.03\n",
      "Epoch [1/1], Step [257900/274360], Loss: 0.0000, Train time: 310.18\n",
      "Epoch [1/1], Step [258000/274360], Loss: 0.0037, Train time: 310.41\n",
      "Epoch [1/1], Step [258100/274360], Loss: 0.0000, Train time: 310.52\n",
      "Epoch [1/1], Step [258200/274360], Loss: 0.0000, Train time: 310.59\n",
      "Epoch [1/1], Step [258300/274360], Loss: 0.0030, Train time: 310.72\n",
      "Epoch [1/1], Step [258400/274360], Loss: 0.0000, Train time: 310.85\n",
      "Epoch [1/1], Step [258500/274360], Loss: 0.0003, Train time: 311.10\n",
      "Epoch [1/1], Step [258600/274360], Loss: 0.0017, Train time: 311.26\n",
      "Epoch [1/1], Step [258700/274360], Loss: 0.0000, Train time: 311.32\n",
      "Epoch [1/1], Step [258800/274360], Loss: 0.0001, Train time: 311.38\n",
      "Epoch [1/1], Step [258900/274360], Loss: 0.0000, Train time: 311.49\n",
      "Epoch [1/1], Step [259000/274360], Loss: 0.0000, Train time: 311.78\n",
      "Epoch [1/1], Step [259100/274360], Loss: 0.0000, Train time: 311.84\n",
      "Epoch [1/1], Step [259200/274360], Loss: 0.0000, Train time: 311.90\n",
      "Epoch [1/1], Step [259300/274360], Loss: 0.0000, Train time: 311.97\n",
      "Epoch [1/1], Step [259400/274360], Loss: 0.0007, Train time: 312.20\n",
      "Epoch [1/1], Step [259500/274360], Loss: 0.0000, Train time: 312.26\n",
      "Epoch [1/1], Step [259600/274360], Loss: 0.0005, Train time: 312.33\n",
      "Epoch [1/1], Step [259700/274360], Loss: 0.0000, Train time: 312.47\n",
      "Epoch [1/1], Step [259800/274360], Loss: 0.0000, Train time: 312.59\n",
      "Epoch [1/1], Step [259900/274360], Loss: 0.0004, Train time: 312.90\n",
      "Epoch [1/1], Step [260000/274360], Loss: 0.0000, Train time: 312.96\n",
      "Epoch [1/1], Step [260100/274360], Loss: 0.0000, Train time: 313.04\n",
      "Epoch [1/1], Step [260200/274360], Loss: 0.0005, Train time: 313.13\n",
      "Epoch [1/1], Step [260300/274360], Loss: 0.0000, Train time: 313.22\n",
      "Epoch [1/1], Step [260400/274360], Loss: 0.0000, Train time: 313.48\n",
      "Epoch [1/1], Step [260500/274360], Loss: 0.0027, Train time: 313.53\n",
      "Epoch [1/1], Step [260600/274360], Loss: 0.0000, Train time: 313.64\n",
      "Epoch [1/1], Step [260700/274360], Loss: 0.0000, Train time: 313.75\n",
      "Epoch [1/1], Step [260800/274360], Loss: 0.0000, Train time: 314.01\n",
      "Epoch [1/1], Step [260900/274360], Loss: 0.0000, Train time: 314.08\n",
      "Epoch [1/1], Step [261000/274360], Loss: 0.0000, Train time: 314.17\n",
      "Epoch [1/1], Step [261100/274360], Loss: 0.0026, Train time: 314.25\n",
      "Epoch [1/1], Step [261200/274360], Loss: 0.0007, Train time: 314.41\n",
      "Epoch [1/1], Step [261300/274360], Loss: 0.0001, Train time: 314.51\n",
      "Epoch [1/1], Step [261400/274360], Loss: 0.0000, Train time: 314.58\n",
      "Epoch [1/1], Step [261500/274360], Loss: 0.0000, Train time: 314.67\n",
      "Epoch [1/1], Step [261600/274360], Loss: 0.0011, Train time: 314.81\n",
      "Epoch [1/1], Step [261700/274360], Loss: 0.0021, Train time: 314.92\n",
      "Epoch [1/1], Step [261800/274360], Loss: 0.0015, Train time: 315.14\n",
      "Epoch [1/1], Step [261900/274360], Loss: 0.0002, Train time: 315.21\n",
      "Epoch [1/1], Step [262000/274360], Loss: 0.0000, Train time: 315.26\n",
      "Epoch [1/1], Step [262100/274360], Loss: 0.0000, Train time: 315.50\n",
      "Epoch [1/1], Step [262200/274360], Loss: 0.0000, Train time: 315.75\n",
      "Epoch [1/1], Step [262300/274360], Loss: 0.0000, Train time: 315.84\n",
      "Epoch [1/1], Step [262400/274360], Loss: 0.0005, Train time: 315.93\n",
      "Epoch [1/1], Step [262500/274360], Loss: 0.0000, Train time: 316.22\n",
      "Epoch [1/1], Step [262600/274360], Loss: 0.0001, Train time: 316.30\n",
      "Epoch [1/1], Step [262700/274360], Loss: 0.2575, Train time: 316.40\n",
      "Epoch [1/1], Step [262800/274360], Loss: 0.0000, Train time: 316.54\n",
      "Epoch [1/1], Step [262900/274360], Loss: 0.0000, Train time: 316.64\n",
      "Epoch [1/1], Step [263000/274360], Loss: 0.0000, Train time: 316.74\n",
      "Epoch [1/1], Step [263100/274360], Loss: 0.0000, Train time: 316.82\n",
      "Epoch [1/1], Step [263200/274360], Loss: 0.0000, Train time: 316.99\n",
      "Epoch [1/1], Step [263300/274360], Loss: 0.0000, Train time: 317.11\n",
      "Epoch [1/1], Step [263400/274360], Loss: 0.0001, Train time: 317.17\n",
      "Epoch [1/1], Step [263500/274360], Loss: 0.0000, Train time: 317.23\n",
      "Epoch [1/1], Step [263600/274360], Loss: 0.0000, Train time: 317.34\n",
      "Epoch [1/1], Step [263700/274360], Loss: 0.0000, Train time: 317.43\n",
      "Epoch [1/1], Step [263800/274360], Loss: 0.0000, Train time: 317.57\n",
      "Epoch [1/1], Step [263900/274360], Loss: 0.0001, Train time: 317.75\n",
      "Epoch [1/1], Step [264000/274360], Loss: 0.0001, Train time: 317.87\n",
      "Epoch [1/1], Step [264100/274360], Loss: 0.0002, Train time: 317.96\n",
      "Epoch [1/1], Step [264200/274360], Loss: 0.0069, Train time: 318.06\n",
      "Epoch [1/1], Step [264300/274360], Loss: 0.0000, Train time: 318.19\n",
      "Epoch [1/1], Step [264400/274360], Loss: 0.0000, Train time: 318.32\n",
      "Epoch [1/1], Step [264500/274360], Loss: 0.0001, Train time: 318.57\n",
      "Epoch [1/1], Step [264600/274360], Loss: 0.0001, Train time: 318.74\n",
      "Epoch [1/1], Step [264700/274360], Loss: 0.0000, Train time: 318.84\n",
      "Epoch [1/1], Step [264800/274360], Loss: 0.0000, Train time: 318.91\n",
      "Epoch [1/1], Step [264900/274360], Loss: 0.0000, Train time: 319.10\n",
      "Epoch [1/1], Step [265000/274360], Loss: 0.0000, Train time: 319.22\n",
      "Epoch [1/1], Step [265100/274360], Loss: 0.0001, Train time: 319.35\n",
      "Epoch [1/1], Step [265200/274360], Loss: 0.0000, Train time: 319.45\n",
      "Epoch [1/1], Step [265300/274360], Loss: 0.0000, Train time: 319.57\n",
      "Epoch [1/1], Step [265400/274360], Loss: 0.0000, Train time: 319.82\n",
      "Epoch [1/1], Step [265500/274360], Loss: 0.0000, Train time: 319.92\n",
      "Epoch [1/1], Step [265600/274360], Loss: 0.0034, Train time: 320.00\n",
      "Epoch [1/1], Step [265700/274360], Loss: 0.0000, Train time: 320.11\n",
      "Epoch [1/1], Step [265800/274360], Loss: 0.0001, Train time: 320.25\n",
      "Epoch [1/1], Step [265900/274360], Loss: 0.0000, Train time: 320.35\n",
      "Epoch [1/1], Step [266000/274360], Loss: 0.0000, Train time: 320.46\n",
      "Epoch [1/1], Step [266100/274360], Loss: 0.0005, Train time: 320.60\n",
      "Epoch [1/1], Step [266200/274360], Loss: 0.0000, Train time: 320.70\n",
      "Epoch [1/1], Step [266300/274360], Loss: 0.0000, Train time: 320.81\n",
      "Epoch [1/1], Step [266400/274360], Loss: 0.0001, Train time: 320.87\n",
      "Epoch [1/1], Step [266500/274360], Loss: 0.0065, Train time: 320.98\n",
      "Epoch [1/1], Step [266600/274360], Loss: 0.0000, Train time: 321.21\n",
      "Epoch [1/1], Step [266700/274360], Loss: 0.0000, Train time: 321.29\n",
      "Epoch [1/1], Step [266800/274360], Loss: 0.0001, Train time: 321.37\n",
      "Epoch [1/1], Step [266900/274360], Loss: 0.0000, Train time: 321.52\n",
      "Epoch [1/1], Step [267000/274360], Loss: 0.0000, Train time: 321.75\n",
      "Epoch [1/1], Step [267100/274360], Loss: 0.0000, Train time: 321.82\n",
      "Epoch [1/1], Step [267200/274360], Loss: 0.0001, Train time: 321.91\n",
      "Epoch [1/1], Step [267300/274360], Loss: 0.0018, Train time: 321.98\n",
      "Epoch [1/1], Step [267400/274360], Loss: 0.0004, Train time: 322.26\n",
      "Epoch [1/1], Step [267500/274360], Loss: 0.0010, Train time: 322.36\n",
      "Epoch [1/1], Step [267600/274360], Loss: 0.0000, Train time: 322.47\n",
      "Epoch [1/1], Step [267700/274360], Loss: 0.0000, Train time: 322.60\n",
      "Epoch [1/1], Step [267800/274360], Loss: 0.0000, Train time: 322.83\n",
      "Epoch [1/1], Step [267900/274360], Loss: 0.0004, Train time: 322.96\n",
      "Epoch [1/1], Step [268000/274360], Loss: 0.0001, Train time: 323.04\n",
      "Epoch [1/1], Step [268100/274360], Loss: 0.0000, Train time: 323.11\n",
      "Epoch [1/1], Step [268200/274360], Loss: 0.0152, Train time: 323.18\n",
      "Epoch [1/1], Step [268300/274360], Loss: 0.0000, Train time: 323.30\n",
      "Epoch [1/1], Step [268400/274360], Loss: 0.0000, Train time: 323.35\n",
      "Epoch [1/1], Step [268500/274360], Loss: 0.0002, Train time: 323.66\n",
      "Epoch [1/1], Step [268600/274360], Loss: 0.0000, Train time: 323.81\n",
      "Epoch [1/1], Step [268700/274360], Loss: 0.0017, Train time: 323.87\n",
      "Epoch [1/1], Step [268800/274360], Loss: 0.0000, Train time: 323.94\n",
      "Epoch [1/1], Step [268900/274360], Loss: 0.0000, Train time: 324.03\n",
      "Epoch [1/1], Step [269000/274360], Loss: 0.0001, Train time: 324.13\n",
      "Epoch [1/1], Step [269100/274360], Loss: 0.0000, Train time: 324.42\n",
      "Epoch [1/1], Step [269200/274360], Loss: 0.0003, Train time: 324.48\n",
      "Epoch [1/1], Step [269300/274360], Loss: 0.0030, Train time: 324.59\n",
      "Epoch [1/1], Step [269400/274360], Loss: 0.0002, Train time: 324.75\n",
      "Epoch [1/1], Step [269500/274360], Loss: 0.0001, Train time: 324.86\n",
      "Epoch [1/1], Step [269600/274360], Loss: 0.0069, Train time: 324.98\n",
      "Epoch [1/1], Step [269700/274360], Loss: 0.0041, Train time: 325.11\n",
      "Epoch [1/1], Step [269800/274360], Loss: 0.0031, Train time: 325.29\n",
      "Epoch [1/1], Step [269900/274360], Loss: 0.0000, Train time: 325.42\n",
      "Epoch [1/1], Step [270000/274360], Loss: 0.0007, Train time: 325.48\n",
      "Epoch [1/1], Step [270100/274360], Loss: 1.1494, Train time: 325.63\n",
      "Epoch [1/1], Step [270200/274360], Loss: 0.0004, Train time: 325.74\n",
      "Epoch [1/1], Step [270300/274360], Loss: 0.0000, Train time: 326.05\n",
      "Epoch [1/1], Step [270400/274360], Loss: 0.0000, Train time: 326.14\n",
      "Epoch [1/1], Step [270500/274360], Loss: 0.0005, Train time: 326.29\n",
      "Epoch [1/1], Step [270600/274360], Loss: 0.0005, Train time: 326.37\n",
      "Epoch [1/1], Step [270700/274360], Loss: 0.0169, Train time: 326.48\n",
      "Epoch [1/1], Step [270800/274360], Loss: 0.0000, Train time: 326.58\n",
      "Epoch [1/1], Step [270900/274360], Loss: 0.0000, Train time: 326.68\n",
      "Epoch [1/1], Step [271000/274360], Loss: 0.0000, Train time: 326.99\n",
      "Epoch [1/1], Step [271100/274360], Loss: 0.0006, Train time: 327.05\n",
      "Epoch [1/1], Step [271200/274360], Loss: 0.0000, Train time: 327.13\n",
      "Epoch [1/1], Step [271300/274360], Loss: 0.0000, Train time: 327.27\n",
      "Epoch [1/1], Step [271400/274360], Loss: 0.0001, Train time: 327.42\n",
      "Epoch [1/1], Step [271500/274360], Loss: 0.0003, Train time: 327.51\n",
      "Epoch [1/1], Step [271600/274360], Loss: 0.0037, Train time: 327.66\n",
      "Epoch [1/1], Step [271700/274360], Loss: 0.0000, Train time: 327.88\n",
      "Epoch [1/1], Step [271800/274360], Loss: 0.0001, Train time: 328.02\n",
      "Epoch [1/1], Step [271900/274360], Loss: 0.0039, Train time: 328.13\n",
      "Epoch [1/1], Step [272000/274360], Loss: 0.0017, Train time: 328.29\n",
      "Epoch [1/1], Step [272100/274360], Loss: 0.0000, Train time: 328.38\n",
      "Epoch [1/1], Step [272200/274360], Loss: 0.0002, Train time: 328.64\n",
      "Epoch [1/1], Step [272300/274360], Loss: 0.0000, Train time: 328.78\n",
      "Epoch [1/1], Step [272400/274360], Loss: 0.0000, Train time: 328.87\n",
      "Epoch [1/1], Step [272500/274360], Loss: 0.0001, Train time: 328.94\n",
      "Epoch [1/1], Step [272600/274360], Loss: 0.0001, Train time: 329.05\n",
      "Epoch [1/1], Step [272700/274360], Loss: 0.0394, Train time: 329.15\n",
      "Epoch [1/1], Step [272800/274360], Loss: 0.0000, Train time: 329.24\n",
      "Epoch [1/1], Step [272900/274360], Loss: 0.0002, Train time: 329.47\n",
      "Epoch [1/1], Step [273000/274360], Loss: 0.0000, Train time: 329.53\n",
      "Epoch [1/1], Step [273100/274360], Loss: 0.0000, Train time: 329.64\n",
      "Epoch [1/1], Step [273200/274360], Loss: 0.0005, Train time: 329.74\n",
      "Epoch [1/1], Step [273300/274360], Loss: 0.0000, Train time: 329.90\n",
      "Epoch [1/1], Step [273400/274360], Loss: 0.0000, Train time: 330.27\n",
      "Epoch [1/1], Step [273500/274360], Loss: 0.0000, Train time: 330.36\n",
      "Epoch [1/1], Step [273600/274360], Loss: 0.0005, Train time: 330.45\n",
      "Epoch [1/1], Step [273700/274360], Loss: 0.0000, Train time: 330.54\n",
      "Epoch [1/1], Step [273800/274360], Loss: 0.0002, Train time: 330.67\n",
      "Epoch [1/1], Step [273900/274360], Loss: 0.0000, Train time: 330.82\n",
      "Epoch [1/1], Step [274000/274360], Loss: 0.0012, Train time: 331.12\n",
      "Epoch [1/1], Step [274100/274360], Loss: 0.0011, Train time: 331.20\n",
      "Epoch [1/1], Step [274200/274360], Loss: 0.0000, Train time: 331.27\n",
      "Epoch [1/1], Step [274300/274360], Loss: 0.0001, Train time: 331.36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n    # Evaluate the model on the validation dataset\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for inputs, labels in val_loader:\\n            outputs = model(inputs)\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n\\n    print('Accuracy on validation set: {:.2f}%'.format(100 * correct / total))\\n\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 1\n",
    "start_time = time.time()\n",
    "# Loop over the dataset and train the model\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #send data to cuda\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training statistics\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Train time: {:.2f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item(), (time.time()-start_time)/60))\n",
    "'''\n",
    "    # Evaluate the model on the validation dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy on validation set: {:.2f}%'.format(100 * correct / total))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "331136a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/90866], Accuracy: 76.00, Test time: 0.05\n",
      "Epoch [1/1], Step [200/90866], Accuracy: 80.00, Test time: 0.10\n",
      "Epoch [1/1], Step [300/90866], Accuracy: 80.33, Test time: 0.26\n",
      "Epoch [1/1], Step [400/90866], Accuracy: 80.25, Test time: 0.34\n",
      "Epoch [1/1], Step [500/90866], Accuracy: 80.20, Test time: 0.42\n",
      "Epoch [1/1], Step [600/90866], Accuracy: 80.00, Test time: 0.50\n",
      "Epoch [1/1], Step [700/90866], Accuracy: 80.29, Test time: 0.56\n",
      "Epoch [1/1], Step [800/90866], Accuracy: 80.38, Test time: 0.61\n",
      "Epoch [1/1], Step [900/90866], Accuracy: 80.78, Test time: 0.66\n",
      "Epoch [1/1], Step [1000/90866], Accuracy: 80.70, Test time: 0.72\n",
      "Epoch [1/1], Step [1100/90866], Accuracy: 80.73, Test time: 0.77\n",
      "Epoch [1/1], Step [1200/90866], Accuracy: 80.83, Test time: 0.82\n",
      "Epoch [1/1], Step [1300/90866], Accuracy: 81.23, Test time: 0.89\n",
      "Epoch [1/1], Step [1400/90866], Accuracy: 80.86, Test time: 0.94\n",
      "Epoch [1/1], Step [1500/90866], Accuracy: 81.00, Test time: 1.00\n",
      "Epoch [1/1], Step [1600/90866], Accuracy: 81.44, Test time: 1.05\n",
      "Epoch [1/1], Step [1700/90866], Accuracy: 81.41, Test time: 1.11\n",
      "Epoch [1/1], Step [1800/90866], Accuracy: 81.89, Test time: 1.17\n",
      "Epoch [1/1], Step [1900/90866], Accuracy: 81.79, Test time: 1.22\n",
      "Epoch [1/1], Step [2000/90866], Accuracy: 81.50, Test time: 1.31\n",
      "Epoch [1/1], Step [2100/90866], Accuracy: 81.81, Test time: 1.37\n",
      "Epoch [1/1], Step [2200/90866], Accuracy: 81.95, Test time: 1.44\n",
      "Epoch [1/1], Step [2300/90866], Accuracy: 82.30, Test time: 1.50\n",
      "Epoch [1/1], Step [2400/90866], Accuracy: 82.54, Test time: 1.56\n",
      "Epoch [1/1], Step [2500/90866], Accuracy: 82.64, Test time: 1.62\n",
      "Epoch [1/1], Step [2600/90866], Accuracy: 82.77, Test time: 1.68\n",
      "Epoch [1/1], Step [2700/90866], Accuracy: 82.48, Test time: 1.73\n",
      "Epoch [1/1], Step [2800/90866], Accuracy: 82.61, Test time: 1.79\n",
      "Epoch [1/1], Step [2900/90866], Accuracy: 82.48, Test time: 1.86\n",
      "Epoch [1/1], Step [3000/90866], Accuracy: 82.57, Test time: 1.91\n",
      "Epoch [1/1], Step [3100/90866], Accuracy: 82.71, Test time: 1.99\n",
      "Epoch [1/1], Step [3200/90866], Accuracy: 82.56, Test time: 2.04\n",
      "Epoch [1/1], Step [3300/90866], Accuracy: 82.61, Test time: 2.12\n",
      "Epoch [1/1], Step [3400/90866], Accuracy: 82.53, Test time: 2.19\n",
      "Epoch [1/1], Step [3500/90866], Accuracy: 82.43, Test time: 2.31\n",
      "Epoch [1/1], Step [3600/90866], Accuracy: 82.56, Test time: 2.37\n",
      "Epoch [1/1], Step [3700/90866], Accuracy: 82.59, Test time: 2.43\n",
      "Epoch [1/1], Step [3800/90866], Accuracy: 82.61, Test time: 2.54\n",
      "Epoch [1/1], Step [3900/90866], Accuracy: 82.54, Test time: 2.62\n",
      "Epoch [1/1], Step [4000/90866], Accuracy: 82.35, Test time: 2.68\n",
      "Epoch [1/1], Step [4100/90866], Accuracy: 82.49, Test time: 2.77\n",
      "Epoch [1/1], Step [4200/90866], Accuracy: 82.40, Test time: 2.83\n",
      "Epoch [1/1], Step [4300/90866], Accuracy: 82.51, Test time: 2.88\n",
      "Epoch [1/1], Step [4400/90866], Accuracy: 82.55, Test time: 2.96\n",
      "Epoch [1/1], Step [4500/90866], Accuracy: 82.58, Test time: 3.08\n",
      "Epoch [1/1], Step [4600/90866], Accuracy: 82.57, Test time: 3.13\n",
      "Epoch [1/1], Step [4700/90866], Accuracy: 82.40, Test time: 3.24\n",
      "Epoch [1/1], Step [4800/90866], Accuracy: 82.23, Test time: 3.30\n",
      "Epoch [1/1], Step [4900/90866], Accuracy: 82.22, Test time: 3.44\n",
      "Epoch [1/1], Step [5000/90866], Accuracy: 82.24, Test time: 3.54\n",
      "Epoch [1/1], Step [5100/90866], Accuracy: 82.33, Test time: 3.63\n",
      "Epoch [1/1], Step [5200/90866], Accuracy: 82.33, Test time: 3.71\n",
      "Epoch [1/1], Step [5300/90866], Accuracy: 82.15, Test time: 3.77\n",
      "Epoch [1/1], Step [5400/90866], Accuracy: 82.22, Test time: 3.90\n",
      "Epoch [1/1], Step [5500/90866], Accuracy: 82.13, Test time: 3.97\n",
      "Epoch [1/1], Step [5600/90866], Accuracy: 82.11, Test time: 4.03\n",
      "Epoch [1/1], Step [5700/90866], Accuracy: 82.12, Test time: 4.13\n",
      "Epoch [1/1], Step [5800/90866], Accuracy: 82.10, Test time: 4.18\n",
      "Epoch [1/1], Step [5900/90866], Accuracy: 82.15, Test time: 4.27\n",
      "Epoch [1/1], Step [6000/90866], Accuracy: 82.17, Test time: 4.35\n",
      "Epoch [1/1], Step [6100/90866], Accuracy: 82.11, Test time: 4.50\n",
      "Epoch [1/1], Step [6200/90866], Accuracy: 82.13, Test time: 4.61\n",
      "Epoch [1/1], Step [6300/90866], Accuracy: 82.24, Test time: 4.73\n",
      "Epoch [1/1], Step [6400/90866], Accuracy: 82.23, Test time: 4.85\n",
      "Epoch [1/1], Step [6500/90866], Accuracy: 82.15, Test time: 4.93\n",
      "Epoch [1/1], Step [6600/90866], Accuracy: 82.24, Test time: 5.02\n",
      "Epoch [1/1], Step [6700/90866], Accuracy: 82.16, Test time: 5.10\n",
      "Epoch [1/1], Step [6800/90866], Accuracy: 82.22, Test time: 5.25\n",
      "Epoch [1/1], Step [6900/90866], Accuracy: 82.25, Test time: 5.32\n",
      "Epoch [1/1], Step [7000/90866], Accuracy: 82.21, Test time: 5.46\n",
      "Epoch [1/1], Step [7100/90866], Accuracy: 82.15, Test time: 5.58\n",
      "Epoch [1/1], Step [7200/90866], Accuracy: 82.11, Test time: 5.69\n",
      "Epoch [1/1], Step [7300/90866], Accuracy: 82.16, Test time: 5.83\n",
      "Epoch [1/1], Step [7400/90866], Accuracy: 82.14, Test time: 5.94\n",
      "Epoch [1/1], Step [7500/90866], Accuracy: 82.07, Test time: 6.11\n",
      "Epoch [1/1], Step [7600/90866], Accuracy: 82.03, Test time: 6.28\n",
      "Epoch [1/1], Step [7700/90866], Accuracy: 82.10, Test time: 6.39\n",
      "Epoch [1/1], Step [7800/90866], Accuracy: 82.19, Test time: 6.47\n",
      "Epoch [1/1], Step [7900/90866], Accuracy: 82.25, Test time: 6.57\n",
      "Epoch [1/1], Step [8000/90866], Accuracy: 82.16, Test time: 6.69\n",
      "Epoch [1/1], Step [8100/90866], Accuracy: 82.11, Test time: 6.78\n",
      "Epoch [1/1], Step [8200/90866], Accuracy: 82.06, Test time: 6.84\n",
      "Epoch [1/1], Step [8300/90866], Accuracy: 82.11, Test time: 7.07\n",
      "Epoch [1/1], Step [8400/90866], Accuracy: 82.15, Test time: 7.17\n",
      "Epoch [1/1], Step [8500/90866], Accuracy: 82.19, Test time: 7.23\n",
      "Epoch [1/1], Step [8600/90866], Accuracy: 82.22, Test time: 7.28\n",
      "Epoch [1/1], Step [8700/90866], Accuracy: 82.20, Test time: 7.49\n",
      "Epoch [1/1], Step [8800/90866], Accuracy: 82.20, Test time: 7.61\n",
      "Epoch [1/1], Step [8900/90866], Accuracy: 82.19, Test time: 7.80\n",
      "Epoch [1/1], Step [9000/90866], Accuracy: 82.22, Test time: 7.85\n",
      "Epoch [1/1], Step [9100/90866], Accuracy: 82.11, Test time: 7.91\n",
      "Epoch [1/1], Step [9200/90866], Accuracy: 82.08, Test time: 8.01\n",
      "Epoch [1/1], Step [9300/90866], Accuracy: 82.03, Test time: 8.16\n",
      "Epoch [1/1], Step [9400/90866], Accuracy: 82.10, Test time: 8.23\n",
      "Epoch [1/1], Step [9500/90866], Accuracy: 82.11, Test time: 8.37\n",
      "Epoch [1/1], Step [9600/90866], Accuracy: 82.18, Test time: 8.48\n",
      "Epoch [1/1], Step [9700/90866], Accuracy: 82.24, Test time: 8.57\n",
      "Epoch [1/1], Step [9800/90866], Accuracy: 82.19, Test time: 8.63\n",
      "Epoch [1/1], Step [9900/90866], Accuracy: 82.27, Test time: 8.75\n",
      "Epoch [1/1], Step [10000/90866], Accuracy: 82.23, Test time: 8.94\n",
      "Epoch [1/1], Step [10100/90866], Accuracy: 82.23, Test time: 9.02\n",
      "Epoch [1/1], Step [10200/90866], Accuracy: 82.23, Test time: 9.17\n",
      "Epoch [1/1], Step [10300/90866], Accuracy: 82.25, Test time: 9.27\n",
      "Epoch [1/1], Step [10400/90866], Accuracy: 82.29, Test time: 9.42\n",
      "Epoch [1/1], Step [10500/90866], Accuracy: 82.27, Test time: 9.51\n",
      "Epoch [1/1], Step [10600/90866], Accuracy: 82.27, Test time: 9.61\n",
      "Epoch [1/1], Step [10700/90866], Accuracy: 82.32, Test time: 9.72\n",
      "Epoch [1/1], Step [10800/90866], Accuracy: 82.30, Test time: 9.88\n",
      "Epoch [1/1], Step [10900/90866], Accuracy: 82.34, Test time: 9.95\n",
      "Epoch [1/1], Step [11000/90866], Accuracy: 82.34, Test time: 10.09\n",
      "Epoch [1/1], Step [11100/90866], Accuracy: 82.33, Test time: 10.18\n",
      "Epoch [1/1], Step [11200/90866], Accuracy: 82.38, Test time: 10.26\n",
      "Epoch [1/1], Step [11300/90866], Accuracy: 82.36, Test time: 10.35\n",
      "Epoch [1/1], Step [11400/90866], Accuracy: 82.34, Test time: 10.45\n",
      "Epoch [1/1], Step [11500/90866], Accuracy: 82.34, Test time: 10.55\n",
      "Epoch [1/1], Step [11600/90866], Accuracy: 82.30, Test time: 10.71\n",
      "Epoch [1/1], Step [11700/90866], Accuracy: 82.29, Test time: 10.80\n",
      "Epoch [1/1], Step [11800/90866], Accuracy: 82.28, Test time: 10.92\n",
      "Epoch [1/1], Step [11900/90866], Accuracy: 82.25, Test time: 11.02\n",
      "Epoch [1/1], Step [12000/90866], Accuracy: 82.25, Test time: 11.12\n",
      "Epoch [1/1], Step [12100/90866], Accuracy: 82.26, Test time: 11.24\n",
      "Epoch [1/1], Step [12200/90866], Accuracy: 82.25, Test time: 11.33\n",
      "Epoch [1/1], Step [12300/90866], Accuracy: 82.28, Test time: 11.45\n",
      "Epoch [1/1], Step [12400/90866], Accuracy: 82.32, Test time: 11.54\n",
      "Epoch [1/1], Step [12500/90866], Accuracy: 82.34, Test time: 11.65\n",
      "Epoch [1/1], Step [12600/90866], Accuracy: 82.40, Test time: 11.76\n",
      "Epoch [1/1], Step [12700/90866], Accuracy: 82.39, Test time: 11.85\n",
      "Epoch [1/1], Step [12800/90866], Accuracy: 82.34, Test time: 11.93\n",
      "Epoch [1/1], Step [12900/90866], Accuracy: 82.29, Test time: 12.10\n",
      "Epoch [1/1], Step [13000/90866], Accuracy: 82.25, Test time: 12.25\n",
      "Epoch [1/1], Step [13100/90866], Accuracy: 82.24, Test time: 12.40\n",
      "Epoch [1/1], Step [13200/90866], Accuracy: 82.25, Test time: 12.49\n",
      "Epoch [1/1], Step [13300/90866], Accuracy: 82.24, Test time: 12.58\n",
      "Epoch [1/1], Step [13400/90866], Accuracy: 82.27, Test time: 12.73\n",
      "Epoch [1/1], Step [13500/90866], Accuracy: 82.27, Test time: 12.81\n",
      "Epoch [1/1], Step [13600/90866], Accuracy: 82.26, Test time: 12.91\n",
      "Epoch [1/1], Step [13700/90866], Accuracy: 82.26, Test time: 13.00\n",
      "Epoch [1/1], Step [13800/90866], Accuracy: 82.27, Test time: 13.19\n",
      "Epoch [1/1], Step [13900/90866], Accuracy: 82.22, Test time: 13.29\n",
      "Epoch [1/1], Step [14000/90866], Accuracy: 82.23, Test time: 13.40\n",
      "Epoch [1/1], Step [14100/90866], Accuracy: 82.18, Test time: 13.50\n",
      "Epoch [1/1], Step [14200/90866], Accuracy: 82.16, Test time: 13.64\n",
      "Epoch [1/1], Step [14300/90866], Accuracy: 82.15, Test time: 13.74\n",
      "Epoch [1/1], Step [14400/90866], Accuracy: 82.19, Test time: 13.87\n",
      "Epoch [1/1], Step [14500/90866], Accuracy: 82.17, Test time: 13.96\n",
      "Epoch [1/1], Step [14600/90866], Accuracy: 82.21, Test time: 14.13\n",
      "Epoch [1/1], Step [14700/90866], Accuracy: 82.24, Test time: 14.19\n",
      "Epoch [1/1], Step [14800/90866], Accuracy: 82.24, Test time: 14.33\n",
      "Epoch [1/1], Step [14900/90866], Accuracy: 82.19, Test time: 14.41\n",
      "Epoch [1/1], Step [15000/90866], Accuracy: 82.21, Test time: 14.55\n",
      "Epoch [1/1], Step [15100/90866], Accuracy: 82.17, Test time: 14.65\n",
      "Epoch [1/1], Step [15200/90866], Accuracy: 82.15, Test time: 14.77\n",
      "Epoch [1/1], Step [15300/90866], Accuracy: 82.11, Test time: 14.86\n",
      "Epoch [1/1], Step [15400/90866], Accuracy: 82.11, Test time: 15.01\n",
      "Epoch [1/1], Step [15500/90866], Accuracy: 82.15, Test time: 15.11\n",
      "Epoch [1/1], Step [15600/90866], Accuracy: 82.11, Test time: 15.18\n",
      "Epoch [1/1], Step [15700/90866], Accuracy: 82.13, Test time: 15.34\n",
      "Epoch [1/1], Step [15800/90866], Accuracy: 82.16, Test time: 15.40\n",
      "Epoch [1/1], Step [15900/90866], Accuracy: 82.14, Test time: 15.55\n",
      "Epoch [1/1], Step [16000/90866], Accuracy: 82.11, Test time: 15.66\n",
      "Epoch [1/1], Step [16100/90866], Accuracy: 82.13, Test time: 15.78\n",
      "Epoch [1/1], Step [16200/90866], Accuracy: 82.12, Test time: 15.89\n",
      "Epoch [1/1], Step [16300/90866], Accuracy: 82.07, Test time: 15.97\n",
      "Epoch [1/1], Step [16400/90866], Accuracy: 82.09, Test time: 16.10\n",
      "Epoch [1/1], Step [16500/90866], Accuracy: 82.07, Test time: 16.21\n",
      "Epoch [1/1], Step [16600/90866], Accuracy: 82.09, Test time: 16.30\n",
      "Epoch [1/1], Step [16700/90866], Accuracy: 82.10, Test time: 16.41\n",
      "Epoch [1/1], Step [16800/90866], Accuracy: 82.12, Test time: 16.55\n",
      "Epoch [1/1], Step [16900/90866], Accuracy: 82.15, Test time: 16.63\n",
      "Epoch [1/1], Step [17000/90866], Accuracy: 82.12, Test time: 16.75\n",
      "Epoch [1/1], Step [17100/90866], Accuracy: 82.15, Test time: 16.85\n",
      "Epoch [1/1], Step [17200/90866], Accuracy: 82.13, Test time: 16.98\n",
      "Epoch [1/1], Step [17300/90866], Accuracy: 82.14, Test time: 17.07\n",
      "Epoch [1/1], Step [17400/90866], Accuracy: 82.11, Test time: 17.18\n",
      "Epoch [1/1], Step [17500/90866], Accuracy: 82.13, Test time: 17.26\n",
      "Epoch [1/1], Step [17600/90866], Accuracy: 82.10, Test time: 17.34\n",
      "Epoch [1/1], Step [17700/90866], Accuracy: 82.11, Test time: 17.49\n",
      "Epoch [1/1], Step [17800/90866], Accuracy: 82.15, Test time: 17.57\n",
      "Epoch [1/1], Step [17900/90866], Accuracy: 82.13, Test time: 17.69\n",
      "Epoch [1/1], Step [18000/90866], Accuracy: 82.09, Test time: 17.81\n",
      "Epoch [1/1], Step [18100/90866], Accuracy: 82.09, Test time: 17.95\n",
      "Epoch [1/1], Step [18200/90866], Accuracy: 82.12, Test time: 18.08\n",
      "Epoch [1/1], Step [18300/90866], Accuracy: 82.13, Test time: 18.19\n",
      "Epoch [1/1], Step [18400/90866], Accuracy: 82.11, Test time: 18.29\n",
      "Epoch [1/1], Step [18500/90866], Accuracy: 82.08, Test time: 18.39\n",
      "Epoch [1/1], Step [18600/90866], Accuracy: 82.07, Test time: 18.50\n",
      "Epoch [1/1], Step [18700/90866], Accuracy: 82.11, Test time: 18.62\n",
      "Epoch [1/1], Step [18800/90866], Accuracy: 82.12, Test time: 18.69\n",
      "Epoch [1/1], Step [18900/90866], Accuracy: 82.13, Test time: 18.80\n",
      "Epoch [1/1], Step [19000/90866], Accuracy: 82.11, Test time: 18.91\n",
      "Epoch [1/1], Step [19100/90866], Accuracy: 82.12, Test time: 19.09\n",
      "Epoch [1/1], Step [19200/90866], Accuracy: 82.11, Test time: 19.27\n",
      "Epoch [1/1], Step [19300/90866], Accuracy: 82.10, Test time: 19.38\n",
      "Epoch [1/1], Step [19400/90866], Accuracy: 82.11, Test time: 19.53\n",
      "Epoch [1/1], Step [19500/90866], Accuracy: 82.15, Test time: 19.64\n",
      "Epoch [1/1], Step [19600/90866], Accuracy: 82.10, Test time: 19.71\n",
      "Epoch [1/1], Step [19700/90866], Accuracy: 82.10, Test time: 19.82\n",
      "Epoch [1/1], Step [19800/90866], Accuracy: 82.09, Test time: 19.94\n",
      "Epoch [1/1], Step [19900/90866], Accuracy: 82.09, Test time: 20.07\n",
      "Epoch [1/1], Step [20000/90866], Accuracy: 82.06, Test time: 20.18\n",
      "Epoch [1/1], Step [20100/90866], Accuracy: 82.08, Test time: 20.28\n",
      "Epoch [1/1], Step [20200/90866], Accuracy: 82.12, Test time: 20.42\n",
      "Epoch [1/1], Step [20300/90866], Accuracy: 82.11, Test time: 20.50\n",
      "Epoch [1/1], Step [20400/90866], Accuracy: 82.11, Test time: 20.62\n",
      "Epoch [1/1], Step [20500/90866], Accuracy: 82.10, Test time: 20.79\n",
      "Epoch [1/1], Step [20600/90866], Accuracy: 82.09, Test time: 20.90\n",
      "Epoch [1/1], Step [20700/90866], Accuracy: 82.06, Test time: 21.04\n",
      "Epoch [1/1], Step [20800/90866], Accuracy: 82.07, Test time: 21.13\n",
      "Epoch [1/1], Step [20900/90866], Accuracy: 82.02, Test time: 21.29\n",
      "Epoch [1/1], Step [21000/90866], Accuracy: 82.00, Test time: 21.40\n",
      "Epoch [1/1], Step [21100/90866], Accuracy: 82.02, Test time: 21.49\n",
      "Epoch [1/1], Step [21200/90866], Accuracy: 82.00, Test time: 21.68\n",
      "Epoch [1/1], Step [21300/90866], Accuracy: 82.00, Test time: 21.77\n",
      "Epoch [1/1], Step [21400/90866], Accuracy: 82.02, Test time: 21.89\n",
      "Epoch [1/1], Step [21500/90866], Accuracy: 82.03, Test time: 22.00\n",
      "Epoch [1/1], Step [21600/90866], Accuracy: 82.05, Test time: 22.12\n",
      "Epoch [1/1], Step [21700/90866], Accuracy: 82.06, Test time: 22.21\n",
      "Epoch [1/1], Step [21800/90866], Accuracy: 82.04, Test time: 22.33\n",
      "Epoch [1/1], Step [21900/90866], Accuracy: 82.03, Test time: 22.48\n",
      "Epoch [1/1], Step [22000/90866], Accuracy: 82.05, Test time: 22.62\n",
      "Epoch [1/1], Step [22100/90866], Accuracy: 82.02, Test time: 22.69\n",
      "Epoch [1/1], Step [22200/90866], Accuracy: 82.05, Test time: 22.78\n",
      "Epoch [1/1], Step [22300/90866], Accuracy: 82.06, Test time: 22.93\n",
      "Epoch [1/1], Step [22400/90866], Accuracy: 82.06, Test time: 23.02\n",
      "Epoch [1/1], Step [22500/90866], Accuracy: 82.02, Test time: 23.13\n",
      "Epoch [1/1], Step [22600/90866], Accuracy: 82.02, Test time: 23.22\n",
      "Epoch [1/1], Step [22700/90866], Accuracy: 82.03, Test time: 23.36\n",
      "Epoch [1/1], Step [22800/90866], Accuracy: 82.06, Test time: 23.45\n",
      "Epoch [1/1], Step [22900/90866], Accuracy: 82.04, Test time: 23.58\n",
      "Epoch [1/1], Step [23000/90866], Accuracy: 82.03, Test time: 23.70\n",
      "Epoch [1/1], Step [23100/90866], Accuracy: 82.03, Test time: 23.82\n",
      "Epoch [1/1], Step [23200/90866], Accuracy: 82.01, Test time: 23.96\n",
      "Epoch [1/1], Step [23300/90866], Accuracy: 81.99, Test time: 24.07\n",
      "Epoch [1/1], Step [23400/90866], Accuracy: 82.01, Test time: 24.13\n",
      "Epoch [1/1], Step [23500/90866], Accuracy: 82.02, Test time: 24.37\n",
      "Epoch [1/1], Step [23600/90866], Accuracy: 82.03, Test time: 24.44\n",
      "Epoch [1/1], Step [23700/90866], Accuracy: 81.99, Test time: 24.52\n",
      "Epoch [1/1], Step [23800/90866], Accuracy: 82.01, Test time: 24.69\n",
      "Epoch [1/1], Step [23900/90866], Accuracy: 81.98, Test time: 24.79\n",
      "Epoch [1/1], Step [24000/90866], Accuracy: 82.00, Test time: 24.88\n",
      "Epoch [1/1], Step [24100/90866], Accuracy: 81.98, Test time: 25.03\n",
      "Epoch [1/1], Step [24200/90866], Accuracy: 81.93, Test time: 25.12\n",
      "Epoch [1/1], Step [24300/90866], Accuracy: 81.90, Test time: 25.27\n",
      "Epoch [1/1], Step [24400/90866], Accuracy: 81.90, Test time: 25.35\n",
      "Epoch [1/1], Step [24500/90866], Accuracy: 81.90, Test time: 25.50\n",
      "Epoch [1/1], Step [24600/90866], Accuracy: 81.89, Test time: 25.60\n",
      "Epoch [1/1], Step [24700/90866], Accuracy: 81.89, Test time: 25.71\n",
      "Epoch [1/1], Step [24800/90866], Accuracy: 81.89, Test time: 25.81\n",
      "Epoch [1/1], Step [24900/90866], Accuracy: 81.91, Test time: 26.02\n",
      "Epoch [1/1], Step [25000/90866], Accuracy: 81.91, Test time: 26.10\n",
      "Epoch [1/1], Step [25100/90866], Accuracy: 81.89, Test time: 26.26\n",
      "Epoch [1/1], Step [25200/90866], Accuracy: 81.91, Test time: 26.37\n",
      "Epoch [1/1], Step [25300/90866], Accuracy: 81.89, Test time: 26.49\n",
      "Epoch [1/1], Step [25400/90866], Accuracy: 81.87, Test time: 26.60\n",
      "Epoch [1/1], Step [25500/90866], Accuracy: 81.89, Test time: 26.69\n",
      "Epoch [1/1], Step [25600/90866], Accuracy: 81.91, Test time: 26.83\n",
      "Epoch [1/1], Step [25700/90866], Accuracy: 81.92, Test time: 26.89\n",
      "Epoch [1/1], Step [25800/90866], Accuracy: 81.92, Test time: 27.10\n",
      "Epoch [1/1], Step [25900/90866], Accuracy: 81.91, Test time: 27.19\n",
      "Epoch [1/1], Step [26000/90866], Accuracy: 81.90, Test time: 27.30\n",
      "Epoch [1/1], Step [26100/90866], Accuracy: 81.93, Test time: 27.43\n",
      "Epoch [1/1], Step [26200/90866], Accuracy: 81.94, Test time: 27.51\n",
      "Epoch [1/1], Step [26300/90866], Accuracy: 81.95, Test time: 27.67\n",
      "Epoch [1/1], Step [26400/90866], Accuracy: 81.97, Test time: 27.77\n",
      "Epoch [1/1], Step [26500/90866], Accuracy: 81.98, Test time: 27.85\n",
      "Epoch [1/1], Step [26600/90866], Accuracy: 81.98, Test time: 27.96\n",
      "Epoch [1/1], Step [26700/90866], Accuracy: 82.00, Test time: 28.08\n",
      "Epoch [1/1], Step [26800/90866], Accuracy: 82.03, Test time: 28.19\n",
      "Epoch [1/1], Step [26900/90866], Accuracy: 82.01, Test time: 28.33\n",
      "Epoch [1/1], Step [27000/90866], Accuracy: 82.01, Test time: 28.45\n",
      "Epoch [1/1], Step [27100/90866], Accuracy: 82.03, Test time: 28.53\n",
      "Epoch [1/1], Step [27200/90866], Accuracy: 82.01, Test time: 28.63\n",
      "Epoch [1/1], Step [27300/90866], Accuracy: 82.01, Test time: 28.75\n",
      "Epoch [1/1], Step [27400/90866], Accuracy: 82.01, Test time: 28.88\n",
      "Epoch [1/1], Step [27500/90866], Accuracy: 82.01, Test time: 28.98\n",
      "Epoch [1/1], Step [27600/90866], Accuracy: 82.00, Test time: 29.10\n",
      "Epoch [1/1], Step [27700/90866], Accuracy: 82.00, Test time: 29.28\n",
      "Epoch [1/1], Step [27800/90866], Accuracy: 81.99, Test time: 29.38\n",
      "Epoch [1/1], Step [27900/90866], Accuracy: 82.00, Test time: 29.49\n",
      "Epoch [1/1], Step [28000/90866], Accuracy: 82.00, Test time: 29.62\n",
      "Epoch [1/1], Step [28100/90866], Accuracy: 82.00, Test time: 29.79\n",
      "Epoch [1/1], Step [28200/90866], Accuracy: 82.00, Test time: 29.96\n",
      "Epoch [1/1], Step [28300/90866], Accuracy: 82.00, Test time: 30.03\n",
      "Epoch [1/1], Step [28400/90866], Accuracy: 82.01, Test time: 30.16\n",
      "Epoch [1/1], Step [28500/90866], Accuracy: 82.02, Test time: 30.21\n",
      "Epoch [1/1], Step [28600/90866], Accuracy: 82.01, Test time: 30.40\n",
      "Epoch [1/1], Step [28700/90866], Accuracy: 82.01, Test time: 30.48\n",
      "Epoch [1/1], Step [28800/90866], Accuracy: 82.02, Test time: 30.61\n",
      "Epoch [1/1], Step [28900/90866], Accuracy: 82.03, Test time: 30.74\n",
      "Epoch [1/1], Step [29000/90866], Accuracy: 82.02, Test time: 30.85\n",
      "Epoch [1/1], Step [29100/90866], Accuracy: 82.03, Test time: 30.98\n",
      "Epoch [1/1], Step [29200/90866], Accuracy: 82.03, Test time: 31.07\n",
      "Epoch [1/1], Step [29300/90866], Accuracy: 82.03, Test time: 31.18\n",
      "Epoch [1/1], Step [29400/90866], Accuracy: 82.04, Test time: 31.28\n",
      "Epoch [1/1], Step [29500/90866], Accuracy: 82.04, Test time: 31.39\n",
      "Epoch [1/1], Step [29600/90866], Accuracy: 82.05, Test time: 31.50\n",
      "Epoch [1/1], Step [29700/90866], Accuracy: 82.04, Test time: 31.61\n",
      "Epoch [1/1], Step [29800/90866], Accuracy: 82.03, Test time: 31.76\n",
      "Epoch [1/1], Step [29900/90866], Accuracy: 82.04, Test time: 31.83\n",
      "Epoch [1/1], Step [30000/90866], Accuracy: 82.04, Test time: 31.93\n",
      "Epoch [1/1], Step [30100/90866], Accuracy: 82.04, Test time: 32.11\n",
      "Epoch [1/1], Step [30200/90866], Accuracy: 82.03, Test time: 32.22\n",
      "Epoch [1/1], Step [30300/90866], Accuracy: 82.04, Test time: 32.28\n",
      "Epoch [1/1], Step [30400/90866], Accuracy: 82.07, Test time: 32.42\n",
      "Epoch [1/1], Step [30500/90866], Accuracy: 82.06, Test time: 32.57\n",
      "Epoch [1/1], Step [30600/90866], Accuracy: 82.05, Test time: 32.69\n",
      "Epoch [1/1], Step [30700/90866], Accuracy: 82.07, Test time: 32.77\n",
      "Epoch [1/1], Step [30800/90866], Accuracy: 82.06, Test time: 32.94\n",
      "Epoch [1/1], Step [30900/90866], Accuracy: 82.07, Test time: 33.03\n",
      "Epoch [1/1], Step [31000/90866], Accuracy: 82.06, Test time: 33.14\n",
      "Epoch [1/1], Step [31100/90866], Accuracy: 82.10, Test time: 33.31\n",
      "Epoch [1/1], Step [31200/90866], Accuracy: 82.09, Test time: 33.42\n",
      "Epoch [1/1], Step [31300/90866], Accuracy: 82.11, Test time: 33.50\n",
      "Epoch [1/1], Step [31400/90866], Accuracy: 82.10, Test time: 33.61\n",
      "Epoch [1/1], Step [31500/90866], Accuracy: 82.08, Test time: 33.69\n",
      "Epoch [1/1], Step [31600/90866], Accuracy: 82.07, Test time: 33.83\n",
      "Epoch [1/1], Step [31700/90866], Accuracy: 82.06, Test time: 33.92\n",
      "Epoch [1/1], Step [31800/90866], Accuracy: 82.06, Test time: 34.10\n",
      "Epoch [1/1], Step [31900/90866], Accuracy: 82.06, Test time: 34.19\n",
      "Epoch [1/1], Step [32000/90866], Accuracy: 82.06, Test time: 34.31\n",
      "Epoch [1/1], Step [32100/90866], Accuracy: 82.06, Test time: 34.44\n",
      "Epoch [1/1], Step [32200/90866], Accuracy: 82.07, Test time: 34.55\n",
      "Epoch [1/1], Step [32300/90866], Accuracy: 82.06, Test time: 34.66\n",
      "Epoch [1/1], Step [32400/90866], Accuracy: 82.06, Test time: 34.79\n",
      "Epoch [1/1], Step [32500/90866], Accuracy: 82.06, Test time: 34.95\n",
      "Epoch [1/1], Step [32600/90866], Accuracy: 82.06, Test time: 35.06\n",
      "Epoch [1/1], Step [32700/90866], Accuracy: 82.05, Test time: 35.18\n",
      "Epoch [1/1], Step [32800/90866], Accuracy: 82.04, Test time: 35.28\n",
      "Epoch [1/1], Step [32900/90866], Accuracy: 82.05, Test time: 35.38\n",
      "Epoch [1/1], Step [33000/90866], Accuracy: 82.06, Test time: 35.50\n",
      "Epoch [1/1], Step [33100/90866], Accuracy: 82.06, Test time: 35.62\n",
      "Epoch [1/1], Step [33200/90866], Accuracy: 82.07, Test time: 35.70\n",
      "Epoch [1/1], Step [33300/90866], Accuracy: 82.06, Test time: 35.87\n",
      "Epoch [1/1], Step [33400/90866], Accuracy: 82.07, Test time: 35.98\n",
      "Epoch [1/1], Step [33500/90866], Accuracy: 82.07, Test time: 36.07\n",
      "Epoch [1/1], Step [33600/90866], Accuracy: 82.09, Test time: 36.18\n",
      "Epoch [1/1], Step [33700/90866], Accuracy: 82.09, Test time: 36.29\n",
      "Epoch [1/1], Step [33800/90866], Accuracy: 82.10, Test time: 36.51\n",
      "Epoch [1/1], Step [33900/90866], Accuracy: 82.09, Test time: 36.62\n",
      "Epoch [1/1], Step [34000/90866], Accuracy: 82.10, Test time: 36.71\n",
      "Epoch [1/1], Step [34100/90866], Accuracy: 82.09, Test time: 36.85\n",
      "Epoch [1/1], Step [34200/90866], Accuracy: 82.10, Test time: 36.98\n",
      "Epoch [1/1], Step [34300/90866], Accuracy: 82.11, Test time: 37.19\n",
      "Epoch [1/1], Step [34400/90866], Accuracy: 82.10, Test time: 37.23\n",
      "Epoch [1/1], Step [34500/90866], Accuracy: 82.10, Test time: 37.32\n",
      "Epoch [1/1], Step [34600/90866], Accuracy: 82.10, Test time: 37.48\n",
      "Epoch [1/1], Step [34700/90866], Accuracy: 82.10, Test time: 37.62\n",
      "Epoch [1/1], Step [34800/90866], Accuracy: 82.09, Test time: 37.80\n",
      "Epoch [1/1], Step [34900/90866], Accuracy: 82.09, Test time: 37.87\n",
      "Epoch [1/1], Step [35000/90866], Accuracy: 82.10, Test time: 37.93\n",
      "Epoch [1/1], Step [35100/90866], Accuracy: 82.08, Test time: 38.05\n",
      "Epoch [1/1], Step [35200/90866], Accuracy: 82.09, Test time: 38.21\n",
      "Epoch [1/1], Step [35300/90866], Accuracy: 82.10, Test time: 38.28\n",
      "Epoch [1/1], Step [35400/90866], Accuracy: 82.09, Test time: 38.38\n",
      "Epoch [1/1], Step [35500/90866], Accuracy: 82.10, Test time: 38.51\n",
      "Epoch [1/1], Step [35600/90866], Accuracy: 82.08, Test time: 38.59\n",
      "Epoch [1/1], Step [35700/90866], Accuracy: 82.07, Test time: 38.73\n",
      "Epoch [1/1], Step [35800/90866], Accuracy: 82.08, Test time: 38.82\n",
      "Epoch [1/1], Step [35900/90866], Accuracy: 82.08, Test time: 38.98\n",
      "Epoch [1/1], Step [36000/90866], Accuracy: 82.08, Test time: 39.09\n",
      "Epoch [1/1], Step [36100/90866], Accuracy: 82.07, Test time: 39.18\n",
      "Epoch [1/1], Step [36200/90866], Accuracy: 82.07, Test time: 39.29\n",
      "Epoch [1/1], Step [36300/90866], Accuracy: 82.06, Test time: 39.41\n",
      "Epoch [1/1], Step [36400/90866], Accuracy: 82.07, Test time: 39.57\n",
      "Epoch [1/1], Step [36500/90866], Accuracy: 82.10, Test time: 39.71\n",
      "Epoch [1/1], Step [36600/90866], Accuracy: 82.10, Test time: 39.92\n",
      "Epoch [1/1], Step [36700/90866], Accuracy: 82.12, Test time: 39.99\n",
      "Epoch [1/1], Step [36800/90866], Accuracy: 82.12, Test time: 40.14\n",
      "Epoch [1/1], Step [36900/90866], Accuracy: 82.11, Test time: 40.24\n",
      "Epoch [1/1], Step [37000/90866], Accuracy: 82.12, Test time: 40.35\n",
      "Epoch [1/1], Step [37100/90866], Accuracy: 82.12, Test time: 40.48\n",
      "Epoch [1/1], Step [37200/90866], Accuracy: 82.10, Test time: 40.56\n",
      "Epoch [1/1], Step [37300/90866], Accuracy: 82.11, Test time: 40.73\n",
      "Epoch [1/1], Step [37400/90866], Accuracy: 82.11, Test time: 40.81\n",
      "Epoch [1/1], Step [37500/90866], Accuracy: 82.11, Test time: 40.91\n",
      "Epoch [1/1], Step [37600/90866], Accuracy: 82.11, Test time: 41.01\n",
      "Epoch [1/1], Step [37700/90866], Accuracy: 82.11, Test time: 41.12\n",
      "Epoch [1/1], Step [37800/90866], Accuracy: 82.11, Test time: 41.25\n",
      "Epoch [1/1], Step [37900/90866], Accuracy: 82.12, Test time: 41.33\n",
      "Epoch [1/1], Step [38000/90866], Accuracy: 82.09, Test time: 41.44\n",
      "Epoch [1/1], Step [38100/90866], Accuracy: 82.09, Test time: 41.53\n",
      "Epoch [1/1], Step [38200/90866], Accuracy: 82.11, Test time: 41.63\n",
      "Epoch [1/1], Step [38300/90866], Accuracy: 82.12, Test time: 41.75\n",
      "Epoch [1/1], Step [38400/90866], Accuracy: 82.13, Test time: 41.82\n",
      "Epoch [1/1], Step [38500/90866], Accuracy: 82.13, Test time: 41.96\n",
      "Epoch [1/1], Step [38600/90866], Accuracy: 82.13, Test time: 42.08\n",
      "Epoch [1/1], Step [38700/90866], Accuracy: 82.11, Test time: 42.16\n",
      "Epoch [1/1], Step [38800/90866], Accuracy: 82.11, Test time: 42.29\n",
      "Epoch [1/1], Step [38900/90866], Accuracy: 82.12, Test time: 42.44\n",
      "Epoch [1/1], Step [39000/90866], Accuracy: 82.12, Test time: 42.54\n",
      "Epoch [1/1], Step [39100/90866], Accuracy: 82.14, Test time: 42.69\n",
      "Epoch [1/1], Step [39200/90866], Accuracy: 82.12, Test time: 42.78\n",
      "Epoch [1/1], Step [39300/90866], Accuracy: 82.12, Test time: 42.86\n",
      "Epoch [1/1], Step [39400/90866], Accuracy: 82.11, Test time: 43.01\n",
      "Epoch [1/1], Step [39500/90866], Accuracy: 82.11, Test time: 43.14\n",
      "Epoch [1/1], Step [39600/90866], Accuracy: 82.11, Test time: 43.23\n",
      "Epoch [1/1], Step [39700/90866], Accuracy: 82.10, Test time: 43.30\n",
      "Epoch [1/1], Step [39800/90866], Accuracy: 82.09, Test time: 43.46\n",
      "Epoch [1/1], Step [39900/90866], Accuracy: 82.09, Test time: 43.52\n",
      "Epoch [1/1], Step [40000/90866], Accuracy: 82.09, Test time: 43.63\n",
      "Epoch [1/1], Step [40100/90866], Accuracy: 82.10, Test time: 43.74\n",
      "Epoch [1/1], Step [40200/90866], Accuracy: 82.09, Test time: 43.91\n",
      "Epoch [1/1], Step [40300/90866], Accuracy: 82.08, Test time: 44.04\n",
      "Epoch [1/1], Step [40400/90866], Accuracy: 82.06, Test time: 44.20\n",
      "Epoch [1/1], Step [40500/90866], Accuracy: 82.07, Test time: 44.29\n",
      "Epoch [1/1], Step [40600/90866], Accuracy: 82.06, Test time: 44.39\n",
      "Epoch [1/1], Step [40700/90866], Accuracy: 82.07, Test time: 44.51\n",
      "Epoch [1/1], Step [40800/90866], Accuracy: 82.09, Test time: 44.60\n",
      "Epoch [1/1], Step [40900/90866], Accuracy: 82.08, Test time: 44.70\n",
      "Epoch [1/1], Step [41000/90866], Accuracy: 82.07, Test time: 44.80\n",
      "Epoch [1/1], Step [41100/90866], Accuracy: 82.09, Test time: 44.92\n",
      "Epoch [1/1], Step [41200/90866], Accuracy: 82.09, Test time: 45.07\n",
      "Epoch [1/1], Step [41300/90866], Accuracy: 82.08, Test time: 45.16\n",
      "Epoch [1/1], Step [41400/90866], Accuracy: 82.09, Test time: 45.30\n",
      "Epoch [1/1], Step [41500/90866], Accuracy: 82.10, Test time: 45.39\n",
      "Epoch [1/1], Step [41600/90866], Accuracy: 82.10, Test time: 45.52\n",
      "Epoch [1/1], Step [41700/90866], Accuracy: 82.08, Test time: 45.61\n",
      "Epoch [1/1], Step [41800/90866], Accuracy: 82.08, Test time: 45.75\n",
      "Epoch [1/1], Step [41900/90866], Accuracy: 82.08, Test time: 45.83\n",
      "Epoch [1/1], Step [42000/90866], Accuracy: 82.08, Test time: 45.95\n",
      "Epoch [1/1], Step [42100/90866], Accuracy: 82.08, Test time: 46.10\n",
      "Epoch [1/1], Step [42200/90866], Accuracy: 82.08, Test time: 46.24\n",
      "Epoch [1/1], Step [42300/90866], Accuracy: 82.08, Test time: 46.33\n",
      "Epoch [1/1], Step [42400/90866], Accuracy: 82.08, Test time: 46.49\n",
      "Epoch [1/1], Step [42500/90866], Accuracy: 82.08, Test time: 46.68\n",
      "Epoch [1/1], Step [42600/90866], Accuracy: 82.08, Test time: 46.75\n",
      "Epoch [1/1], Step [42700/90866], Accuracy: 82.09, Test time: 46.83\n",
      "Epoch [1/1], Step [42800/90866], Accuracy: 82.07, Test time: 47.16\n",
      "Epoch [1/1], Step [42900/90866], Accuracy: 82.07, Test time: 47.24\n",
      "Epoch [1/1], Step [43000/90866], Accuracy: 82.07, Test time: 47.37\n",
      "Epoch [1/1], Step [43100/90866], Accuracy: 82.08, Test time: 47.47\n",
      "Epoch [1/1], Step [43200/90866], Accuracy: 82.08, Test time: 47.60\n",
      "Epoch [1/1], Step [43300/90866], Accuracy: 82.09, Test time: 47.72\n",
      "Epoch [1/1], Step [43400/90866], Accuracy: 82.09, Test time: 47.87\n",
      "Epoch [1/1], Step [43500/90866], Accuracy: 82.09, Test time: 48.00\n",
      "Epoch [1/1], Step [43600/90866], Accuracy: 82.10, Test time: 48.10\n",
      "Epoch [1/1], Step [43700/90866], Accuracy: 82.10, Test time: 48.19\n",
      "Epoch [1/1], Step [43800/90866], Accuracy: 82.09, Test time: 48.31\n",
      "Epoch [1/1], Step [43900/90866], Accuracy: 82.08, Test time: 48.42\n",
      "Epoch [1/1], Step [44000/90866], Accuracy: 82.07, Test time: 48.57\n",
      "Epoch [1/1], Step [44100/90866], Accuracy: 82.07, Test time: 48.65\n",
      "Epoch [1/1], Step [44200/90866], Accuracy: 82.07, Test time: 48.72\n",
      "Epoch [1/1], Step [44300/90866], Accuracy: 82.05, Test time: 48.89\n",
      "Epoch [1/1], Step [44400/90866], Accuracy: 82.04, Test time: 48.96\n",
      "Epoch [1/1], Step [44500/90866], Accuracy: 82.03, Test time: 49.10\n",
      "Epoch [1/1], Step [44600/90866], Accuracy: 82.04, Test time: 49.20\n",
      "Epoch [1/1], Step [44700/90866], Accuracy: 82.02, Test time: 49.31\n",
      "Epoch [1/1], Step [44800/90866], Accuracy: 82.02, Test time: 49.40\n",
      "Epoch [1/1], Step [44900/90866], Accuracy: 82.01, Test time: 49.49\n",
      "Epoch [1/1], Step [45000/90866], Accuracy: 82.00, Test time: 49.57\n",
      "Epoch [1/1], Step [45100/90866], Accuracy: 81.98, Test time: 49.66\n",
      "Epoch [1/1], Step [45200/90866], Accuracy: 81.98, Test time: 49.82\n",
      "Epoch [1/1], Step [45300/90866], Accuracy: 81.98, Test time: 49.97\n",
      "Epoch [1/1], Step [45400/90866], Accuracy: 81.96, Test time: 50.12\n",
      "Epoch [1/1], Step [45500/90866], Accuracy: 81.97, Test time: 50.21\n",
      "Epoch [1/1], Step [45600/90866], Accuracy: 81.98, Test time: 50.38\n",
      "Epoch [1/1], Step [45700/90866], Accuracy: 81.97, Test time: 50.47\n",
      "Epoch [1/1], Step [45800/90866], Accuracy: 81.96, Test time: 50.56\n",
      "Epoch [1/1], Step [45900/90866], Accuracy: 81.96, Test time: 50.62\n",
      "Epoch [1/1], Step [46000/90866], Accuracy: 81.96, Test time: 50.79\n",
      "Epoch [1/1], Step [46100/90866], Accuracy: 81.96, Test time: 50.87\n",
      "Epoch [1/1], Step [46200/90866], Accuracy: 81.97, Test time: 51.02\n",
      "Epoch [1/1], Step [46300/90866], Accuracy: 81.97, Test time: 51.07\n",
      "Epoch [1/1], Step [46400/90866], Accuracy: 81.98, Test time: 51.19\n",
      "Epoch [1/1], Step [46500/90866], Accuracy: 81.98, Test time: 51.29\n",
      "Epoch [1/1], Step [46600/90866], Accuracy: 81.98, Test time: 51.39\n",
      "Epoch [1/1], Step [46700/90866], Accuracy: 81.99, Test time: 51.50\n",
      "Epoch [1/1], Step [46800/90866], Accuracy: 82.00, Test time: 51.61\n",
      "Epoch [1/1], Step [46900/90866], Accuracy: 81.98, Test time: 51.69\n",
      "Epoch [1/1], Step [47000/90866], Accuracy: 81.98, Test time: 51.78\n",
      "Epoch [1/1], Step [47100/90866], Accuracy: 81.99, Test time: 51.93\n",
      "Epoch [1/1], Step [47200/90866], Accuracy: 82.00, Test time: 52.04\n",
      "Epoch [1/1], Step [47300/90866], Accuracy: 82.00, Test time: 52.13\n",
      "Epoch [1/1], Step [47400/90866], Accuracy: 82.01, Test time: 52.28\n",
      "Epoch [1/1], Step [47500/90866], Accuracy: 82.01, Test time: 52.51\n",
      "Epoch [1/1], Step [47600/90866], Accuracy: 82.01, Test time: 52.61\n",
      "Epoch [1/1], Step [47700/90866], Accuracy: 82.00, Test time: 52.73\n",
      "Epoch [1/1], Step [47800/90866], Accuracy: 81.99, Test time: 52.82\n",
      "Epoch [1/1], Step [47900/90866], Accuracy: 81.97, Test time: 52.90\n",
      "Epoch [1/1], Step [48000/90866], Accuracy: 81.98, Test time: 53.03\n",
      "Epoch [1/1], Step [48100/90866], Accuracy: 81.99, Test time: 53.13\n",
      "Epoch [1/1], Step [48200/90866], Accuracy: 82.00, Test time: 53.32\n",
      "Epoch [1/1], Step [48300/90866], Accuracy: 82.00, Test time: 53.43\n",
      "Epoch [1/1], Step [48400/90866], Accuracy: 81.99, Test time: 53.49\n",
      "Epoch [1/1], Step [48500/90866], Accuracy: 81.98, Test time: 53.73\n",
      "Epoch [1/1], Step [48600/90866], Accuracy: 81.99, Test time: 53.83\n",
      "Epoch [1/1], Step [48700/90866], Accuracy: 81.98, Test time: 53.91\n",
      "Epoch [1/1], Step [48800/90866], Accuracy: 82.00, Test time: 54.02\n",
      "Epoch [1/1], Step [48900/90866], Accuracy: 82.00, Test time: 54.10\n",
      "Epoch [1/1], Step [49000/90866], Accuracy: 82.00, Test time: 54.23\n",
      "Epoch [1/1], Step [49100/90866], Accuracy: 81.99, Test time: 54.31\n",
      "Epoch [1/1], Step [49200/90866], Accuracy: 82.00, Test time: 54.47\n",
      "Epoch [1/1], Step [49300/90866], Accuracy: 81.99, Test time: 54.56\n",
      "Epoch [1/1], Step [49400/90866], Accuracy: 82.00, Test time: 54.64\n",
      "Epoch [1/1], Step [49500/90866], Accuracy: 82.00, Test time: 54.82\n",
      "Epoch [1/1], Step [49600/90866], Accuracy: 82.01, Test time: 55.03\n",
      "Epoch [1/1], Step [49700/90866], Accuracy: 82.01, Test time: 55.13\n",
      "Epoch [1/1], Step [49800/90866], Accuracy: 82.00, Test time: 55.26\n",
      "Epoch [1/1], Step [49900/90866], Accuracy: 82.00, Test time: 55.33\n",
      "Epoch [1/1], Step [50000/90866], Accuracy: 82.00, Test time: 55.45\n",
      "Epoch [1/1], Step [50100/90866], Accuracy: 82.00, Test time: 55.56\n",
      "Epoch [1/1], Step [50200/90866], Accuracy: 82.00, Test time: 55.66\n",
      "Epoch [1/1], Step [50300/90866], Accuracy: 82.01, Test time: 55.80\n",
      "Epoch [1/1], Step [50400/90866], Accuracy: 82.01, Test time: 55.92\n",
      "Epoch [1/1], Step [50500/90866], Accuracy: 82.02, Test time: 56.04\n",
      "Epoch [1/1], Step [50600/90866], Accuracy: 82.02, Test time: 56.14\n",
      "Epoch [1/1], Step [50700/90866], Accuracy: 82.03, Test time: 56.24\n",
      "Epoch [1/1], Step [50800/90866], Accuracy: 82.03, Test time: 56.42\n",
      "Epoch [1/1], Step [50900/90866], Accuracy: 82.02, Test time: 56.49\n",
      "Epoch [1/1], Step [51000/90866], Accuracy: 82.02, Test time: 56.61\n",
      "Epoch [1/1], Step [51100/90866], Accuracy: 82.02, Test time: 56.73\n",
      "Epoch [1/1], Step [51200/90866], Accuracy: 82.02, Test time: 56.82\n",
      "Epoch [1/1], Step [51300/90866], Accuracy: 82.02, Test time: 56.94\n",
      "Epoch [1/1], Step [51400/90866], Accuracy: 82.02, Test time: 57.07\n",
      "Epoch [1/1], Step [51500/90866], Accuracy: 82.02, Test time: 57.17\n",
      "Epoch [1/1], Step [51600/90866], Accuracy: 82.02, Test time: 57.30\n",
      "Epoch [1/1], Step [51700/90866], Accuracy: 82.02, Test time: 57.41\n",
      "Epoch [1/1], Step [51800/90866], Accuracy: 82.02, Test time: 57.53\n",
      "Epoch [1/1], Step [51900/90866], Accuracy: 82.03, Test time: 57.69\n",
      "Epoch [1/1], Step [52000/90866], Accuracy: 82.04, Test time: 57.78\n",
      "Epoch [1/1], Step [52100/90866], Accuracy: 82.02, Test time: 57.85\n",
      "Epoch [1/1], Step [52200/90866], Accuracy: 82.01, Test time: 57.95\n",
      "Epoch [1/1], Step [52300/90866], Accuracy: 82.01, Test time: 58.08\n",
      "Epoch [1/1], Step [52400/90866], Accuracy: 82.01, Test time: 58.16\n",
      "Epoch [1/1], Step [52500/90866], Accuracy: 82.00, Test time: 58.27\n",
      "Epoch [1/1], Step [52600/90866], Accuracy: 81.99, Test time: 58.38\n",
      "Epoch [1/1], Step [52700/90866], Accuracy: 82.00, Test time: 58.47\n",
      "Epoch [1/1], Step [52800/90866], Accuracy: 82.00, Test time: 58.61\n",
      "Epoch [1/1], Step [52900/90866], Accuracy: 81.99, Test time: 58.68\n",
      "Epoch [1/1], Step [53000/90866], Accuracy: 81.98, Test time: 58.82\n",
      "Epoch [1/1], Step [53100/90866], Accuracy: 81.99, Test time: 58.92\n",
      "Epoch [1/1], Step [53200/90866], Accuracy: 81.99, Test time: 59.03\n",
      "Epoch [1/1], Step [53300/90866], Accuracy: 81.99, Test time: 59.14\n",
      "Epoch [1/1], Step [53400/90866], Accuracy: 81.99, Test time: 59.21\n",
      "Epoch [1/1], Step [53500/90866], Accuracy: 82.00, Test time: 59.37\n",
      "Epoch [1/1], Step [53600/90866], Accuracy: 82.00, Test time: 59.51\n",
      "Epoch [1/1], Step [53700/90866], Accuracy: 82.00, Test time: 59.57\n",
      "Epoch [1/1], Step [53800/90866], Accuracy: 82.01, Test time: 59.97\n",
      "Epoch [1/1], Step [53900/90866], Accuracy: 82.02, Test time: 60.04\n",
      "Epoch [1/1], Step [54000/90866], Accuracy: 82.01, Test time: 60.11\n",
      "Epoch [1/1], Step [54100/90866], Accuracy: 82.00, Test time: 60.20\n",
      "Epoch [1/1], Step [54200/90866], Accuracy: 82.01, Test time: 60.36\n",
      "Epoch [1/1], Step [54300/90866], Accuracy: 82.01, Test time: 60.45\n",
      "Epoch [1/1], Step [54400/90866], Accuracy: 81.99, Test time: 60.56\n",
      "Epoch [1/1], Step [54500/90866], Accuracy: 81.99, Test time: 60.67\n",
      "Epoch [1/1], Step [54600/90866], Accuracy: 81.98, Test time: 60.77\n",
      "Epoch [1/1], Step [54700/90866], Accuracy: 81.99, Test time: 60.92\n",
      "Epoch [1/1], Step [54800/90866], Accuracy: 81.97, Test time: 60.98\n",
      "Epoch [1/1], Step [54900/90866], Accuracy: 81.98, Test time: 61.10\n",
      "Epoch [1/1], Step [55000/90866], Accuracy: 81.98, Test time: 61.19\n",
      "Epoch [1/1], Step [55100/90866], Accuracy: 81.99, Test time: 61.41\n",
      "Epoch [1/1], Step [55200/90866], Accuracy: 81.98, Test time: 61.50\n",
      "Epoch [1/1], Step [55300/90866], Accuracy: 81.99, Test time: 61.57\n",
      "Epoch [1/1], Step [55400/90866], Accuracy: 81.98, Test time: 61.72\n",
      "Epoch [1/1], Step [55500/90866], Accuracy: 81.98, Test time: 61.79\n",
      "Epoch [1/1], Step [55600/90866], Accuracy: 81.98, Test time: 61.90\n",
      "Epoch [1/1], Step [55700/90866], Accuracy: 81.98, Test time: 61.96\n",
      "Epoch [1/1], Step [55800/90866], Accuracy: 81.97, Test time: 62.18\n",
      "Epoch [1/1], Step [55900/90866], Accuracy: 81.98, Test time: 62.29\n",
      "Epoch [1/1], Step [56000/90866], Accuracy: 81.99, Test time: 62.38\n",
      "Epoch [1/1], Step [56100/90866], Accuracy: 81.99, Test time: 62.54\n",
      "Epoch [1/1], Step [56200/90866], Accuracy: 81.98, Test time: 62.64\n",
      "Epoch [1/1], Step [56300/90866], Accuracy: 81.99, Test time: 62.72\n",
      "Epoch [1/1], Step [56400/90866], Accuracy: 81.98, Test time: 62.81\n",
      "Epoch [1/1], Step [56500/90866], Accuracy: 81.99, Test time: 62.99\n",
      "Epoch [1/1], Step [56600/90866], Accuracy: 82.00, Test time: 63.07\n",
      "Epoch [1/1], Step [56700/90866], Accuracy: 81.99, Test time: 63.14\n",
      "Epoch [1/1], Step [56800/90866], Accuracy: 81.98, Test time: 63.23\n",
      "Epoch [1/1], Step [56900/90866], Accuracy: 81.98, Test time: 63.38\n",
      "Epoch [1/1], Step [57000/90866], Accuracy: 81.98, Test time: 63.51\n",
      "Epoch [1/1], Step [57100/90866], Accuracy: 81.98, Test time: 63.60\n",
      "Epoch [1/1], Step [57200/90866], Accuracy: 81.99, Test time: 63.72\n",
      "Epoch [1/1], Step [57300/90866], Accuracy: 81.99, Test time: 63.84\n",
      "Epoch [1/1], Step [57400/90866], Accuracy: 81.98, Test time: 63.93\n",
      "Epoch [1/1], Step [57500/90866], Accuracy: 81.99, Test time: 64.02\n",
      "Epoch [1/1], Step [57600/90866], Accuracy: 81.98, Test time: 64.20\n",
      "Epoch [1/1], Step [57700/90866], Accuracy: 81.97, Test time: 64.25\n",
      "Epoch [1/1], Step [57800/90866], Accuracy: 81.98, Test time: 64.34\n",
      "Epoch [1/1], Step [57900/90866], Accuracy: 81.97, Test time: 64.47\n",
      "Epoch [1/1], Step [58000/90866], Accuracy: 81.97, Test time: 64.56\n",
      "Epoch [1/1], Step [58100/90866], Accuracy: 81.98, Test time: 64.67\n",
      "Epoch [1/1], Step [58200/90866], Accuracy: 81.98, Test time: 64.78\n",
      "Epoch [1/1], Step [58300/90866], Accuracy: 81.98, Test time: 64.88\n",
      "Epoch [1/1], Step [58400/90866], Accuracy: 81.98, Test time: 65.00\n",
      "Epoch [1/1], Step [58500/90866], Accuracy: 81.97, Test time: 65.12\n",
      "Epoch [1/1], Step [58600/90866], Accuracy: 81.97, Test time: 65.20\n",
      "Epoch [1/1], Step [58700/90866], Accuracy: 81.96, Test time: 65.50\n",
      "Epoch [1/1], Step [58800/90866], Accuracy: 81.97, Test time: 65.65\n",
      "Epoch [1/1], Step [58900/90866], Accuracy: 81.97, Test time: 65.76\n",
      "Epoch [1/1], Step [59000/90866], Accuracy: 81.97, Test time: 65.89\n",
      "Epoch [1/1], Step [59100/90866], Accuracy: 81.98, Test time: 65.99\n",
      "Epoch [1/1], Step [59200/90866], Accuracy: 81.97, Test time: 66.11\n",
      "Epoch [1/1], Step [59300/90866], Accuracy: 81.97, Test time: 66.21\n",
      "Epoch [1/1], Step [59400/90866], Accuracy: 81.97, Test time: 66.32\n",
      "Epoch [1/1], Step [59500/90866], Accuracy: 81.98, Test time: 66.39\n",
      "Epoch [1/1], Step [59600/90866], Accuracy: 81.97, Test time: 66.46\n",
      "Epoch [1/1], Step [59700/90866], Accuracy: 81.96, Test time: 66.60\n",
      "Epoch [1/1], Step [59800/90866], Accuracy: 81.97, Test time: 66.69\n",
      "Epoch [1/1], Step [59900/90866], Accuracy: 81.97, Test time: 66.80\n",
      "Epoch [1/1], Step [60000/90866], Accuracy: 81.98, Test time: 66.96\n",
      "Epoch [1/1], Step [60100/90866], Accuracy: 81.98, Test time: 67.07\n",
      "Epoch [1/1], Step [60200/90866], Accuracy: 81.97, Test time: 67.18\n",
      "Epoch [1/1], Step [60300/90866], Accuracy: 81.98, Test time: 67.32\n",
      "Epoch [1/1], Step [60400/90866], Accuracy: 81.97, Test time: 67.44\n",
      "Epoch [1/1], Step [60500/90866], Accuracy: 81.96, Test time: 67.61\n",
      "Epoch [1/1], Step [60600/90866], Accuracy: 81.97, Test time: 67.66\n",
      "Epoch [1/1], Step [60700/90866], Accuracy: 81.97, Test time: 67.74\n",
      "Epoch [1/1], Step [60800/90866], Accuracy: 81.97, Test time: 67.86\n",
      "Epoch [1/1], Step [60900/90866], Accuracy: 81.97, Test time: 68.01\n",
      "Epoch [1/1], Step [61000/90866], Accuracy: 81.98, Test time: 68.24\n",
      "Epoch [1/1], Step [61100/90866], Accuracy: 81.98, Test time: 68.29\n",
      "Epoch [1/1], Step [61200/90866], Accuracy: 81.99, Test time: 68.36\n",
      "Epoch [1/1], Step [61300/90866], Accuracy: 81.99, Test time: 68.46\n",
      "Epoch [1/1], Step [61400/90866], Accuracy: 81.98, Test time: 68.59\n",
      "Epoch [1/1], Step [61500/90866], Accuracy: 81.99, Test time: 68.68\n",
      "Epoch [1/1], Step [61600/90866], Accuracy: 81.99, Test time: 68.78\n",
      "Epoch [1/1], Step [61700/90866], Accuracy: 81.99, Test time: 68.95\n",
      "Epoch [1/1], Step [61800/90866], Accuracy: 81.99, Test time: 69.03\n",
      "Epoch [1/1], Step [61900/90866], Accuracy: 82.00, Test time: 69.17\n",
      "Epoch [1/1], Step [62000/90866], Accuracy: 82.00, Test time: 69.28\n",
      "Epoch [1/1], Step [62100/90866], Accuracy: 82.00, Test time: 69.39\n",
      "Epoch [1/1], Step [62200/90866], Accuracy: 82.01, Test time: 69.49\n",
      "Epoch [1/1], Step [62300/90866], Accuracy: 82.01, Test time: 69.64\n",
      "Epoch [1/1], Step [62400/90866], Accuracy: 82.02, Test time: 69.74\n",
      "Epoch [1/1], Step [62500/90866], Accuracy: 82.02, Test time: 69.80\n",
      "Epoch [1/1], Step [62600/90866], Accuracy: 82.02, Test time: 69.91\n",
      "Epoch [1/1], Step [62700/90866], Accuracy: 82.03, Test time: 70.02\n",
      "Epoch [1/1], Step [62800/90866], Accuracy: 82.03, Test time: 70.15\n",
      "Epoch [1/1], Step [62900/90866], Accuracy: 82.02, Test time: 70.27\n",
      "Epoch [1/1], Step [63000/90866], Accuracy: 82.02, Test time: 70.35\n",
      "Epoch [1/1], Step [63100/90866], Accuracy: 82.02, Test time: 70.46\n",
      "Epoch [1/1], Step [63200/90866], Accuracy: 82.01, Test time: 70.59\n",
      "Epoch [1/1], Step [63300/90866], Accuracy: 82.01, Test time: 70.65\n",
      "Epoch [1/1], Step [63400/90866], Accuracy: 82.01, Test time: 70.78\n",
      "Epoch [1/1], Step [63500/90866], Accuracy: 82.01, Test time: 70.86\n",
      "Epoch [1/1], Step [63600/90866], Accuracy: 82.02, Test time: 70.98\n",
      "Epoch [1/1], Step [63700/90866], Accuracy: 82.02, Test time: 71.09\n",
      "Epoch [1/1], Step [63800/90866], Accuracy: 82.01, Test time: 71.18\n",
      "Epoch [1/1], Step [63900/90866], Accuracy: 82.01, Test time: 71.32\n",
      "Epoch [1/1], Step [64000/90866], Accuracy: 82.02, Test time: 71.41\n",
      "Epoch [1/1], Step [64100/90866], Accuracy: 82.02, Test time: 71.50\n",
      "Epoch [1/1], Step [64200/90866], Accuracy: 82.02, Test time: 71.59\n",
      "Epoch [1/1], Step [64300/90866], Accuracy: 82.02, Test time: 71.82\n",
      "Epoch [1/1], Step [64400/90866], Accuracy: 82.02, Test time: 71.89\n",
      "Epoch [1/1], Step [64500/90866], Accuracy: 82.02, Test time: 71.95\n",
      "Epoch [1/1], Step [64600/90866], Accuracy: 82.03, Test time: 72.07\n",
      "Epoch [1/1], Step [64700/90866], Accuracy: 82.03, Test time: 72.16\n",
      "Epoch [1/1], Step [64800/90866], Accuracy: 82.03, Test time: 72.29\n",
      "Epoch [1/1], Step [64900/90866], Accuracy: 82.02, Test time: 72.35\n",
      "Epoch [1/1], Step [65000/90866], Accuracy: 82.03, Test time: 72.52\n",
      "Epoch [1/1], Step [65100/90866], Accuracy: 82.03, Test time: 72.58\n",
      "Epoch [1/1], Step [65200/90866], Accuracy: 82.02, Test time: 72.69\n",
      "Epoch [1/1], Step [65300/90866], Accuracy: 82.02, Test time: 72.80\n",
      "Epoch [1/1], Step [65400/90866], Accuracy: 82.03, Test time: 72.92\n",
      "Epoch [1/1], Step [65500/90866], Accuracy: 82.03, Test time: 73.02\n",
      "Epoch [1/1], Step [65600/90866], Accuracy: 82.03, Test time: 73.14\n",
      "Epoch [1/1], Step [65700/90866], Accuracy: 82.03, Test time: 73.29\n",
      "Epoch [1/1], Step [65800/90866], Accuracy: 82.03, Test time: 73.40\n",
      "Epoch [1/1], Step [65900/90866], Accuracy: 82.03, Test time: 73.51\n",
      "Epoch [1/1], Step [66000/90866], Accuracy: 82.02, Test time: 73.64\n",
      "Epoch [1/1], Step [66100/90866], Accuracy: 82.03, Test time: 73.79\n",
      "Epoch [1/1], Step [66200/90866], Accuracy: 82.03, Test time: 73.91\n",
      "Epoch [1/1], Step [66300/90866], Accuracy: 82.01, Test time: 73.98\n",
      "Epoch [1/1], Step [66400/90866], Accuracy: 82.02, Test time: 74.13\n",
      "Epoch [1/1], Step [66500/90866], Accuracy: 82.02, Test time: 74.23\n",
      "Epoch [1/1], Step [66600/90866], Accuracy: 82.02, Test time: 74.37\n",
      "Epoch [1/1], Step [66700/90866], Accuracy: 82.02, Test time: 74.57\n",
      "Epoch [1/1], Step [66800/90866], Accuracy: 82.02, Test time: 74.64\n",
      "Epoch [1/1], Step [66900/90866], Accuracy: 82.02, Test time: 74.83\n",
      "Epoch [1/1], Step [67000/90866], Accuracy: 82.03, Test time: 74.90\n",
      "Epoch [1/1], Step [67100/90866], Accuracy: 82.02, Test time: 75.09\n",
      "Epoch [1/1], Step [67200/90866], Accuracy: 82.02, Test time: 75.21\n",
      "Epoch [1/1], Step [67300/90866], Accuracy: 82.03, Test time: 75.49\n",
      "Epoch [1/1], Step [67400/90866], Accuracy: 82.03, Test time: 75.59\n",
      "Epoch [1/1], Step [67500/90866], Accuracy: 82.03, Test time: 75.67\n",
      "Epoch [1/1], Step [67600/90866], Accuracy: 82.03, Test time: 75.82\n",
      "Epoch [1/1], Step [67700/90866], Accuracy: 82.01, Test time: 75.90\n",
      "Epoch [1/1], Step [67800/90866], Accuracy: 82.01, Test time: 76.00\n",
      "Epoch [1/1], Step [67900/90866], Accuracy: 82.01, Test time: 76.09\n",
      "Epoch [1/1], Step [68000/90866], Accuracy: 82.01, Test time: 76.20\n",
      "Epoch [1/1], Step [68100/90866], Accuracy: 82.00, Test time: 76.31\n",
      "Epoch [1/1], Step [68200/90866], Accuracy: 82.00, Test time: 76.40\n",
      "Epoch [1/1], Step [68300/90866], Accuracy: 82.01, Test time: 76.51\n",
      "Epoch [1/1], Step [68400/90866], Accuracy: 82.01, Test time: 76.59\n",
      "Epoch [1/1], Step [68500/90866], Accuracy: 82.01, Test time: 76.70\n",
      "Epoch [1/1], Step [68600/90866], Accuracy: 82.01, Test time: 76.81\n",
      "Epoch [1/1], Step [68700/90866], Accuracy: 82.01, Test time: 76.90\n",
      "Epoch [1/1], Step [68800/90866], Accuracy: 82.02, Test time: 76.99\n",
      "Epoch [1/1], Step [68900/90866], Accuracy: 82.03, Test time: 77.09\n",
      "Epoch [1/1], Step [69000/90866], Accuracy: 82.03, Test time: 77.18\n",
      "Epoch [1/1], Step [69100/90866], Accuracy: 82.03, Test time: 77.30\n",
      "Epoch [1/1], Step [69200/90866], Accuracy: 82.03, Test time: 77.44\n",
      "Epoch [1/1], Step [69300/90866], Accuracy: 82.02, Test time: 77.52\n",
      "Epoch [1/1], Step [69400/90866], Accuracy: 82.02, Test time: 77.61\n",
      "Epoch [1/1], Step [69500/90866], Accuracy: 82.02, Test time: 77.71\n",
      "Epoch [1/1], Step [69600/90866], Accuracy: 82.02, Test time: 77.88\n",
      "Epoch [1/1], Step [69700/90866], Accuracy: 82.02, Test time: 77.97\n",
      "Epoch [1/1], Step [69800/90866], Accuracy: 82.02, Test time: 78.06\n",
      "Epoch [1/1], Step [69900/90866], Accuracy: 82.02, Test time: 78.15\n",
      "Epoch [1/1], Step [70000/90866], Accuracy: 82.03, Test time: 78.31\n",
      "Epoch [1/1], Step [70100/90866], Accuracy: 82.02, Test time: 78.40\n",
      "Epoch [1/1], Step [70200/90866], Accuracy: 82.03, Test time: 78.65\n",
      "Epoch [1/1], Step [70300/90866], Accuracy: 82.02, Test time: 78.76\n",
      "Epoch [1/1], Step [70400/90866], Accuracy: 82.02, Test time: 78.83\n",
      "Epoch [1/1], Step [70500/90866], Accuracy: 82.01, Test time: 78.97\n",
      "Epoch [1/1], Step [70600/90866], Accuracy: 82.01, Test time: 79.07\n",
      "Epoch [1/1], Step [70700/90866], Accuracy: 82.01, Test time: 79.25\n",
      "Epoch [1/1], Step [70800/90866], Accuracy: 82.01, Test time: 79.34\n",
      "Epoch [1/1], Step [70900/90866], Accuracy: 82.01, Test time: 79.46\n",
      "Epoch [1/1], Step [71000/90866], Accuracy: 82.01, Test time: 79.52\n",
      "Epoch [1/1], Step [71100/90866], Accuracy: 82.01, Test time: 79.62\n",
      "Epoch [1/1], Step [71200/90866], Accuracy: 82.02, Test time: 79.78\n",
      "Epoch [1/1], Step [71300/90866], Accuracy: 82.02, Test time: 79.85\n",
      "Epoch [1/1], Step [71400/90866], Accuracy: 82.02, Test time: 79.95\n",
      "Epoch [1/1], Step [71500/90866], Accuracy: 82.02, Test time: 80.06\n",
      "Epoch [1/1], Step [71600/90866], Accuracy: 82.03, Test time: 80.17\n",
      "Epoch [1/1], Step [71700/90866], Accuracy: 82.02, Test time: 80.27\n",
      "Epoch [1/1], Step [71800/90866], Accuracy: 82.03, Test time: 80.50\n",
      "Epoch [1/1], Step [71900/90866], Accuracy: 82.03, Test time: 80.68\n",
      "Epoch [1/1], Step [72000/90866], Accuracy: 82.03, Test time: 80.81\n",
      "Epoch [1/1], Step [72100/90866], Accuracy: 82.03, Test time: 81.11\n",
      "Epoch [1/1], Step [72200/90866], Accuracy: 82.04, Test time: 81.21\n",
      "Epoch [1/1], Step [72300/90866], Accuracy: 82.05, Test time: 81.34\n",
      "Epoch [1/1], Step [72400/90866], Accuracy: 82.04, Test time: 81.48\n",
      "Epoch [1/1], Step [72500/90866], Accuracy: 82.05, Test time: 81.54\n",
      "Epoch [1/1], Step [72600/90866], Accuracy: 82.05, Test time: 81.65\n",
      "Epoch [1/1], Step [72700/90866], Accuracy: 82.04, Test time: 81.71\n",
      "Epoch [1/1], Step [72800/90866], Accuracy: 82.05, Test time: 81.83\n",
      "Epoch [1/1], Step [72900/90866], Accuracy: 82.05, Test time: 81.91\n",
      "Epoch [1/1], Step [73000/90866], Accuracy: 82.05, Test time: 82.19\n",
      "Epoch [1/1], Step [73100/90866], Accuracy: 82.05, Test time: 82.27\n",
      "Epoch [1/1], Step [73200/90866], Accuracy: 82.05, Test time: 82.41\n",
      "Epoch [1/1], Step [73300/90866], Accuracy: 82.03, Test time: 82.54\n",
      "Epoch [1/1], Step [73400/90866], Accuracy: 82.03, Test time: 82.69\n",
      "Epoch [1/1], Step [73500/90866], Accuracy: 82.02, Test time: 82.80\n",
      "Epoch [1/1], Step [73600/90866], Accuracy: 82.02, Test time: 82.89\n",
      "Epoch [1/1], Step [73700/90866], Accuracy: 82.01, Test time: 83.11\n",
      "Epoch [1/1], Step [73800/90866], Accuracy: 82.01, Test time: 83.19\n",
      "Epoch [1/1], Step [73900/90866], Accuracy: 82.01, Test time: 83.27\n",
      "Epoch [1/1], Step [74000/90866], Accuracy: 82.01, Test time: 83.36\n",
      "Epoch [1/1], Step [74100/90866], Accuracy: 82.01, Test time: 83.46\n",
      "Epoch [1/1], Step [74200/90866], Accuracy: 82.01, Test time: 83.54\n",
      "Epoch [1/1], Step [74300/90866], Accuracy: 82.02, Test time: 83.68\n",
      "Epoch [1/1], Step [74400/90866], Accuracy: 82.01, Test time: 83.77\n",
      "Epoch [1/1], Step [74500/90866], Accuracy: 82.01, Test time: 83.86\n",
      "Epoch [1/1], Step [74600/90866], Accuracy: 82.01, Test time: 83.94\n",
      "Epoch [1/1], Step [74700/90866], Accuracy: 82.01, Test time: 84.04\n",
      "Epoch [1/1], Step [74800/90866], Accuracy: 82.02, Test time: 84.15\n",
      "Epoch [1/1], Step [74900/90866], Accuracy: 82.01, Test time: 84.26\n",
      "Epoch [1/1], Step [75000/90866], Accuracy: 82.01, Test time: 84.34\n",
      "Epoch [1/1], Step [75100/90866], Accuracy: 82.00, Test time: 84.44\n",
      "Epoch [1/1], Step [75200/90866], Accuracy: 82.01, Test time: 84.56\n",
      "Epoch [1/1], Step [75300/90866], Accuracy: 82.00, Test time: 84.65\n",
      "Epoch [1/1], Step [75400/90866], Accuracy: 82.00, Test time: 84.72\n",
      "Epoch [1/1], Step [75500/90866], Accuracy: 82.00, Test time: 84.83\n",
      "Epoch [1/1], Step [75600/90866], Accuracy: 81.99, Test time: 84.95\n",
      "Epoch [1/1], Step [75700/90866], Accuracy: 82.00, Test time: 85.03\n",
      "Epoch [1/1], Step [75800/90866], Accuracy: 81.99, Test time: 85.24\n",
      "Epoch [1/1], Step [75900/90866], Accuracy: 81.99, Test time: 85.34\n",
      "Epoch [1/1], Step [76000/90866], Accuracy: 81.98, Test time: 85.48\n",
      "Epoch [1/1], Step [76100/90866], Accuracy: 81.98, Test time: 85.54\n",
      "Epoch [1/1], Step [76200/90866], Accuracy: 81.97, Test time: 85.61\n",
      "Epoch [1/1], Step [76300/90866], Accuracy: 81.97, Test time: 85.76\n",
      "Epoch [1/1], Step [76400/90866], Accuracy: 81.98, Test time: 85.86\n",
      "Epoch [1/1], Step [76500/90866], Accuracy: 81.98, Test time: 85.92\n",
      "Epoch [1/1], Step [76600/90866], Accuracy: 81.98, Test time: 86.05\n",
      "Epoch [1/1], Step [76700/90866], Accuracy: 81.97, Test time: 86.24\n",
      "Epoch [1/1], Step [76800/90866], Accuracy: 81.97, Test time: 86.31\n",
      "Epoch [1/1], Step [76900/90866], Accuracy: 81.96, Test time: 86.36\n",
      "Epoch [1/1], Step [77000/90866], Accuracy: 81.96, Test time: 86.48\n",
      "Epoch [1/1], Step [77100/90866], Accuracy: 81.96, Test time: 86.56\n",
      "Epoch [1/1], Step [77200/90866], Accuracy: 81.97, Test time: 86.68\n",
      "Epoch [1/1], Step [77300/90866], Accuracy: 81.96, Test time: 86.74\n",
      "Epoch [1/1], Step [77400/90866], Accuracy: 81.96, Test time: 86.87\n",
      "Epoch [1/1], Step [77500/90866], Accuracy: 81.96, Test time: 87.00\n",
      "Epoch [1/1], Step [77600/90866], Accuracy: 81.97, Test time: 87.10\n",
      "Epoch [1/1], Step [77700/90866], Accuracy: 81.98, Test time: 87.16\n",
      "Epoch [1/1], Step [77800/90866], Accuracy: 81.98, Test time: 87.30\n",
      "Epoch [1/1], Step [77900/90866], Accuracy: 81.98, Test time: 87.47\n",
      "Epoch [1/1], Step [78000/90866], Accuracy: 81.98, Test time: 87.56\n",
      "Epoch [1/1], Step [78100/90866], Accuracy: 81.98, Test time: 87.68\n",
      "Epoch [1/1], Step [78200/90866], Accuracy: 81.98, Test time: 87.75\n",
      "Epoch [1/1], Step [78300/90866], Accuracy: 81.98, Test time: 87.90\n",
      "Epoch [1/1], Step [78400/90866], Accuracy: 81.97, Test time: 87.98\n",
      "Epoch [1/1], Step [78500/90866], Accuracy: 81.98, Test time: 88.09\n",
      "Epoch [1/1], Step [78600/90866], Accuracy: 81.98, Test time: 88.19\n",
      "Epoch [1/1], Step [78700/90866], Accuracy: 81.99, Test time: 88.31\n",
      "Epoch [1/1], Step [78800/90866], Accuracy: 81.99, Test time: 88.41\n",
      "Epoch [1/1], Step [78900/90866], Accuracy: 81.99, Test time: 88.46\n",
      "Epoch [1/1], Step [79000/90866], Accuracy: 81.99, Test time: 88.56\n",
      "Epoch [1/1], Step [79100/90866], Accuracy: 81.99, Test time: 88.70\n",
      "Epoch [1/1], Step [79200/90866], Accuracy: 81.99, Test time: 88.82\n",
      "Epoch [1/1], Step [79300/90866], Accuracy: 81.99, Test time: 88.91\n",
      "Epoch [1/1], Step [79400/90866], Accuracy: 81.99, Test time: 89.04\n",
      "Epoch [1/1], Step [79500/90866], Accuracy: 81.99, Test time: 89.13\n",
      "Epoch [1/1], Step [79600/90866], Accuracy: 82.00, Test time: 89.23\n",
      "Epoch [1/1], Step [79700/90866], Accuracy: 82.00, Test time: 89.30\n",
      "Epoch [1/1], Step [79800/90866], Accuracy: 82.00, Test time: 89.41\n",
      "Epoch [1/1], Step [79900/90866], Accuracy: 82.01, Test time: 89.57\n",
      "Epoch [1/1], Step [80000/90866], Accuracy: 82.01, Test time: 89.65\n",
      "Epoch [1/1], Step [80100/90866], Accuracy: 82.01, Test time: 89.83\n",
      "Epoch [1/1], Step [80200/90866], Accuracy: 82.01, Test time: 89.95\n",
      "Epoch [1/1], Step [80300/90866], Accuracy: 82.02, Test time: 90.05\n",
      "Epoch [1/1], Step [80400/90866], Accuracy: 82.02, Test time: 90.19\n",
      "Epoch [1/1], Step [80500/90866], Accuracy: 82.02, Test time: 90.25\n",
      "Epoch [1/1], Step [80600/90866], Accuracy: 82.02, Test time: 90.40\n",
      "Epoch [1/1], Step [80700/90866], Accuracy: 82.02, Test time: 90.50\n",
      "Epoch [1/1], Step [80800/90866], Accuracy: 82.03, Test time: 90.55\n",
      "Epoch [1/1], Step [80900/90866], Accuracy: 82.02, Test time: 90.73\n",
      "Epoch [1/1], Step [81000/90866], Accuracy: 82.02, Test time: 90.88\n",
      "Epoch [1/1], Step [81100/90866], Accuracy: 82.02, Test time: 90.95\n",
      "Epoch [1/1], Step [81200/90866], Accuracy: 82.01, Test time: 91.05\n",
      "Epoch [1/1], Step [81300/90866], Accuracy: 82.00, Test time: 91.13\n",
      "Epoch [1/1], Step [81400/90866], Accuracy: 82.00, Test time: 91.26\n",
      "Epoch [1/1], Step [81500/90866], Accuracy: 82.00, Test time: 91.39\n",
      "Epoch [1/1], Step [81600/90866], Accuracy: 82.00, Test time: 91.45\n",
      "Epoch [1/1], Step [81700/90866], Accuracy: 82.01, Test time: 91.63\n",
      "Epoch [1/1], Step [81800/90866], Accuracy: 82.00, Test time: 91.78\n",
      "Epoch [1/1], Step [81900/90866], Accuracy: 82.00, Test time: 91.83\n",
      "Epoch [1/1], Step [82000/90866], Accuracy: 82.01, Test time: 91.93\n",
      "Epoch [1/1], Step [82100/90866], Accuracy: 82.01, Test time: 92.09\n",
      "Epoch [1/1], Step [82200/90866], Accuracy: 82.01, Test time: 92.17\n",
      "Epoch [1/1], Step [82300/90866], Accuracy: 82.00, Test time: 92.27\n",
      "Epoch [1/1], Step [82400/90866], Accuracy: 82.00, Test time: 92.37\n",
      "Epoch [1/1], Step [82500/90866], Accuracy: 82.00, Test time: 92.54\n",
      "Epoch [1/1], Step [82600/90866], Accuracy: 82.00, Test time: 92.60\n",
      "Epoch [1/1], Step [82700/90866], Accuracy: 82.00, Test time: 92.75\n",
      "Epoch [1/1], Step [82800/90866], Accuracy: 82.00, Test time: 92.84\n",
      "Epoch [1/1], Step [82900/90866], Accuracy: 82.00, Test time: 92.96\n",
      "Epoch [1/1], Step [83000/90866], Accuracy: 82.00, Test time: 93.08\n",
      "Epoch [1/1], Step [83100/90866], Accuracy: 82.00, Test time: 93.16\n",
      "Epoch [1/1], Step [83200/90866], Accuracy: 82.01, Test time: 93.27\n",
      "Epoch [1/1], Step [83300/90866], Accuracy: 82.01, Test time: 93.37\n",
      "Epoch [1/1], Step [83400/90866], Accuracy: 82.01, Test time: 93.55\n",
      "Epoch [1/1], Step [83500/90866], Accuracy: 82.00, Test time: 93.63\n",
      "Epoch [1/1], Step [83600/90866], Accuracy: 82.00, Test time: 93.75\n",
      "Epoch [1/1], Step [83700/90866], Accuracy: 82.00, Test time: 93.84\n",
      "Epoch [1/1], Step [83800/90866], Accuracy: 82.00, Test time: 94.03\n",
      "Epoch [1/1], Step [83900/90866], Accuracy: 81.99, Test time: 94.11\n",
      "Epoch [1/1], Step [84000/90866], Accuracy: 82.00, Test time: 94.20\n",
      "Epoch [1/1], Step [84100/90866], Accuracy: 82.00, Test time: 94.31\n",
      "Epoch [1/1], Step [84200/90866], Accuracy: 82.01, Test time: 94.37\n",
      "Epoch [1/1], Step [84300/90866], Accuracy: 82.00, Test time: 94.49\n",
      "Epoch [1/1], Step [84400/90866], Accuracy: 82.01, Test time: 94.58\n",
      "Epoch [1/1], Step [84500/90866], Accuracy: 82.01, Test time: 94.74\n",
      "Epoch [1/1], Step [84600/90866], Accuracy: 82.01, Test time: 94.83\n",
      "Epoch [1/1], Step [84700/90866], Accuracy: 82.01, Test time: 94.92\n",
      "Epoch [1/1], Step [84800/90866], Accuracy: 82.01, Test time: 95.13\n",
      "Epoch [1/1], Step [84900/90866], Accuracy: 82.01, Test time: 95.19\n",
      "Epoch [1/1], Step [85000/90866], Accuracy: 82.00, Test time: 95.28\n",
      "Epoch [1/1], Step [85100/90866], Accuracy: 82.01, Test time: 95.40\n",
      "Epoch [1/1], Step [85200/90866], Accuracy: 82.01, Test time: 95.53\n",
      "Epoch [1/1], Step [85300/90866], Accuracy: 82.01, Test time: 95.64\n",
      "Epoch [1/1], Step [85400/90866], Accuracy: 82.00, Test time: 95.73\n",
      "Epoch [1/1], Step [85500/90866], Accuracy: 82.00, Test time: 95.85\n",
      "Epoch [1/1], Step [85600/90866], Accuracy: 82.00, Test time: 95.93\n",
      "Epoch [1/1], Step [85700/90866], Accuracy: 82.00, Test time: 96.09\n",
      "Epoch [1/1], Step [85800/90866], Accuracy: 82.00, Test time: 96.17\n",
      "Epoch [1/1], Step [85900/90866], Accuracy: 82.00, Test time: 96.30\n",
      "Epoch [1/1], Step [86000/90866], Accuracy: 82.00, Test time: 96.39\n",
      "Epoch [1/1], Step [86100/90866], Accuracy: 82.01, Test time: 96.51\n",
      "Epoch [1/1], Step [86200/90866], Accuracy: 82.01, Test time: 96.63\n",
      "Epoch [1/1], Step [86300/90866], Accuracy: 82.01, Test time: 96.73\n",
      "Epoch [1/1], Step [86400/90866], Accuracy: 82.01, Test time: 96.87\n",
      "Epoch [1/1], Step [86500/90866], Accuracy: 82.01, Test time: 96.96\n",
      "Epoch [1/1], Step [86600/90866], Accuracy: 82.02, Test time: 97.09\n",
      "Epoch [1/1], Step [86700/90866], Accuracy: 82.02, Test time: 97.20\n",
      "Epoch [1/1], Step [86800/90866], Accuracy: 82.03, Test time: 97.34\n",
      "Epoch [1/1], Step [86900/90866], Accuracy: 82.03, Test time: 97.42\n",
      "Epoch [1/1], Step [87000/90866], Accuracy: 82.03, Test time: 97.54\n",
      "Epoch [1/1], Step [87100/90866], Accuracy: 82.02, Test time: 97.58\n",
      "Epoch [1/1], Step [87200/90866], Accuracy: 82.02, Test time: 97.65\n",
      "Epoch [1/1], Step [87300/90866], Accuracy: 82.02, Test time: 97.99\n",
      "Epoch [1/1], Step [87400/90866], Accuracy: 82.01, Test time: 98.04\n",
      "Epoch [1/1], Step [87500/90866], Accuracy: 82.01, Test time: 98.09\n",
      "Epoch [1/1], Step [87600/90866], Accuracy: 82.02, Test time: 98.18\n",
      "Epoch [1/1], Step [87700/90866], Accuracy: 82.01, Test time: 98.26\n",
      "Epoch [1/1], Step [87800/90866], Accuracy: 82.01, Test time: 98.37\n",
      "Epoch [1/1], Step [87900/90866], Accuracy: 82.01, Test time: 98.51\n",
      "Epoch [1/1], Step [88000/90866], Accuracy: 82.01, Test time: 98.67\n",
      "Epoch [1/1], Step [88100/90866], Accuracy: 82.01, Test time: 98.75\n",
      "Epoch [1/1], Step [88200/90866], Accuracy: 82.00, Test time: 98.84\n",
      "Epoch [1/1], Step [88300/90866], Accuracy: 82.00, Test time: 98.92\n",
      "Epoch [1/1], Step [88400/90866], Accuracy: 82.00, Test time: 99.03\n",
      "Epoch [1/1], Step [88500/90866], Accuracy: 82.00, Test time: 99.12\n",
      "Epoch [1/1], Step [88600/90866], Accuracy: 82.00, Test time: 99.45\n",
      "Epoch [1/1], Step [88700/90866], Accuracy: 82.00, Test time: 99.52\n",
      "Epoch [1/1], Step [88800/90866], Accuracy: 82.00, Test time: 99.58\n",
      "Epoch [1/1], Step [88900/90866], Accuracy: 81.99, Test time: 99.68\n",
      "Epoch [1/1], Step [89000/90866], Accuracy: 81.99, Test time: 99.77\n",
      "Epoch [1/1], Step [89100/90866], Accuracy: 81.99, Test time: 100.03\n",
      "Epoch [1/1], Step [89200/90866], Accuracy: 82.00, Test time: 100.11\n",
      "Epoch [1/1], Step [89300/90866], Accuracy: 82.00, Test time: 100.17\n",
      "Epoch [1/1], Step [89400/90866], Accuracy: 82.00, Test time: 100.23\n",
      "Epoch [1/1], Step [89500/90866], Accuracy: 82.00, Test time: 100.42\n",
      "Epoch [1/1], Step [89600/90866], Accuracy: 81.99, Test time: 100.53\n",
      "Epoch [1/1], Step [89700/90866], Accuracy: 82.00, Test time: 100.63\n",
      "Epoch [1/1], Step [89800/90866], Accuracy: 82.00, Test time: 100.73\n",
      "Epoch [1/1], Step [89900/90866], Accuracy: 82.00, Test time: 100.82\n",
      "Epoch [1/1], Step [90000/90866], Accuracy: 82.00, Test time: 101.09\n",
      "Epoch [1/1], Step [90100/90866], Accuracy: 82.00, Test time: 101.15\n",
      "Epoch [1/1], Step [90200/90866], Accuracy: 82.00, Test time: 101.23\n",
      "Epoch [1/1], Step [90300/90866], Accuracy: 82.00, Test time: 101.31\n",
      "Epoch [1/1], Step [90400/90866], Accuracy: 82.00, Test time: 101.42\n",
      "Epoch [1/1], Step [90500/90866], Accuracy: 82.01, Test time: 101.70\n",
      "Epoch [1/1], Step [90600/90866], Accuracy: 82.00, Test time: 101.91\n",
      "Epoch [1/1], Step [90700/90866], Accuracy: 82.00, Test time: 101.97\n",
      "Epoch [1/1], Step [90800/90866], Accuracy: 82.00, Test time: 102.07\n",
      "Accuracy on test set: 82.01%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # Print training statistics\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Accuracy: {:.2f}, Test time: {:.2f}'\n",
    "                      .format(epoch+1, num_epochs, i+1, len(val_loader), 100 * correct / total, (time.time()-start_time)/60))\n",
    "\n",
    "    print('Accuracy on test set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506933c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
